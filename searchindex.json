{"categories":[{"title":"blog","uri":"https://healinyoon.github.io/categories/blog/"},{"title":"ci/cd","uri":"https://healinyoon.github.io/categories/ci/cd/"},{"title":"cloud","uri":"https://healinyoon.github.io/categories/cloud/"},{"title":"devops","uri":"https://healinyoon.github.io/categories/devops/"},{"title":"gpu","uri":"https://healinyoon.github.io/categories/gpu/"},{"title":"jvm","uri":"https://healinyoon.github.io/categories/jvm/"},{"title":"linux","uri":"https://healinyoon.github.io/categories/linux/"},{"title":"msa","uri":"https://healinyoon.github.io/categories/msa/"},{"title":"programming","uri":"https://healinyoon.github.io/categories/programming/"}],"posts":[{"content":"Intro 이번 장에서는 Helm Chart를 더 잘 활용하기 위한 방법을 살펴본다.\nHelm Chart 기본 명령어 사용 방법 helm search: Chart 검색 Helm은 강력한 검색 명령어를 제공한다. 아래의 두 명령어는 서로 다른 유형의 Repository Source로부터 검색하는데 사용할 수 있다.\n helm search hub: 여러 저장소에있는 helm chart를 포괄하는 [helm hub](https://artifacthub.io/)에서 검색한다. helm search repo: helm repo add를 사용하여 local helm client에 추가한 저장소에서 검색한다. 검색은 local data 상에서 이루어지며, 퍼블릭 네트워크 접속이 필요하지 않다.  helm search hub 사용 예시 helm search hub 명령어를 사용하면 공개적으로 사용 가능한 chart를 찾아볼 수 있다.\n$ helm search hub wordpress URL CHART VERSION\tAPP VERSION\tDESCRIPTION https://hub.helm.sh/charts/bitnami/wordpress 9.9.1 5.5.3 Web publishing platform for building blogs and ... https://hub.helm.sh/charts/seccurecodebox/old-w...\t2.1.0 4.0 Insecure \u0026amp; Outdated Wordpress Instance: Never e... https://hub.helm.sh/charts/presslabs/wordpress-...\t0.10.5 0.10.5 Presslabs WordPress Operator Helm Chart  helm search repo 사용 예시 helm search repo 명령어를 사용하면 기존에 추가된 저장소에서 사용 가능한 chart를 찾아볼 수 있다.\n$ helm search repo wordpress NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/wordpress\t9.0.3 5.3.2   (참고) helm search는 fuzzy string matcing 알고리즘을 사용하기 때문에 검색시 단어 또는 구문의 일부분만 입력해도 된다.\n $ helm search repo word NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/wordpress\t9.0.3 5.3.2 DEPRECATED Web publishing platform for building... $ helm search repo wordp NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/wordpress\t9.0.3 5.3.2 DEPRECATED Web publishing platform for building...  helm install: Chart 패키지 설치 사용하려는 Chart를 찾은 후에는 helm install {사용자 지정 release name} {chart name} 명령어를 사용하여 설치해준다.\n$ helm install my-mariadb stable/mariadb WARNING: This chart is deprecated NAME: my-mariadb LAST DEPLOYED: Tue Nov 3 05:07:12 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: This Helm chart is deprecated (중략)  Chart를 설치하면 새로운 relase 인스턴스가 생성되는 것을 알아두자. 위의 명령어에서 release의 이름은 my-mariadb이다. 만약 별도의 사용자 지정 release name 없이 helm이 자동으로 생성해주는 이름을 그대로 사용하려면 --generate-name 옵션을 사용하면 된다.\nHelm은 release시 설치되는 모든 리소스가 running 상태로 변할 때까지 기다리지 않는다. 따라서 release의 리소스 상태 추척을 계속하거나, 구성 정보를 확인하고 싶을 때는 helm status 명령어를 사용하자.\n$ helm status my-mariadb NAME: my-mariadb LAST DEPLOYED: Tue Nov 3 05:40:42 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: This Helm chart is deprecated  그런데.. helm status 명령어를 사용해도 각 리소스 상태가 출력되지 않는다. 원래 Helm version2에서는 아래와 같이 출력되는데, version3에서 부터 리소스 출력이 사라졌다.\n (Helm version2에서의 helm status 출력 예시)\n $ helm status mychart LAST DEPLOYED: Fri Jun 28 11:16:50 2019 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE mychart 1/1 1 1 4m2s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE mychart-b9488ff5c-b6xzz 1/1 Running 0 4m2s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mychart ClusterIP 10.110.150.104 \u0026lt;none\u0026gt; 80/TCP 4m2s  원인을 찾아보니, \u0026ldquo;The main reason for this is that by doing this in Helm, it leads to code duplication (we basically end up copying kubectl code), possible bugs, and maintenance overhead in the Helm codebase. So I think it would be better to keep it out(상세 내용 확인하기).\u0026quot; 라고 논의되고 있다. 해결하기 위한 노력(참고)이 이루어지고 있지만, 아직까지는 지원되지 않는다.\n아쉽지만 kubectl get all 명령어로 조회하자.\n$ kubectl get all | grep my-mariadb pod/my-mariadb-master-0 0/1 Pending 0 44m pod/my-mariadb-slave-0 0/1 Pending 0 44m service/my-mariadb ClusterIP 10.104.91.81 \u0026lt;none\u0026gt; 3306/TCP 44m service/my-mariadb-slave ClusterIP 10.105.42.35 \u0026lt;none\u0026gt; 3306/TCP 44m statefulset.apps/my-mariadb-master 0/1 44m statefulset.apps/my-mariadb-slave 0/1 44m  설치 전 Chart 커스터마이징하는 방법 여\n출처  https://helm.sh/ko/docs/intro/using_helm/  ","id":0,"section":"posts","summary":"Intro 이번 장에서는 Helm Chart를 더 잘 활용하기 위한 방법을 살펴본다. Helm Chart 기본 명령어 사용 방법 helm search: Chart 검색 Helm은 강력한 검색 명령어를 제공한다. 아래의 두 명","tags":["kubernetes","helm"],"title":"Helm(2) - Helm Chart 사용 방법","uri":"https://healinyoon.github.io/2020/11/20201103_helm_02/","year":"2020"},{"content":"Helm 이란? Helm은 Kubernetes package managing tool이다. Python에서의 pip, Node.js에서의 npm과 같은 역할을 하는 Kubernetes package 설치를 가능하게 하는 tool이라고 보면 된다.\nHelm의 필요성 Kubernetest에 하나의 애플리케이션을 배포하기 위해서는 간단하게 Pod만 배포해서는 사용하기 어려운 경우가 많다. 외부 서비스를 위한 Service, Pod 관리를 위한 Deployment등 추가적인 리소스 배포가 필요하기 때문이다.\n이를 위해 Helm은 애플리케이션 컨테이너 배포는 물론이고, 이에 필요한 Kubernetes 리소스를 모두 패키지 형태로 배포해주는 역할을 한다.\nHelm의 구성도 이미지 출처: https://tech.osci.kr/2019/11/23/86027123/\nHelm Chart Helm 패키지이다. 이 패키지에는 Kubernetest 클러스터 내에서 애플리케이션을 동작시키기 위해 필요한 모든 리소스 정의가 포함되어 있다.\nHelm Chart Repository Helm Chart를 저장하고 공유하는 저장소이다.\nRelease Kubernetes 클러스터에서 구동되는 Chart의 인스턴스이다. 일반적으로 하나의 Chart는 동일한 클러스터 내에 여러번 설치될 수 있다. 설치할 때마다 새로운 Release가 생성된다.\nHelm Client Helm Server 모듈과 통신하는 역할을 한다.\nHelm Server(Tiller) Helm Client의 요청을 처리하여 Kubernetes에 Chart를 설치하고 릴리즈를 관리한다.\nHelm 설치하기  Helm 공식 설치 문서  Helm 설치 Helm을 설치하는 다양한 방법이 있으나, 여기서는 Helm에서 제공하는 최신 버전을 자동으로 가져와서 설치하는 스크립트를 사용한다.\n$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3  $ chmod 700 get_helm.sh  $ ./get_helm.sh Downloading https://get.helm.sh/helm-v3.4.0-linux-amd64.tar.gz Verifying checksum... Done. Preparing to install helm into /usr/local/bin helm installed into /usr/local/bin/helm  Helm Chart Repository 초기화 Helm이 준비되면 Chart Repository를 추가할 수 있다. 현재 repository를 조회해보면 다음과 같이 아무것도 없다고 출력된다.\n$ helm repo list Error: no repositories to show  다양한 저장소를 추가할 수 있지만 가장 기본인 Helm official stable charts를 추가하였다.\n$ helm repo add stable https://charts.helm.sh/stable \u0026quot;stable\u0026quot; has been added to your repositories $ helm repo list NAME URL stable\thttps://charts.helm.sh/stable  stable에서 제공하는 chart 리스트를 조회할 수 있다.\n$ helm search repo stable NAME CHART VERSION\tAPP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.2 2.1.1 DEPRECATED Scales worker nodes within agent pools stable/aerospike 0.3.3 v4.5.0.5 A Helm chart for Aerospike in Kubernetes stable/airflow 7.13.2 1.10.12 Airflow is a platform to programmatically autho... stable/ambassador 5.3.2 0.86.1 DEPRECATED A Helm chart for Datawire Ambassador (중략)  Helm Chart Repository 업데이트 Repository로 부터 최신 Chart를 다운로드 하기 위해서는 애플리케이션 설치 이전에 repository를 업데이트해주는 것이 좋다.\n$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. ⎈Happy Helming!⎈  Helm Chart Repository 사용해보기 Helm Chart 확인 Helm Chart를 찾아 애플리케이션을 설치하는 여러 가지 방법이 있지만, 시작하기 가장 좋은 방법은 위에서 구성한 official stable chart 중 하나를 사용하여 설치하는 것이다.\n여기서는 mysql을 예시로 설치해본다.\n설치하려는 MySQL chart의 간단한 정보는 다음 명령어를 통해 확인 가능하다.\n$ helm show chart stable/mysql apiVersion: v1 appVersion: 5.7.30 description: Fast, reliable, scalable, and easy to use open-source relational database system. home: https://www.mysql.com/ icon: https://www.mysql.com/common/logos/logo-mysql-170x115.png keywords: - mysql - database - sql maintainers: - email: o.with@sportradar.com name: olemarkus - email: viglesias@google.com name: viglesiasce name: mysql sources: - https://github.com/kubernetes/charts - https://github.com/docker-library/mysql version: 1.6.7  만약 해당 chart에 대한 모든 정보를 확인하고 싶다면 다음 명령어를 사용하자(상당히 많은 정보가 출력된다).\n$ helm show all stable/mysql  Helm Chart를 이용하여 애플리케이션 설치 이제 helm chart를 이용하여 MySQL 애플리케이션을 설치해보자.\n$ helm install stable/mysql --generate-name NAME: mysql-1604311201 LAST DEPLOYED: Mon Nov 2 10:00:04 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: mysql-1604311201.default.svc.cluster.local To get your root password run: MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace default mysql-1604311201 -o jsonpath=\u0026quot;{.data.mysql-root-password}\u0026quot; | base64 --decode; echo) To connect to your database: 1. Run an Ubuntu pod that you can use as a client: kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il 2. Install the mysql client: $ apt-get update \u0026amp;\u0026amp; apt-get install mysql-client -y 3. Connect using the mysql cli, then provide your password: $ mysql -h mysql-1604311201 -p To connect to your database directly from outside the K8s cluster: MYSQL_HOST=127.0.0.1 MYSQL_PORT=3306 # Execute the following command to route the connection: kubectl port-forward svc/mysql-1604311201 3306 mysql -h ${MYSQL_HOST} -P${MYSQL_PORT} -u root -p${MYSQL_ROOT_PASSWORD}  위의 예에서 stable/mysql chart가 설치되었고, 이름은 mysql-1604311201인 것을 확인할 수 있다.\nHelm chart로 설치한 애플리케이션을 조회해보자.\n$ helm ls NAME NAMESPACE\tREVISION\tUPDATED STATUS CHART APP VERSION mysql-1604311201\tdefault 1 2020-11-02 10:00:04.300938829 +0000 UTC\tdeployed\tmysql-1.6.7\t5.7.30  당연히 kubectl로도 조회가 가능하다.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mysql-1604311201-654cf77476-w58m7 0/1 Pending 0 35m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 55d service/mysql-1604311201 ClusterIP 10.111.137.120 \u0026lt;none\u0026gt; 3306/TCP 35m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mysql-1604311201 0/1 1 0 35m NAME DESIRED CURRENT READY AGE replicaset.apps/mysql-1604311201-654cf77476 1 1 0 35m  애플리케이션 설치를 위해 필요한 모든 kubernetes 리소스가 함께 설치된 것을 볼 수 있다.\n이렇게 Helm chart를 사용하여 설치를 하게 되면 새로운 release가 된것이다. 즉 하나의 Helm chart로 여러번의 설치를 할 수 있는데, 이때 각각의 설치는 덮어씌어지는 것이 아니라 독립적으로 관리되고 업그레이드된다.\n그렇다면 실제로 그렇게 동작하는지 테스트해보자. 위와 동일한 방식으로 mysql을 하나 더 설치한다.\n$ helm install stable/mysql --generate-name NAME: mysql-1604313417 LAST DEPLOYED: Mon Nov 2 10:37:00 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: mysql-1604313417.default.svc.cluster.local  helm ls로 조회해보면 새로운 release가 생성되었음을 확인할 수 있고\n$ helm ls NAME NAMESPACE\tREVISION\tUPDATED STATUS CHART APP VERSION mysql-1604311201\tdefault 1 2020-11-02 10:00:04.300938829 +0000 UTC\tdeployed\tmysql-1.6.7\t5.7.30 mysql-1604313417\tdefault 1 2020-11-02 10:37:00.578674443 +0000 UTC\tdeployed\tmysql-1.6.7\t5.7.30  kubectl get all로 조회해도 마찬가지로 독립적인 애플리케이션이 설치되었음을 알 수 있다.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mysql-1604311201-654cf77476-w58m7 0/1 Pending 0 39m pod/mysql-1604313417-9685cfbd6-srxtl 0/1 Pending 0 2m53s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 55d service/mysql-1604311201 ClusterIP 10.111.137.120 \u0026lt;none\u0026gt; 3306/TCP 39m service/mysql-1604313417 ClusterIP 10.104.229.201 \u0026lt;none\u0026gt; 3306/TCP 2m53s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mysql-1604311201 0/1 1 0 39m deployment.apps/mysql-1604313417 0/1 1 0 2m53s NAME DESIRED CURRENT READY AGE replicaset.apps/mysql-1604311201-654cf77476 1 1 0 39m replicaset.apps/mysql-1604313417-9685cfbd6 1 1 0 2m53s  Helm Chart를 이용하여 설치한 애플리케이션 제거 Helm chart를 이용하여 설치한 애플리케이션을 제거하는 방법은 아주 간단하다.\n$ helm uninstall mysql-1604311201 release \u0026quot;mysql-1604311201\u0026quot; uninstalled $ helm uninstall mysql-1604313417 release \u0026quot;mysql-1604313417\u0026quot; uninstalled $ helm ls NAME\tNAMESPACE\tREVISION\tUPDATED\tSTATUS\tCHART\tAPP VERSION  출처  https://bcho.tistory.com/1335 https://helm.sh/docs/intro/install/ https://docs.helm.sh/docs/intro/quickstart/ https://tech.osci.kr/2019/11/23/86027123/  ","id":1,"section":"posts","summary":"Helm 이란? Helm은 Kubernetes package managing tool이다. Python에서의 pip, Node.js에서의 npm과 같은 역할을 하는 Kubernetes package 설치를 가능하게 하는 tool이라고 보면","tags":["kubernetes","helm"],"title":"Helm(1) - Helm 설치 및 사용 방법","uri":"https://healinyoon.github.io/2020/11/20201103_helm_01/","year":"2020"},{"content":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자.\n사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은 조건이 있다면 선행해주자.\n1. kubernetes cluster 구축 아직 구축이 되어 있지 않다면 Kubernetes Cluster 설치하기(ubuntu18.04)를 참고하여 진행\n2. nvidia-docker 설치 아직 설치가 되어 있지 않다면 nvidia-docker install guide를 참고하여 진행\n1. Nvidia Plugin Pod 생성 ref)  Nvidia k8s-device-plugin 공식 사이트 Nvidia docker 공식 사이트  1.1 가장 정보가 많았던 YAML으로 설치, 그러나 실패 처음에 여기 링크를 참고하여 진행했다.\n다만 아래와 같이 버전만 변경하여 실행했다.\n$ kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml daemonset.extensions/nvidia-device-plugin- daemonset-1.12 created  하지만 아래 이슈 발생해서 실패했다 ↓ ↓ ↓\n1.2. 이슈 1.2.1. 이슈 내용 쿠버네티스 1.15 버전 이하를 설치했을 경우 문제 없겠지만, 1.16 버전 이상을 설치했을 경우 다음과 같은 에러가 발생한다.\nerror: unable to recognize \u0026quot;https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml\u0026quot;: no matches for kind \u0026quot;DaemonSet\u0026quot; in version \u0026quot;extensions/v1beta1\u0026quot;  이는 쿠버네티스 버전이 업그레이드 되면서, Daemonset의 extensions/v1beta1 버전을 더이상 지원하지 않기 때문이다.\n→ 1) 따라서 버전을 apps/v1 으로 변경하고 selector object를 추가한 후\n→ 2) k8s-device-plugin을 다시 설치하고\n→ 3) 매니패스트 파일도 적절하게 수정해주었다.\nref)  Kubectl convert 참고 자료 No matches 이슈 해결 자료   (참고) Pod 생성 실패시 에러 정보 확인\n $ kubectl describe pod {pod 명}  1.2.2. 변경한 YAML 파일을 사용하여 DaemonSet Pod 생성 이제 커스터 마이징한 YAML 파일로 Pod를 생성해보자.\n gpu-plugin.yaml\n apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-device-plugin-daemonset-1.12 namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: name: nvidia-device-plugin-ds template: metadata: # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler # reserves resources for critical add-on pods so that they can be rescheduled after # a failure. This annotation works in tandem with the toleration below. annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; labels: name: nvidia-device-plugin-ds spec: tolerations: # Allow this pod to be rescheduled while the node is in \u0026quot;critical add-ons only\u0026quot; mode. # This, along with the annotation above marks this pod as a critical add-on. - key: CriticalAddonsOnly operator: Exists - key: nvidia.com/gpu operator: Exists effect: NoSchedule containers: - image: nvidia/k8s-device-plugin:1.11 name: nvidia-device-plugin-ctr securityContext: allowPrivilegeEscalation: false capabilities: drop: [\u0026quot;ALL\u0026quot;] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins nodeSelector: gpus: \u0026quot;true\u0026quot;  위의 매니패스트 주요 사항은 다음과 같다.\n① 리소스 유형 = DaemonSet\nkind: DaemonSet  따라서 기본적으로는 모든 worker 노드 하나씩 동작하게 한다.  ② RollingUpdate\nspec.selector.matchLabels.name\nname: nvidia-device-plugin-ds  spec.template.matadata.labels.name\nlabels: name: nvidia-device-plugin-ds  name object가 nvidia-device-plugin-ds 인 리소스에 대하여 RollingUpdate를 설정한다.\n③ node Label 지정\nspec.template.spec.nodeSelector 로 어느 노드의 DaemonSet으로 띄워줄 것인지 레이블링해준다.\nnodeSelector: gpus: \u0026quot;true\u0026quot;  1.3. gpu-plugin DeamonSet Pod 정상 동작 확인 $ kubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-d7t2g 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-jmcg9 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-zhsqm 1/1 Running 0 4h8mubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-7kh27 1/1 Running 0 27m $ kubectl -n kube-system logs -l name=nvidia-device-plugin-ds 2020/07/01 05:02:30 Loading NVML 2020/07/01 05:02:30 Fetching devices. 2020/07/01 05:02:30 Starting FS watcher. 2020/07/01 05:02:30 Starting OS watcher. 2020/07/01 05:02:30 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock 2020/07/01 05:02:30 Registered device plugin with Kubelet  🌟🌟 여기서 잠깐! 🌟🌟 중요한 사항은 gpu를 사용하려는 Worker node가 gpus: \u0026quot;true\u0026quot; 레이블링이 되어 있어야 한다는 것이다.\n 만약 GPU가 있는 node인데 해당 DeamonSet이 올라가있지 않거나 신규 Worker node를 추가하려는 경우  ⇒ kubectl label nodes {Worker node 명} gpus=true 로 레이블링을 해주자.\n2. GPU 개수 확인 이제 쿠버네티스가 사용 가능한 GPU 개수를 확인해보자.\nmaster node에서 아래의 명령어를 실행하면 각 worker node에서 사용 가능한 GPU 개수가 출력된다.\n$ kubectl get nodes \u0026quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\u0026quot; NAME GPU gpu-1080ti-XX 9 gpu-1080ti-XX 9  3. Pod에서 그래픽 카드 명령어 테스트 3.1. GPU를 사용하는 Pod 생성하기 3.1.1. YAML 파일  gpu-k8s.yaml\n apiVersion: v1 kind: Pod metadata: name: gpu-k8s spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null resources: requests: nvidia.com/gpu: 2 limits: nvidia.com/gpu: 2  ###ref) nvidia/cuda 도커 이미지 버전이 맞지 않은 이슈 발생 시 ⇒ 도커 허브에서 맞는 이미지 버전을 찾아서 사용해주면 된다.\n Docker Hub Kubernetes Resource Request와 Limit의 이해 Schedule GPUs  3.1.2. Pod 생성 및 확인 $ kubectl apply -f gpu-k8s.yaml pod/gpu-k8s created $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 12s  3.1.3. nvidia-smi 확인 $ kubectl logs gpu-k8s Thu Jul 2 06:00:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 30C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.2. 2개의 Pod를 띄워서 gpu 4개를 모두 사용하기 gpu-k8s2.yaml 매니패스트 파일을 하나 더 만들어서 위와 동일하게 실행해보자.\n3.2.1. 결과 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 2m44s gpu-k8s2 1/1 Running 0 2  3.2.2. nvidia-smi 확인 $ kubectl logs gpu-k8s2 Thu Jul 2 06:03:06 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 31C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.3. 컨테이너가 노드의 모든 GPU를 사용 가능하게 하고 싶다면  request와 limit 설정 부분을 없애주면 된다. 특이한 점은 이미 다른 파드에 GPU를 모두 할당 해준 상태에서도 파드 생성 가능하다.  3.3.1. YAML 파일 apiVersion: v1 kind: Pod metadata: name: gpu-all spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime env: - name: DP_DISABLE_HEALTHCHECKS value: \u0026quot;xids\u0026quot; command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null  3.3.2. Pod 실행 $ kubectl apply -f gpu-all.yaml pod/gpu-all created  3.3.3. 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-all 1/1 Running 0 50s gpu-k8s 1/1 Running 0 42m gpu-k8s2 1/1 Running 0 40m $ kubectl logs gpu-all Thu Jul 2 06:42:25 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 32C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  ","id":2,"section":"posts","summary":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자. 사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은","tags":["docker","kubernetes"],"title":"Kubernets(쿠버네티스) with GPU 구축하기","uri":"https://healinyoon.github.io/2020/10/20201023_kubernetes_with_gpu/","year":"2020"},{"content":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주하는 질문이므로, 이번 기회에 정리를 해보려고 한다.\n1. Host vs Container GPU 성능 테스트 요약 1.1. 결론 GPU on Host VS Container를 비교하였을 때 속도의 성능 차이가 없었음\n1.2. 상세 내용  GPU Performance on Host VS Container 비교시 정확도에는 차이가(모델이 같으므로) 없으므로, 속도만 비교하였다. GPU를 1개 사용하였을 경우와 2개 사용하였을 경우를 나누어 비교하였다. 동일한 조건에 대해 테스트 케이스(횟수)를 총 2회씩 수행하였다.  2. GPU 성능 비교를 위한 Host 환경 설정 2.1. 가상환경 생성 # pip3 install virtualenv # virtualenv /home/rin_gu/PerformTestEnv/performTestEnv  2.2. Tensorflow 설치 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # python -m pip install --upgrade pip # sudo -H pip install --upgrade tf-nightly-gpu  2.3. 성능 테스트 코드 실행 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # cd /home/rin_gu/PerformanceTest/ # git clone https://github.com/tensorflow/benchmarks.git # cd benchmarks  3. GPU 성능 비교를 위한 Container 환경 설정 3.1. 컨테이너 이미지 다운로드 root@ubuntu:~# docker pull tensorflow/tensorflow:nightly-gpu  3.2. 컨테이너 run with GPU root@ubuntu:~# docker container run -d --gpus all -it tensorflow/tensorflow:nightly-gpu 94515f052adbddf25bb9c66e5d5d7ee6a6010cfc74c6936f3b01bdf764202488  3.3. 컨테이너 접속 root@ubuntu:~# docker exec -it 94515f052adb /bin/bash \u0026quot;docker exec\u0026quot; requires at least 2 arguments. See 'docker exec --help'. Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container root@ubuntu:~# docker container exec -it 94515f052adb /bin/bash ________ _______________ ___ __/__________________________________ ____/__ /________ __ __ / _ _ \\_ __ \\_ ___/ __ \\_ ___/_ /_ __ /_ __ \\_ | /| / / _ / / __/ / / /(__ )/ /_/ / / _ __/ _ / / /_/ /_ |/ |/ / /_/ \\___//_/ /_//____/ \\____//_/ /_/ /_/ \\____/____/|__/ WARNING: You are running this container as root, which can cause new files in mounted volumes to be created as the root user on your host machine. To avoid this, run the container by specifying your user's userid: $ docker run -u $(id -u):$(id -g) args...  3.4. 파이썬 버전 확인 root@282efcfdc3c8:/# python Python 3.6.9 (default, Apr 18 2020, 01:56:04) [GCC 8.4.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import tensorflow as tf 2020-06-22 11:16:19.882107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 \u0026gt;\u0026gt;\u0026gt; tf.__version__ '2.3.0-dev20200621'  3.5. git 설치 / 리포지토리 다운로드 root@94515f052adb:/# apt-get install git root@94515f052adb:/# git clone https://github.com/tensorflow/benchmarks.git  4. Host vs Container 성능 비교 4.1. GPU 1개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스} python tf_cnn_benchmarks.py --num_gpus=1 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 124.77 126.95 model: Resnet50   2 125.76 125.07 model: Resnet50    4.2. GPU 2개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스1, GPU 디바이스2} python tf_cnn_benchmarks.py --num_gpus=2 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 242.60 240.64 model: Resnet50   2 241.52 236.72 model: Resnet50    5. GPU 성능 측정 상세 - Host 5.1. 테스트 1 5.1.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 127.9 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 126.4 +/- 0.2 (jitter = 0.4)\t7.849 20\timages/sec: 126.3 +/- 0.1 (jitter = 0.5)\t8.013 30\timages/sec: 126.2 +/- 0.1 (jitter = 0.7)\t7.940 40\timages/sec: 126.0 +/- 0.1 (jitter = 0.7)\t8.137 50\timages/sec: 125.8 +/- 0.1 (jitter = 0.6)\t8.052 60\timages/sec: 125.6 +/- 0.1 (jitter = 0.7)\t7.782 70\timages/sec: 125.4 +/- 0.1 (jitter = 0.9)\t7.856 80\timages/sec: 125.3 +/- 0.1 (jitter = 1.0)\t8.011 90\timages/sec: 125.1 +/- 0.1 (jitter = 1.2)\t7.843 100\timages/sec: 124.8 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 124.77 ----------------------------------------------------------------  5.2. 테스트 2 5.2.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중량) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 246.2 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 244.9 +/- 0.6 (jitter = 1.7)\t7.892 20\timages/sec: 244.3 +/- 0.4 (jitter = 2.6)\t7.968 30\timages/sec: 244.5 +/- 0.4 (jitter = 2.3)\t7.934 40\timages/sec: 244.2 +/- 0.3 (jitter = 2.6)\t8.016 50\timages/sec: 243.8 +/- 0.3 (jitter = 2.6)\t7.922 60\timages/sec: 243.6 +/- 0.3 (jitter = 2.0)\t7.872 70\timages/sec: 243.5 +/- 0.2 (jitter = 1.9)\t7.837 80\timages/sec: 243.3 +/- 0.2 (jitter = 1.8)\t7.850 90\timages/sec: 243.0 +/- 0.2 (jitter = 2.1)\t7.859 100\timages/sec: 242.7 +/- 0.2 (jitter = 2.2)\t7.946 ---------------------------------------------------------------- total images/sec: 242.60 ----------------------------------------------------------------  6. GPU 성능 측정 상세 - Container 6.1. 테스트 1 6.1.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 129.2 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 128.7 +/- 0.4 (jitter = 0.9)\t7.849 20\timages/sec: 128.4 +/- 0.2 (jitter = 1.1)\t8.013 30\timages/sec: 128.4 +/- 0.2 (jitter = 0.8)\t7.940 40\timages/sec: 128.3 +/- 0.1 (jitter = 0.9)\t8.136 50\timages/sec: 128.1 +/- 0.1 (jitter = 0.9)\t8.053 60\timages/sec: 127.9 +/- 0.1 (jitter = 1.1)\t7.784 70\timages/sec: 127.7 +/- 0.1 (jitter = 1.4)\t7.859 80\timages/sec: 127.5 +/- 0.1 (jitter = 1.4)\t8.014 90\timages/sec: 127.3 +/- 0.1 (jitter = 1.4)\t7.842 100\timages/sec: 127.1 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 126.95 ----------------------------------------------------------------  6.2. 테스트 2 6.2.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 240.6 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 243.4 +/- 0.8 (jitter = 3.5)\t7.892 20\timages/sec: 243.3 +/- 0.5 (jitter = 2.7)\t7.968 30\timages/sec: 243.1 +/- 0.4 (jitter = 2.8)\t7.934 40\timages/sec: 242.6 +/- 0.3 (jitter = 2.3)\t8.019 50\timages/sec: 242.4 +/- 0.3 (jitter = 2.0)\t7.923 60\timages/sec: 242.1 +/- 0.3 (jitter = 1.8)\t7.878 70\timages/sec: 241.8 +/- 0.2 (jitter = 1.6)\t7.834 80\timages/sec: 241.5 +/- 0.2 (jitter = 1.6)\t7.861 90\timages/sec: 241.1 +/- 0.2 (jitter = 2.0)\t7.852 100\timages/sec: 240.8 +/- 0.3 (jitter = 2.1)\t7.948 ---------------------------------------------------------------- total images/sec: 240.64 ----------------------------------------------------------------  ","id":3,"section":"posts","summary":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주","tags":["docker"],"title":"GPU 성능 비교하기: Host vs Container(+Container에서 GPU 사용하기)","uri":"https://healinyoon.github.io/2020/10/20201023_gpu_performance_host_vs_container/","year":"2020"},{"content":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다.\n문제는 apt-get 등 기본 패키지 관리 도구를 사용하여 생각 없이 설치하면 기존에 운영하던 k8s 클러스터 버전과 맞지 않은 최신 버전이 설치 된다는 것입니다\u0026hellip;(그러면 저처럼 작업을 2번 하게 됩니다)\n그런데 바이너리 파일을 사용해서 설치하기는 또 귀찮고..\n따라서 apt를 사용하되, 버전을 옵션으로 주는 방식으로 설치를 하기로 했습니다.\n나중에 또 2번 작업하지 않기 위해서, 그리고 중간에 발생한 이슈도 기록해둘겸 내용을 정리하였습니다.\n설치하기 사실 기존 k8s 클러스터 설치 프로세스와 다른 점은 거의 없습니다. 기존의 프로세스는 여기를 참고 바랍니다.\n특정 버전 설치 옵션을 주는 부분만 신경써서 진행하면 됩니다.\n# cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl // 패키지가 자동으로 업그레이드 되지 않도록 설정 sudo apt-mark hold kubelet kubeadm kubectl  원래는 아래와 같이 그냥 최신 버전을 설치했다면 이번엔 옵션으로 버전을 줘야 합니다. sudo apt-get install -y kubelet kubeadm kubectl  운영 중인 클러스터의 버전 확인하기 먼저 기존에 운영 중인 k8s 클러스터의 버전을 확인합니다.\nroot@hci-k8s-master-01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-worker-01 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-worker-02 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-master-01 Ready master 76d v1.18.6  클러스터의 node들은 1.18.x 버전을 사용하는 것을 알 수 있습니다.\n버전을 확인했으니 이제 설치를 진행합니다.\n설치 가능한 버전 확인하기 저는 sudo apt-get install -y kubelet=1.18.8 이렇게 옵션을 주고 설치하려고 했는데, 애석하게도 실패했습니다.\n아래와 같은 로그가 발생합니다.\n# sudo apt-get install kubelet=1.18.8 Reading package lists... Done Building dependency tree Reading state information... Done E: Version '1.18.8' for 'kubelet' was not found  정확한 버전 옵션을 확인해보도록 합니다.\n# apt-cache madison kubeadm  출력 결과에 제가 사용하려던 1.18.8 버전은 다음과 같이 나와있습니다.\nkubeadm | 1.18.8-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages  1.18.8-00 버전으로 설치하기 이제 다시 설치를 진행해봅니다.\n원래 kubeadm만 설치해도 kubectl과 kubelet이 의존적으로 설치됩니다.\n그런데 말입니다\u0026hellip; 특이점이 발생합니다.\n# sudo apt-get install kubeadm=1.18.8-00 Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: kubectl The following NEW packages will be installed: kubeadm kubectl 0 upgraded, 2 newly installed, 0 to remove and 5 not upgraded. Need to get 0 B/16.5 MB of archives. After this operation, 82.8 MB of additional disk space will be used. Do you want to continue? [Y/n] Y Selecting previously unselected package kubectl. (Reading database ... 128167 files and directories currently installed.) Preparing to unpack .../kubectl_1.19.2-00_amd64.deb ... Unpacking kubectl (1.19.2-00) ... Selecting previously unselected package kubeadm. Preparing to unpack .../kubeadm_1.18.8-00_amd64.deb ... Unpacking kubeadm (1.18.8-00) ... Setting up kubectl (1.19.2-00) ... Setting up kubeadm (1.18.8-00) ...  위와 같이 kubectl이 1.19.2-00 버전으로 설치가 됩니다.\n이런 경우, kubectl을 downgrade 해주는 방법도 있지만..\n# apt-get install kubectl=1.18.8-00  애초에 처음부터 모두 버전을 지정해주면 됩니다.\n# sudo apt-get install -y kubelet=1.18.8-00 kubeadm=1.18.8-00 kubectl=1.18.8-00  Master에 join하기 참고와 동일하게 진행합니다.\n","id":4,"section":"posts","summary":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다. 문제는 apt-get 등 기본 패키지 관리 도구를 사용하","tags":["kubernetes"],"title":"Kuberenets 특정 버전으로 설치하기","uri":"https://healinyoon.github.io/2020/10/20201009_k8s_install_specific_version/","year":"2020"},{"content":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다.\n$ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $ cd azure-cli-with-python3 $ virtualenv -p /usr/bin/python3.5. $ source ./bin/activate $ sudo apt install python3-dev $ pip install azure-cli $ python --version  ref)  The way to configure Azure CLI to use Python 3.5 on system where the default Python version is 2.x · Issue #2529 · Azure/azure-cli  ","id":5,"section":"posts","summary":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다. $ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $","tags":["azure"],"title":"Azure CLI 설치하기 with virtualenv","uri":"https://healinyoon.github.io/2020/09/20201023_azure_cli_with_virtualenv/","year":"2020"},{"content":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다.\n구성 H/W 구성하려는 kubernetes cluster는 다음과 같습니다.\n   노드 vCPU RAM Disk     master01 2 8GiB    worker01 2 8GiB    worker02 2 8GiB     Required ports 1) Control-plane node(s)\n2) Worker node(s)\nDocker 설치(모든 node) # curl -fsSL https://get.docker.com/ | sudo sh # systemctl start docker # systemctl enable docker  Kubernetes 클러스터 구성 쿠버네티스 공식 사이트 kubeam 설치\n모든 노드에 아래의 패키지를 설치한다.\n kubeadm: 클러스터를 부트스트랩하는 명령(쿠버네티스 관리) kubelet: 클러스터의 모든 시스템에서 실행되는 구성 요소로, 포트 및 컨테이너 시작과 같은 작업을 수행(쿠버네티스 서비스) kubectl: 클러스터와 통신하기 위한 command line util(쿠버네티스 클라이언트 프로그램, 클러스터 구성과는 전혀 상관 없음)  1) Kubernetes 리포지토리 구성(모든 node) # sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -  2) Kubeadm, Kubelet, Kubectl 설치(모든 node) # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl // 패키지가 자동으로 업그레이드 되지 않도록 설정 sudo apt-mark hold kubelet kubeadm kubectl // 데몬 재시작 systemctl daemon-reload systemctl restart kubelet  3) hostname 등록(모든 node) # sudo hostnamectl set-hostname master01 또는 # sudo hostnamectl set-hostname worker01  4) /etc/hosts 파일 수정(모든 node) # vi /etc/hosts 아래에 추가 {IP} master01 {IP} worker01 {IP} worker02  5) Iptables 설정(모든 node) 브릿지 되어있는 IPv4 트래픽을 iptables 체인으로 전달될 수 있도록 한다.\n# cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF # sudo sysctl --system  6) 스왑 기능 비활성화(모든 node) swap 끄기 # sudo swapoff -a 재부팅 후에도 swap 설정 유지 # sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab  7) 마스터 노드 초기화 # kubeadm init  kubeadm init 명령어 실행시 아래와 같이 출력된다.\nTo start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 아래 명령어를 수행하고\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config  worker node에서 아래의 명령어를 수행하여 master node와 join한다.\nkubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 kubectl get nodes 명령어를 입력하면 다음과 같이 출력된다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 NotReady master 10m v1.19.0 healin-k8s-worker01 NotReady \u0026lt;none\u0026gt; 30s v1.19.0 healin-k8s-worker02 NotReady \u0026lt;none\u0026gt; 29s v1.19.0  8) 네트워크 애플리케이션 설치 pod 네트워크 애플리케이션을 설치해야 클러스터 내의 node간 통신이 가능하다.\n사용 가능한 네트워크 옵션은 여기에서 확인할 수 있다.\n다음 명령을 master node에서 수행하여 weave pod 네트워크 애플리케이션을 설치한다.\nkubectl apply -f \u0026quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\u0026quot; 이런 오류 발생시 The connection to the server localhost:8080 was refused - did you specify the right host or port? export KUBECONFIG=/etc/kubernetes/admin.conf 을 적용해보자  master node에서 kubectl get nodes 명령어를 잠시 후 다시 입력하면 다음과 같이 STATUS가 NotReady =\u0026gt; Ready로 변경된 것을 확인할 수 있다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 Ready master 17m v1.19.0 healin-k8s-worker01 Ready \u0026lt;none\u0026gt; 7m42s v1.19.0 healin-k8s-worker02 Ready \u0026lt;none\u0026gt; 7m41s v1.19.0  9) master node를 worker node로도 사용하고 싶다면, 쿠버네티스 클러스터의 control-plane 노드는 보안상의 이유로 격리되어 있다(기본값).\nmaster node에서는 pod 가 스케줄링 되지 않으므로, 1대의 머신으로만 쿠버네티스 클러스터를 구축할 경우 격리 해제해야 한다.\n$ kubectl taint nodes –all node-role.kubernetes.io/master-  Control plane node isolation에 대한 자세한 내용은 아래 경로를 참고한다.\n* 쿠버네티스 공식 문서 - Control plane node isolation\n* Kubernetes Control-Plane Node에 Pod 띄울수 있는 방법 (Taints)\n참고 쿠버네티스(kubernetes) 설치 및 환경 구성하기\n","id":6,"section":"posts","summary":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다. 구성 H/W 구성하","tags":["docker","kubernetes"],"title":"Kubernetes Cluster 설치하기(ubuntu18.04)","uri":"https://healinyoon.github.io/2020/09/20200828_install_kubernetes_cluster_ubuntu/","year":"2020"},{"content":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지\n공식 사이트의 설명: Installing a Pod network add-on\n","id":7,"section":"posts","summary":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지 공식 사이트의 설명: Installing a Pod network add-on","tags":["kubernetes"],"title":"쿠버네티스 POD Network","uri":"https://healinyoon.github.io/2020/09/%EB%AF%B8%EC%99%8420200901_k8s_pod_network/","year":"2020"},{"content":"목적 Jenkins node heap memory 사이즈 변경 방법 정리\n참고 자료  Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양  Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인수로 받을 수 있음\nJenkins node heap memory 사이즈를 변경하기 위한 인수: -Xmx와 -Xms\n-Xms\u0026lt;size\u0026gt; set initial Java heap size -Xmx\u0026lt;size\u0026gt; set maximum Java heap size  예시 1) master의 /etc/default/jenkins에서 설정 AVA_ARGS=\u0026quot;-Xmx256m\u0026quot; # default value JAVA_ARGS=\u0026quot;-Xmx2048m\u0026quot; # 2048MB size  2) Jenkins UI에서 slave 설정 [Jenkins 관리] \u0026gt; [노드 관리] \u0026gt; \u0026lsquo;노드 선택 후 ' [설정] \u0026gt; [고급] Jenkins 권장 사양(자세히) 참고  It is recommended to define the same value for both -Xms and -Xmx so that the memory is allocated on startup rather than runtime. 대/소문자는 상관 없음(예: -Xmx10G는 -Xmx10g와 동일함) Java processes 전역에 적용하고 싶으면 JAVA_TOOL_OPTIONS 환경 변수 사용(예: export JAVA_TOOL_OPTIONS=\u0026quot;-Xmx6g\u0026quot;)  ","id":8,"section":"posts","summary":"목적 Jenkins node heap memory 사이즈 변경 방법 정리 참고 자료 Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양 Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인","tags":["jenkins"],"title":"Jenkins Node Heap Memory 사이즈 설정하기(-Xmx/-Xms 옵션)","uri":"https://healinyoon.github.io/2020/08/20200831_jenkins_node_heap_size/","year":"2020"},{"content":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다.\n 빌드 테스트를 위한 다양한 환경이 필요한 경우 크고 무거운 프로젝트의 작업 부하를 분산해야하는 경우  위의 요구사항을 충족하기 위해 Jenknis 분산 아키텍처인 Master-Slave 구성이 도입되었습니다. Jenkins Master와 Slave의 역할    구분 역할     Master * Build 작업 예약* Build 실행을 위한 작업 분배* Slave node 모니터링(필요에 따라 on/offline 전환 가능)* Build 결과 기록   Slave * Jenkins Master의 요청 수신* Build 실행    Jenkins Master-Slave 연동 방법 Jenkins Master-Slave는 다음의 요구사항을 만족시키면 매우 간단하게 구성할 수 있습니다.\n Master 서버에서 Slave 서버에 접근 가능하도록 설정 Slave 서버의 Jenkins Java 요구사항 충족  1. Jenkins 사용자 생성(모든 Slave node에서 진행) # adduser jenkins 사용자 생성 후 다음과 같이 홈디렉토리의 권한을 변경해준다 # chown ldccai:jenkins /home/jenkins # chmod 775 /home/jenkins  2. Java 8 설치(모든 Slave node에서 진행) # apt-get install openjdk-8-jdk  3. Jenkins에서 Slave node 등록 등록된 노드의 [로그]를 확인하면 다음과 같이 Master 서버와 잘 연동된 것을 확인할 수 있습니다. ","id":9,"section":"posts","summary":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다. 빌드 테스트를 위한 다양한 환경이 필","tags":["jenkins"],"title":"Jenkins Master-Slave 구성하기","uri":"https://healinyoon.github.io/2020/08/20200827_jenkins_master_slave/","year":"2020"},{"content":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미합니다.\nCI(Continuous Integration): 지속적 통합 개발자의 변경 사항이 정기적으로(최상의 경우 하루 여러번) 빌드 및 테스트 되고 공유 리포지토리에 병합되는 프로세스입니다.\nCD(Continous Deploy or ontinuous Delivery): 지속적 배포 Jez Humble의 정의\nContinuous Deployment is about automating the release of a good build to the production environment. In fact, Humble thinks it might be more accurate to call it “continuous release.”\nContinuous Delivery is about 1) ensuring that every good build is potentially ready for production release. At the very least, 2) it’s sent to the user acceptance test (UAT) environment. 3) Your business team can then decide when a successful build in UAT can be deployed to production —and they can do so at the push of a button.\n상황에 따라 이미 운영되고 있는 프로덕션 환경에 바로 release 하는 것은 문제가 될 수 있습니다. 이러한 경우로 임베디드 소프트웨어 등이 해당됩니다.\n따라서 잠재적으로는 release 가능하지만, 프로덕션 환경에 자동으로 release 되지 않는 것이 Continuous Delivery 입니다. 정리  CI: 지속적인 빌드 / 테스트 / 통합 CD: CI의 연장선 ~ Release 준비 완료(Delivery) or 제품 출시(Deploy)   Jenkins for CI/CD 다양한 CI/CD tool CI/CD 구현을 위한 다양한 tool이 있습니다. tool에 대한 자세한 정보 참고\n왜 Jenkins인가 Jenkins는 아래와 같은 강점을 가지고 있습니다.\n It is an open-source tool with great community support. It is easy to install. It has 1000+ plugins to ease your work. If a plugin does not exist, you can code it and share it with the community. It is free of cost. It is built with Java and hence, it is portable to all the major platforms.  하지만 단점 역시 존재합니다. CI/CD tool 중에서 자주 사용되는 Jenkins vs Gitlab vs Travis 비교 글을 참고하시면 Jenkins의 장단점을 이해하는데 도움이 됩니다.\n","id":10,"section":"posts","summary":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미","tags":["jenkins"],"title":"CI/CD와 Jenkins","uri":"https://healinyoon.github.io/2020/08/20200827_cicd_and_jenkins/","year":"2020"},{"content":"HUGO 글 생성하기 $ hugo new {파일명}  HUGO 블로그 빌드 $ hugo -t {테마명}  Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master  ","id":11,"section":"posts","summary":"HUGO 글 생성하기 $ hugo new {파일명} HUGO 블로그 빌드 $ hugo -t {테마명} Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메","tags":["go","hugo"],"title":"HUGO 블로그 새글 업로드하기","uri":"https://healinyoon.github.io/2020/08/20200827_hugo_blog/","year":"2020"},{"content":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부한 내용을 정리하였다.\n 1. 쉘(shell) 이란? 쉘은 사용자가 입력한 명령을 해석해서 커널에 전달하거나, 커널의 처리 결과를 사용자에게 전달하는 역할을 한다.\n1-1. 쉘 명령문 형식 쉘 명령문의 형식은 다음과 같다.\n{프롬프트} {명령어} {옵션} {인자} ex) # cd ~ ex) # ls -al ex) # rm -rf testdir ex) # cat test.txt  1-2. 쉘 환경변수 쉘은 다양한 환경변수 값을 설정할 수 있는데, 리눅스의 주요 환경변수 목록은 아래와 같다.\n   환경변수 설명     HOME 현재 사용자의 홈디렉토리   PATH 실행 파일을 찾는 디렉토리 경로   LANG 기본 지원 언어   PWD 사용자의 현재 작업 디렉토리   TERM 로그인 터미널 타입   SHELL 로그인시 사용하는 쉘   USER 현재 사용자의 이름   DISPLAY X 디스플레이 이름   HOSTNAME 호스트 이름   USERNAME 현재 사용자 이름   OSTYPE 운영체제 타입    현재 설정된 환경 변수는 echo ${환경변수 이름} 명령어를 통해 확인할 수 있다.\nex) echo $HOME ex) echo $HOSTNAME  환경변수 설정 값을 변경하려면 export {환경변수}={값} 명령어를 실행하면 된다. 그 외의 환경변수는 printenv 명령어로 확인할 수 있다. 단, 일부 환경변수는 printenv 명령을 실행해도 출력되지 않는다.\n 2. 쉘 스크립트 사용해보기 쉘 스크립트도 일반적인 프로그래밍 언어와 비슷하게 변수, 반복문, 제어문 등을 사용할 수 있다.\n2-1. 쉘 스크립트 작성하기 아래와 같이 간단한 쉘 스크립트를 작성하고 script.sh으로 저장해보자.\n#!/bin/sh echo \u0026quot;User Name: \u0026quot; $USER echo \u0026quot;User Home Directory: \u0026quot; $HOME exit 0  위의 코드를 간단히 설명하면 다음과 같다.\n 1행: bash를 사용하겠다는 의미로, 쉘 스크립트 작성시 첫 행에 반드시 입력해야한다. 2, 3행: echo 명령은 화면에 출력하는 명령이며, 먼저 문자를 출력하고 ${환경변수}에 해당하는 값을 출력한다. 4행: 종료 코드를 반환하며, 0은 쉘 스크립트 실행 성공을 의미한다.  2-2. 쉘 스크립트 실행하기 쉘 스크립트를 실행하는 방법에는 크게 두 가지가 있다.\n1) sh 명령으로 실행\nsh {실행파일 이름} ex) sh script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  2) 파일 권한을 실행 가능하게 변경하고 실행\n# chmod +x {실행파일 이름} # ./{실행파일 경로} ex) # chmod +x script.sh # ./script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  3. 변수 쉘 스크립트 변수 사용시 주의해야할 특징은 다음과 같다.\n 변수를 사용하기 전에 미리 선언하지 않으며, 처음 변수에 값이 할당되면 자동으로 변수가 생성된다. 변수에 넣는 모든 값을 \u0026lsquo;문자열\u0026rsquo;로 취급한다. 변수 이름의 대소문자를 구분한다. 변수를 대입할 때 \u0026lsquo;=\u0026rsquo; 좌우에 공백이 없어야 한다.  3-1. 변수의 입출력 \u0026lsquo;$\u0026rsquo; 라는 문자가 들어간 글자를 출력하려면 \u0026lsquo;\u0026lsquo;로 묶어주거나 앞에 \u0026lsquo;\\\u0026lsquo;를 붙여줘야 한다.\n#!/bin/sh var=\u0026quot;Hi Shell\u0026quot; echo $var echo '$var' echo \\$var exit 0 [실행 결과] Hi Shell $var $var  변수의 입력과 출력은 다음과 같이 사용할 수 있다.\n#!/bin/sh echo \u0026quot;값을 입력하세요: \u0026quot; read var echo \u0026quot;입력된 값은 \u0026quot; $var \u0026quot;입니다.\u0026quot; exit 0 [실행 결과] 값을 입력하세요: shell script study 입력된 값은 shell script study 입니다.  3-2. 숫자 계산 변수에 넣은 값은 모두 문자열이 되므로, shell script에서 연산을 하기 위해서는 expr 키워드를 사용해야한다. shell script에서 숫자 계산을 하기 위한 규칙은 다음과 같다.\n 수식과 함께 역따옴표로 묶어줘야 한다. 수식에 괄호를 사용하기 위해서는 그 앞에 역슬래시를 붙여줘야 한다. 곱하기 기호를 사용할 때도 그 앞에 역슬래시를 붙여줘야 한다.  #!/bin/sh n1=10 n2=$n1+20 echo $n2 n3=`expr $n1 + 20` echo $n3 n4=`expr \\( $n1 + 20 \\) / 10 \\* 2` echo $n4 exit 0 [실행 결과] 10+20 30 // ※ 각 단어를 띄어쓰기해야하는 것에 주의 6  3-3. 파라미터 변수 파라미터 변수는 $0, $1, $2 등의 형태를 가지며, 아래와 같이 매칭된다.\n   명령어 파라미터 1 파라미터 2 파라미터 n\u0026hellip;     $0 $1 $2 $n\u0026hellip;    예를 들면 다음과 같다.\n   ./script 1 2     $0 $1 $2    #!/bin/sh echo \u0026quot;실행파일 이름: \u0026lt;$0\u0026gt;\u0026quot; echo \u0026quot;첫 번째 파라미터: \u0026lt;$1\u0026gt;.\u0026quot; echo \u0026quot;두 번째 파라미터: \u0026lt;$2\u0026gt;.\u0026quot; echo \u0026quot;전체 파라미터는 \u0026lt;$*\u0026gt;이다.\u0026quot; exit 0 [실행 결과] 실행파일 이름: \u0026lt;./script.sh\u0026gt; 첫 번째 파라미터: \u0026lt;1\u0026gt;. 두 번째 파라미터: \u0026lt;2\u0026gt;. 전체 파라미터는 \u0026lt;1 2\u0026gt;이다.  4. if문 if문의 기본 사용법과 조건 연산자를 알아보자.\n4-1. if문 기본 문법 if문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 fi  이때 주의할 점은 [ 조건 ] 에서 각 단어에 모두 공백이 있어야 한다.\n#!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;rin_gu\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; fi exit 0 [실행 결과] 참 입니다.  4-2. if-else문 if-else문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 else문 거짓인 경우 실행 fi  #!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;shell script\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; else echo \u0026quot;거짓 입니다.\u0026quot; fi exit 0 [실행 결과] 거짓 입니다.  4-3. 조건문에 사용되는 비교 연산자 조건문에 사용디는 비교 연산자에는 문자열 비교 연산자와 산술 비교 연산자가 있다.\n문자열 비교 연산자\n   문자열 비교 내용 예시     = 같으면 참 \u0026ldquo;문자열\u0026rdquo; = \u0026ldquo;문자열\u0026rdquo;   != 같지 않으면 참 \u0026ldquo;문자열\u0026rdquo; != \u0026ldquo;문자열\u0026rdquo;   -n 문자열이 NULL 값이 아니면 참 -n \u0026ldquo;문자열\u0026rdquo;   -z 문자열이 NULL 값이면 참 -z \u0026ldquo;문자열\u0026rdquo;    산술 비교 연산자\n   산술 비교 내용 예시     -eq 두 수식이 같으면 참 수식 -eq 수식   -ne 두 수식이 같지 않으면 참 수식 -ne 수식   -gt 왼쪽 수식이 크면 참 수식 -gt 수식   -ge 왼쪽 수식이 크거나 같으면 참 수식 -ge 수식   -lt 왼쪽 수식이 작으면 참 수식 -lt 수식   -le 왼쪽 수식이 작거나 같으면 참 수식 -le 수식   ! 수식이 거짓이라면 참 !수식    #!/bin/sh if [ 1024 -eq 2024 ] then echo \u0026quot;1024과 2024는 같다.\u0026quot; else echo \u0026quot;1024과 2024는 다르다.\u0026quot; fi exit 0 [실행 결과] 1024과 2024는 다르다.  4-4. 조건문에 사용되는 파일과 관련된 연산자 if문에서 파일을 처리하는 조건 연산자는 다음과 같다.\n파일 조건 | 내용 | 예시 \u0026mdash; | \u0026mdash; -d | 파일이 디렉토리면 참 | -d 파일명 -e | 파일이 존재하면 참 | -e 파일명 -f | 파일이 일반 파일이면 참 | -f 파일명 -g | 파일에 set-group-id가 설정되면 참 | -g 파일명 -r | 파일이 읽기 가능하면 참 | -r 파일명 -s | 파일 크기가 0이 아니면 참 | -s 파일명 -u | 파일에 set-user-id가 설정되면 참 | -u 파일명 -w | 파일이 쓰기 가능하면 참 | -w 파일명 -x | 파일이 실행 가능하면 참 | -x 파일명\n 5. case문 case문의 기본 사용법을 알아보자\n5-1. case문 기본 문법 case문의 기본 문법은 다음과 같다.\ncase [ 변수 ] in 변수값1) 변수와 변수값1이 일치할 경우 실행 변수값2) 변수와 변수값2가 일치할 경우 실행 ... esac  5-2. case 문 사용해보기 (작성중)\n","id":12,"section":"posts","summary":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부","tags":["shell"],"title":"Shell Scripts 사용하기","uri":"https://healinyoon.github.io/2019/06/20190602_shell_script/","year":"2019"},{"content":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다.\n 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry   2. Docker 실행 및 정보 확인 2-1. Docker 실행하기 # 시스템 부팅 시 docker를 시작하도록 설정 systemctl enable docker.service # Docker 실행 systemctl start docker.service # Docker 상태 확인 systemctl status docker.service  2-2. Docker 명령어 사용하기 Docker 명령어의 기본 형식은 docker {명령어} 입니다.\n# Docker 버전 확인 docker version # Docker 실행 환경 확인 docker system info # Docker 디스크 상태 확인 docker system df # 그 외 Docker 명령어 살펴보기 docker -help   3. Docker 사용해보기 Docker가 정상적으로 동작하는지 확인하기 위해 컨테이너를 실행합니다.\n3-1. Docker 컨테이너 실행하기 # Docker 컨테이너 실행 명령어 docker run {옵션} {컨테이너 명/ID}  docker run 명령어는 컨테이너를 생성하고 실행시키는 명령어 입니다. 이때 dockers는 로컬에 해당 이미지가 있는지 학인하고, 없는 경우 docker hub에서 pull을 먼저 진행하고 컨테이너를 생성합니다.\n3-2. Hello, world! docker run hello-world  hello-world 컨테이너가 실행되면 다음과 같이 메세지가 출력됩니다.\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.  3-3. Nginx 이번에는 Nginx 웹서버를 docker로 설치해보겠습니다.\n# Nginx 이미지 다운로드 docker pull nginx # 다운로드한 이미지 확인 docker images # Nginx 컨테이너 실행 docker run --name nginx-webserver -d -p 80:80 nginx  docker run 명령어에서 자주 쓰이는 옵션은 다음과 같습니다.\n \u0026ndash;name: 컨테이너의 이름을 설정한다. -d: 컨테이너를 백그라운드에서 실행시킨다. -p: 호스트 포트와 컨테이너 내부의 포트를 매핑한다(형식: -p {host 포트}:{컨테이너 포트}).  실행중인 docker 컨테이너를 확인하고 싶을 때는 docker ps 명령어로 확인 할 수 있습니다.\n# 실행중인 컨테이너 확인 docker ps # 컨테이너 상태 확인 docker container stats  마지막으로 http://localhost:80으로 접속해서 Nginx 웹 브라우저를 확인합니다.\n 참고 사이트  https://niceman.tistory.com/36 https://futurecreator.github.io/2018/11/16/docker-container-basics/ http://pyrasis.com/Docker/Docker-HOWTO  ","id":13,"section":"posts","summary":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다. 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry 2. Docker 실행 및","tags":["docker"],"title":"Docker 설치하기(CentOS)","uri":"https://healinyoon.github.io/2019/06/20190611_docker_install/","year":"2019"}],"tags":[{"title":"azure","uri":"https://healinyoon.github.io/tags/azure/"},{"title":"docker","uri":"https://healinyoon.github.io/tags/docker/"},{"title":"go","uri":"https://healinyoon.github.io/tags/go/"},{"title":"helm","uri":"https://healinyoon.github.io/tags/helm/"},{"title":"hugo","uri":"https://healinyoon.github.io/tags/hugo/"},{"title":"jenkins","uri":"https://healinyoon.github.io/tags/jenkins/"},{"title":"kubernetes","uri":"https://healinyoon.github.io/tags/kubernetes/"},{"title":"shell","uri":"https://healinyoon.github.io/tags/shell/"}]}