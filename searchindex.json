{"categories":[{"title":"blog","uri":"https://healinyoon.github.io/categories/blog/"},{"title":"ci/cd","uri":"https://healinyoon.github.io/categories/ci/cd/"},{"title":"cloud","uri":"https://healinyoon.github.io/categories/cloud/"},{"title":"devops","uri":"https://healinyoon.github.io/categories/devops/"},{"title":"gpu","uri":"https://healinyoon.github.io/categories/gpu/"},{"title":"jvm","uri":"https://healinyoon.github.io/categories/jvm/"},{"title":"linux","uri":"https://healinyoon.github.io/categories/linux/"},{"title":"msa","uri":"https://healinyoon.github.io/categories/msa/"},{"title":"networks","uri":"https://healinyoon.github.io/categories/networks/"},{"title":"programming","uri":"https://healinyoon.github.io/categories/programming/"}],"posts":[{"content":"이슈 Jenkins Kubernetes 플러그인을 사용하여 Jenkins Slave Pod를 배포하려고 하는 데 아래와 같은 에러가 계속 발생했다.\n{pod name} is offline 또는 Pod 재시작 무한 반복 Still waiting to schedule task \u0026ldquo;Jenkins doesn\u0026rsquo;t have label {pod name} Pod가 배포된 노드에서 docker logs {container name} 명령어로 컨테이너 로그를 확인해보니 다음과 같은 내용이 출력되었다.\n# docker logs 53e57b2202c7 Dec 16, 2020 8:32:45 AM hudson.remoting.jnlp.Main createEngine INFO: Setting up agent: k8s-pipeline-chatbot-pipeline-24-c8mmg-r47mn-mgpnd Dec 16, 2020 8:32:45 AM hudson.remoting.jnlp.Main$CuiListener \u0026lt;init\u0026gt; INFO: Jenkins agent is running in headless mode. Dec 16, 2020 8:32:45 AM hudson.remoting.Engine startEngine INFO: Using Remoting version: 4.3 Dec 16, 2020 8:32:45 AM org.jenkinsci.remoting.engine.WorkDirManager initializeWorkDir INFO: Using /home/jenkins/agent/remoting as a remoting work directory Dec 16, 2020 8:32:45 AM org.jenkinsci.remoting.engine.WorkDirManager setupLogging INFO: Both error and output logs will be printed to /home/jenkins/agent/remoting Dec 16, 2020 8:32:46 AM hudson.remoting.jnlp.Main$CuiListener status INFO: Locating server among [http://x.x.x.x:31122/] Dec 16, 2020 8:32:46 AM org.jenkinsci.remoting.engine.JnlpAgentEndpointResolver resolve INFO: Remoting server accepts the following protocols: [JNLP4-connect, Ping] Dec 16, 2020 8:32:46 AM org.jenkinsci.remoting.engine.JnlpAgentEndpointResolver isPortVisible WARNING: Connection refused (Connection refused) Dec 16, 2020 8:32:46 AM hudson.remoting.jnlp.Main$CuiListener error SEVERE: http://x.x.x.x:31122/ provided port:50000 is not reachable java.io.IOException: http://x.x.x.x:31122/ provided port:50000 is not reachable at org.jenkinsci.remoting.engine.JnlpAgentEndpointResolver.resolve(JnlpAgentEndpointResolver.java:314) at hudson.remoting.Engine.innerRun(Engine.java:693) at hudson.remoting.Engine.run(Engine.java:518)  SEVERE: http://x.x.x.x:31122/ provided port:50000 is not reachable = 즉 jenkins agent에 연결이 안된다는 것..\n해결 방법 [Jenkins 관리] \u0026gt; [시스템 설정] \u0026gt; [Cloud(The cloud configuration has moves to a separate configuration page)]에 접근하여 아래와 같이 설정해준다.\n중요한 점은 jenkins-agent pod는 아래와 같이 ClusterIP serviceType으로 배포되어 있기 때문에 coreDNS로 Jenkins tunnel을 입력해줘야 한다는 것이다!! coreDNS 형식은 {svc name}.{namespace}.svc.cluster.local:50000\n# kubectl get svc -n jenkins -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR jenkins NodePort 10.107.89.164 \u0026lt;none\u0026gt; 8080:31122/TCP 6d2h app.kubernetes.io/component=jenkins-controller,app.kubernetes.io/instance=jenkins jenkins-agent ClusterIP 10.109.207.50 \u0026lt;none\u0026gt; 50000/TCP 6d2h app.kubernetes.io/component=jenkins-controller,app.kubernetes.io/instance=jenkins   Jenkins tunnel: Connect to the specified host and port, instead of connecting directly to Jenkins. Useful when connection to Jenkins needs to be tunneled. Can be also HOST: or :PORT, in which case the missing portion will be auto-configured like the default behavior\n 이제 아름답게 잘된다..\n","id":0,"section":"posts","summary":"이슈 Jenkins Kubernetes 플러그인을 사용하여 Jenkins Slave Pod를 배포하려고 하는 데 아래와 같은 에러가 계속 발생했다. {pod name} is offline 또는 Pod 재시작 무한 반복 Still waiting to schedule task \u0026ldquo;Jenkins doesn\u0026rsquo;t have label {pod name} Pod","tags":["docker","kubernetes","troubleshooting"],"title":"Kubernets(쿠버네티스) Troubleshooting (4) - Jenkins Agent로 Slave Pod가 뜨지 않고 계속 죽을 때","uri":"https://healinyoon.github.io/2020/12/20201216_k8s_troubleshooting_4/","year":"2020"},{"content":"Intro Kubernetes CI/CD 구축기 (1) - Kubernetes CI 구축하기\n목차  Helm으로 Jenkins를 Kubernetes에 배포하기 Kubernetes와 Docker Private Resistry 연동하기 Gitlab과 Jenkins를 사용하여 Kubernetes 애플리케이션 배포하기  Helm으로 Jenkins를 Kubernetes에 배포하기 Persistence Volume을 위한 NFS 서버 구성  참고: https://sarc.io/index.php/os/1780-ubuntu-nfs-configuration  이 포스트는 NFS을 {NFS server}:/mnt/nfs_server와 {NFS client(k8s cluster)}:/mnt/nfs_client 경로를 마운트하여 구성하였다.\nPersistence Volume 생성 1. Persistent Volume으로 마운트할 디렉토리 생성 # mkdir /mnt/nfs_server/pv_jenkins # chmod 777 /mnt/nfs_server/pv_jenkins  2. Persistent Volume YAML 작성  jenkins-pv.yaml\n apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-pv spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: \u0026quot;\u0026quot; nfs: server: {NFS server IP} path: /mnt/nfs_server/pv_jenkins  3. Persistent Volume 생성 및 확인 # kubectl create -f jenkins-pv.yaml persistentvolume/jenkins-pv created # kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE jenkins-pv 10Gi RWO Retain Available  Jenkins Helm Repo 추가 stable/jenkins는 DEPRECATED되었기 때문에 jenkins/jenkins를 사용해야 한다.\n1. 해당 Repository 접근 https://github.com/jenkinsci/helm-charts\n2. Helm Chart Repository 추가 및 조회 # helm repo add jenkins https://charts.jenkins.io \u0026quot;jenkins\u0026quot; has been added to your repositories # helm search repo jenkins NAME CHART VERSION\tAPP VERSION\tDESCRIPTION jenkins/jenkins\t3.0.2 2.249.3 Jenkins - Build great things at any scale! The ...  jenkins/jenkins 가 조회되는 것을 확인할 수 있다.\nJenkins Helm Chart 커스터마이징 1. Jenkins Helm Chart Values 확인 Jenkins helm chart의 어떤 옵션이 구성 가능한지 helm show values {helm repo}/{application name} 명령어로 확인한다. 상당히 길게 출력되는데, 우리에게 필요한 옵션을 찾아 적용하면 된다.\n# helm show values jenkins/jenkins # Default values for jenkins. # This is a YAML-formatted file. # Declare name/value pairs to be passed into your templates. # name: value ## Overrides for generated resource names # See templates/_helpers.tpl # nameOverride: # fullnameOverride: # namespaceOverride: # For FQDN resolving of the master service. Change this value to match your existing configuration. # ref: https://github.com/kubernetes/dns/blob/master/docs/specification.md clusterZone: \u0026quot;cluster.local\u0026quot; master: httpsKeyStore: jenkinsHttpsJksSecretName: '' enable: false httpPort: 8081 path: \u0026quot;/var/jenkins_keystore\u0026quot; fileName: \u0026quot;keystore.jks\u0026quot; password: \u0026quot;password\u0026quot; (중략)  2. Values 파일 생성 # helm show values jenkins/jenkins \u0026gt; jenkins-values.yaml  3. Values 파일 커스터마이징 위에서 생성한 jenkins-values.yaml 파일을 커스터마이징할 옵션은 다음과 같다.\n admin 패스워드 변경  변경 전 # adminPassword: \u0026lt;defaults to random\u0026gt; 변경 후 adminPassword: {password}   controller.serviceType 변경  Service Type을 NodePort로 변경\n변경 전 serviceType: ClusterIP 변경 후 serviceType: NodePort nodePort: {port}   persistence.storageClass.size 변경  위에서 생성한 pv를 사용하기 위해 동일한 조건 기재\n변경 전 persistence: enabled: true existingClaim: storageClass: annotations: {} accessMode: \u0026quot;ReadWriteOnce\u0026quot; size: \u0026quot;8Gi\u0026quot; 변경 후 persistence: enabled: true existingClaim: storageClass: annotations: {} accessMode: \u0026quot;ReadWriteOnce\u0026quot; size: \u0026quot;10Gi\u0026quot;   nodeSelector 지정  Jenkins Master는 특정 Node에서 배포되도록 Node Selector 지정\n변경 전 nodeSelector: {} 변경 후 nodeSelector: kubernetes.io/hostname: \u0026quot;gs-hci-vm-cicd\u0026quot;   Plugin 목록 제거  컨테이너 초기화에서 플러그인 설치를 주석처리하는 이유는 (Pod STATUS가 Running되지 않고 Error 또는 CrashLoopBackOff을 반복할 때)[]를 참고한다.\n변경 전 installPlugins: - kubernetes:1.27.6 - workflow-aggregator:2.6 - git:4.4.5 - configuration-as-code:1.46 변경 후 installPlugins: # - kubernetes:1.27.6 # - workflow-aggregator:2.6 # - git:4.4.5 # - configuration-as-code:1.46  대신 필요한 플러그인을 설치하고 보안 설정도 해줘야 한다.\n kubernetes workflow-aggregator git configuration-as-code  Jenkins Helm Install 1. Jenkins Helm 설치 # helm install jenkins -f jenkins-values.yaml jenkins/jenkins --namespace=jenkins NAME: jenkins LAST DEPLOYED: Wed Dec 9 11:19:15 2020 NAMESPACE: jenkins STATUS: deployed REVISION: 1 NOTES: 1. Get your 'admin' user password by running: kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password \u0026amp;\u0026amp; echo 2. Get the Jenkins URL to visit by running these commands in the same shell: export NODE_PORT=$(kubectl get --namespace jenkins -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot; services jenkins) export NODE_IP=$(kubectl get nodes --namespace jenkins -o jsonpath=\u0026quot;{.items[0].status.addresses[0].address}\u0026quot;) echo http://$NODE_IP:$NODE_PORT/login 3. Login with the password from step 1 and the username: admin 4. Configure security realm and authorization strategy 5. Use Jenkins Configuration as Code by specifying configScripts in your values.yaml file, see documentation: http:///configuration-as-code and examples: https://github.com/jenkinsci/configuration-as-code-plugin/tree/master/demos For more information on running Jenkins on Kubernetes, visit: https://cloud.google.com/solutions/jenkins-on-container-engine For more information about Jenkins Configuration as Code, visit: https://jenkins.io/projects/jcasc/ NOTE: Consider using a custom image with pre-installed plugins  2. Helm 확인 # helm ls --namespace=jenkins NAME NAMESPACE\tREVISION\tUPDATED STATUS CHART APP VERSION jenkins\tdefault 1 2020-12-08 16:12:00.059225605 +0900 KST\tdeployed\tjenkins-3.0.22.249.3  3. Kubernetes Resource 조회 # kubectl get all -n jenkins NAME READY STATUS RESTARTS AGE pod/jenkins2-0 2/2 Running 0 6m20s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jenkins2 NodePort 10.109.131.192 \u0026lt;none\u0026gt; 8080:31133/TCP 178m service/jenkins2-agent ClusterIP 10.99.253.235 \u0026lt;none\u0026gt; 50000/TCP 178m NAME READY AGE statefulset.apps/jenkins2 1/1 178m  한참 기다리면 Init:0/1 → Running 으로 변경되는 것 확인\n3. 사이트 접속 확인: {k8s cluster IP}:31122 현재 계정 관련 플러그인이 설치되어 있지 않기 때문에 로그인하지 않고 접속하게 되어 있다.\n4. pv 정상 동작도 확인 # pwd /mnt/nfs_server/pv_jenkins # ls casc_configs identity.key.enc jenkins.telemetry.Correlator.xml plugins updates config.xml jenkins.install.InstallUtil.lastExecVersion jobs plugins.txt userContent copy_reference_file.log jenkins.install.UpgradeWizard.state logs secret.key users hudson.model.UpdateCenter.xml jenkins.model.JenkinsLocationConfiguration.xml nodeMonitors.xml secret.key.not-so-secret war hudson.plugins.git.GitTool.xml jenkins.security.apitoken.ApiTokenPropertyConfiguration.xml nodes secrets workflow-libs  Kubernetes와 Docker Private Resistry 연동하기 Docker Private Registry의 HTTP 통신을 위한 \u0026lsquo;Insecure\u0026rsquo; 설정 ※ 만약 Docker Private Registry가 HTTPS 통신을 한다면 건너뛰기 ※\nKubernetes worker node의 docker에 insecure-registries 옵션을 설정해준다.\n1. /etc/docker/daemon.json 파일 수정 # vi /etc/docker/daemon.json { \u0026quot;insecure-registries\u0026quot; : [\u0026quot;{docker private registry IP}:{port}\u0026quot;] }  2. Docker 재시작 위에서 PV를 제대로 설정해두었다면 Docker 재시작에도 문제가 없다.\n# systemctl restart docker  # kubectl get all -n jenkins -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/jenkins-0 0/2 Completed 2 4d1h 10.35.128.2 gs-hci-vm-cicd \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Kubernetes master에서 kubectl 명령어로 조회해보면 worker node에서 실행 중이던 pod가 Completed 되었다가 잠시후 Running 되는 것을 확인할 수 있다.\n# kubectl get all -n jenkins -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/jenkins-0 2/2 Running 4 4d1h 10.35.128.2 gs-hci-vm-cicd \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  3. Docker login 테스트 # docker login {docker private registry IP}:{port} Username: {account} Password: {password} WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded  Kubernetes와 Docker Private Registry 연동 Kubernetes에서 Private Registry를 사용하기 위해서는 Secret을 등록해야 한다(여기서는 Harbor로 구축한 Private Registry를 사용한다).\n1. Secret 생성 # kubectl create secret docker-registry {registry name} --docker-server={docker-registry-server} --docker-username={account} --docker-password={password} --docker-email={email}  2. 생성된 Secret 확인 # kubectl get secret {registry name} --output=\u0026quot;jsonpath={.data.\\.dockerconfigjson}\u0026quot; | base64 -d  3. Kubernetes와 Docker Private Registy 연동 확인 먼저 테스트에 사용할 image를 private registry에 push 하자. 저장소에 image를 push 하려면 docker tag 명령어로 저장소 주소와 프로젝트 명을 입력해야 한다.\n# docker tag {source image name}:{tag} {docker-registry-server}/{project}/{image name}:{tag}  tagging이 완료되면 image를 push 하자.\n# docker push {docker-registry-server}/{project}/{image name}:{tag}   Harbor에 image를 push하는 명령어를 얻는 꿀팁은 아래의 그림과 같이 [UI 페이지] \u0026gt;[PUSH COMMAND]를 클릭해서 얻을 수 있다.  마지막으로 push한 image를 사용하여 deployment를 실행해보자.\n http-go-deploy.yaml\n apiVersion: apps/v1 kind: Deployment metadata: labels: app: http-go name: http-go spec: replicas: 1 selector: matchLabels: app: http-go template: metadata: labels: app: http-go spec: containers: - image: {docker-registry-server}/{project}/{image name}:{tag} name: http-go ports: - containerPort: 80 imagePullSecrets: - name: ldccai nodeSelector: kubernetes.io/hostname: gs-hci-vm-cicd  위에서 작성한 YAML 파일을 실행하면 정상적으로 image를 pull하고 pod를 생성하는 것을 확인할 수 있다.\n# kubectl apply -f http-go-deploy.yaml # kubectl get pod NAME READY STATUS RESTARTS AGE http-go-7cdb5779f7-7jz2x 1/1 Running 0 22m # kubectl describe pod http-go-7cdb5779f7-7jz2x (중략) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned default/http-go-7cdb5779f7-7jz2x to gs-hci-vm-cicd Normal Pulling \u0026lt;invalid\u0026gt; kubelet, gs-hci-vm-cicd Pulling image \u0026quot;{docker-registry-server}/{project}/{image name}:{tag}\u0026quot; Normal Pulled \u0026lt;invalid\u0026gt; kubelet, gs-hci-vm-cicd Successfully pulled image \u0026quot;{docker-registry-server}/{project}/{image name}:{tag}\u0026quot; Normal Created \u0026lt;invalid\u0026gt; kubelet, gs-hci-vm-cicd Created container http-go Normal Started \u0026lt;invalid\u0026gt; kubelet, gs-hci-vm-cicd Started container http-go  Gitlab과 Jenkins를 사용하여 Kubernetes 애플리케이션 배포하기 Gitlab를 Jenkins와 연동하기 1. Jenkins Plugin 설치 아래의 3개의 플러그인을 추가로 설치한다.\n git pipeline kubernetes  2. Jenkins Kubernetes 플러그인 설정 여기서 정말 엄청 헤맸다 ㅠㅠ.\n[Jenkins 관리] \u0026gt; [시스템 설정] \u0026gt; [Cloud(The cloud configuration has moves to a separate configuration page)]에 접근하여 아래와 같이 설정해준다.\n중요한 점은 jenkins-agent pod는 아래와 같이 ClusterIP serviceType으로 배포되어 있기 때문에 coreDNS로 Jenkins tunnel을 입력해줘야 한다는 점이다!!\n# kubectl get svc -n jenkins -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR jenkins NodePort 10.107.89.164 \u0026lt;none\u0026gt; 8080:31122/TCP 6d2h app.kubernetes.io/component=jenkins-controller,app.kubernetes.io/instance=jenkins jenkins-agent ClusterIP 10.109.207.50 \u0026lt;none\u0026gt; 50000/TCP 6d2h app.kubernetes.io/component=jenkins-controller,app.kubernetes.io/instance=jenkins   Jenkins tunnel: Connect to the specified host and port, instead of connecting directly to Jenkins. Useful when connection to Jenkins needs to be tunneled. Can be also HOST: or :PORT, in which case the missing portion will be auto-configured like the default behavior\n 3. Jenkins Kubernetes 플러그인 테스트 CI를 위한 Jenkinsfile 작성하기 1. Jenkinsfile 작성 podTemplate( containers: [ containerTemplate(name: 'chatbot-container', image: 'docker', ttyEnabled: true, command: 'cat'), ]) { node(POD_LABEL) { stage('Get a chatbot project') { git branch: 'chatbot/pages-link', credentialsId: 'gitlab', url: 'https://ldccai.lotte.net/gitlab/newdeal-chatbot/chatbot-client.git' container('chatbot-container') { stage('Build a chatbot project') { sh 'docker login 10.231.238.220:31000 -uadmin -pHarbor12345' sh 'docker build -t 10.231.238.220:31000/library/test-chatbot .' sh 'docker push 10.231.238.220:31000/library/test-chatbot' } } } } }  참고 Helm으로 Jenkins를 Kubernetes에 배포하기  https://m.blog.naver.com/alice_k106/221562805601 https://velog.io/@hamon/Kubernetes-Cluster%EC%97%90-Jenkins-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0 https://mathieuherbert.github.io/kubernetes/jenkins/2018/01/04/jenkins-on-kubernetes.html  Kubernetes와 Docker Private Resistry 연동하기  https://lahuman.github.io/kubernetes-harbor/ https://hakurei.tistory.com/282 https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ https://blog.uniqbuild.co.kr/?p=724 https://support.cloudz.co.kr/ko/support/solutions/articles/42000042543-image-registry-public-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EA%B4%80%EB%A6%AC https://velog.io/@tkfrn4799/Harbor%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-Private-Docker-Registry-%EA%B5%AC%EC%B6%95 https://engineering.linecorp.com/ko/blog/harbor-for-private-docker-registry/  Gitlab과 Jenkins를 사용하여 Kubernetes 애플리케이션 배포하기  https://stackoverflow.com/questions/38461705/checkout-jenkins-pipeline-git-scm-with-credentials https://stackoverflow.com/questions/38969044/jenkins-kubernetes-jenkins-slave-node-is-offline  docker images REPOSITORY TAG IMAGE ID CREATED SIZE ldccai-registry/chatbot-server-client 2012141610 743a57b3f7e0 6 minutes ago 561MB jenkins/jenkins lts 1920bf702d7d 12 days ago 713MB kiwigrid/k8s-sidecar 0.1.275 4cbe191ff8a1 4 weeks ago 86MB k8s.gcr.io/kube-proxy v1.18.6 c3d62d6fe412 5 months ago 117MB weaveworks/weave-npc 2.6.5 420d4d5aac6f 6 months ago 36.8MB weaveworks/weave-kube 2.6.5 e9dd2f85e51b 6 months ago 123MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB node 12.2.0-alpine f391dabf9dce 19 months ago 77.7MB root@gs-hci-vm-cicd:~/chatbot-client# docker tag ldccai-registry/chatbot-server-client:2012141610 http://10.231.238.220:31000/library/chatbot-server-client:2012141610 Error parsing reference: \u0026ldquo;http://10.231.238.220:31000/library/chatbot-server-client:2012141610\u0026rdquo; is not a valid repository/tag: invalid reference format root@gs-hci-vm-cicd:~/chatbot-client# docker tag ldccai-registry/chatbot-server-client:2012141610 10.231.238.220:31000/library/chatbot-server-client:2012141610 root@gs-hci-vm-cicd:~/chatbot-client# docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.231.238.220:31000/library/chatbot-server-client 2012141610 743a57b3f7e0 8 minutes ago 561MB ldccai-registry/chatbot-server-client 2012141610 743a57b3f7e0 8 minutes ago 561MB jenkins/jenkins lts 1920bf702d7d 12 days ago 713MB kiwigrid/k8s-sidecar 0.1.275 4cbe191ff8a1 4 weeks ago 86MB k8s.gcr.io/kube-proxy v1.18.6 c3d62d6fe412 5 months ago 117MB weaveworks/weave-npc 2.6.5 420d4d5aac6f 6 months ago 36.8MB weaveworks/weave-kube 2.6.5 e9dd2f85e51b 6 months ago 123MB k8s.gcr.io/pause 3.2 80d28bedfe5d 10 months ago 683kB node 12.2.0-alpine f391dabf9dce 19 months ago 77.7MB root@gs-hci-vm-cicd:~/chatbot-client# docker push 10.231.238.220:31000/library/chatbot-server-client:2012141610 The push refers to repository [10.231.238.220:31000/library/chatbot-server-client] 8345e8881dd0: Pushed 5283c19fe222: Pushed 374955a1cffa: Pushed f6cc7537fab1: Pushed b7ceb6c9e50e: Pushed 59b74033df85: Pushed 917da41f96aa: Pushed 7d6e2801765d: Pushed f1b5933fe4b5: Pushed 2012141610: digest: sha256:0bb6173a94f220e32afa30c921d61c5f7e3f80a3913068720a04a1b25d937710 size: 2214 root@gs-hci-vm-cicd:~/chatbot-client#\n# kubectl create secret docker-registry ldccai --docker-server=http://10.231.238.220:31000/library --docker-username=admin --docker-password='Harbor12345' --docker-email=\u0026quot;healin.yoon@lotte.net\u0026quot; secret/ldccai created # kubectl get secret NAME TYPE DATA AGE ldccai kubernetes.io/dockerconfigjson 1 6s  ","id":1,"section":"posts","summary":"Intro Kubernetes CI/CD 구축기 (1) - Kubernetes CI 구축하기 목차 Helm으로 Jenkins를 Kubernetes에 배포하기 Kubernetes와 Docker Private Resistry 연동하기 Gitlab과","tags":["docker","kubernetes","helm","jenkins","gitlab"],"title":"Kubernetes CI/CD 구축기 (1) - Kubernetes CI 구축하기","uri":"https://healinyoon.github.io/2020/12/20201214_k8s_ci_cd/","year":"2020"},{"content":"이슈 Jenkins를 Helm으로 설치하는 과정에서 이슈가 하나 발생했다. 컨테이너 초기화가 계속 실패했는데, 아래와 같이 STATUS가 Running이 되지 않고 CrashLoopBackOff 또는 Error가 되는 것이다. 일단 Init:0/1은 \u0026lsquo;초기화 컨테이너라는 것\u0026rsquo;인데 자세한 내용은 아래의 공식 문서를 참고하자.\n# kubectl get all -n jenkins NAME READY STATUS RESTARTS AGE pod/jenkins-0 0/2 Init:0/1 0 4m31s   https://kubernetes.io/ko/docs/concepts/workloads/pods/init-containers/ https://kubernetes.io/ko/docs/tasks/debug-application-cluster/debug-init-containers/  # kubectl get all -n jenkins NAME READY STATUS RESTARTS AGE pod/jenkins-0 0/2 Init:CrashLoopBackOff 4 8m45s  여기서 중요한 포인트는 Running이 안되고 Init:0/1 -\u0026gt; CrashLoopBackOff -\u0026gt; Error를 반복한다는 것이다.\nkubectl describe 명령어로 Event를 확인하면 container init 과정에서 계속 에러가 발생한다.\n# kubectl describe pod jenkins-0 -n jenkins Name: jenkins-0 Namespace: jenkins Priority: 0 Node: gs-hci-vm-cicd/10.231.238.230 Start Time: Wed, 09 Dec 2020 20:19:26 +0900 Labels: app.kubernetes.io/component=jenkins-controller app.kubernetes.io/instance=jenkins app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=jenkins controller-revision-hash=jenkins-584bd849f4 statefulset.kubernetes.io/pod-name=jenkins-0 (중략) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 8m51s default-scheduler Successfully assigned jenkins/jenkins-0 to gs-hci-vm-cicd Normal Started \u0026lt;invalid\u0026gt; (x4 over \u0026lt;invalid\u0026gt;) kubelet, gs-hci-vm-cicd Started container init Warning BackOff \u0026lt;invalid\u0026gt; (x7 over \u0026lt;invalid\u0026gt;) kubelet, gs-hci-vm-cicd Back-off restarting failed container Normal Pulling \u0026lt;invalid\u0026gt; (x5 over \u0026lt;invalid\u0026gt;) kubelet, gs-hci-vm-cicd Pulling image \u0026quot;jenkins/jenkins:lts\u0026quot; Normal Pulled \u0026lt;invalid\u0026gt; (x5 over \u0026lt;invalid\u0026gt;) kubelet, gs-hci-vm-cicd Successfully pulled image \u0026quot;jenkins/jenkins:lts\u0026quot; Normal Created \u0026lt;invalid\u0026gt; (x5 over \u0026lt;invalid\u0026gt;) kubelet, gs-hci-vm-cicd Created container init  이를 해결하기 위해서 node에 접근해서 container 로그를 확인해봤다.\n# docker logs 0150155f58e9 Downloading plugin workflow-durable-task-step from url: https://updates.jenkins.io/download/plugins/workflow-durable-task-step/2.37/workflow-durable-task-step.hpi Downloaded file is not a valid ZIP Unable to download workflow-support  특정 플러그인을 설치하는데만 문제가 발생하나 싶어서 로그를 계속 확인해봤는데 \u0026lsquo;특정 플러그인에서만 발생하는 이슈는 아님\u0026rsquo;\n플러그인 다운로드의 시간이 livenessProbe와 startupProbe에 설정한 시간을 초과하는 건가 싶어서 설정 조정 후 helm upgrade를 시도했으나, 여전히 안됐다.\n (참고) Helm Release Upgrade\n # helm upgrade -f jenkins-values.yaml jenkins jenkins/jenkins --namespace=jenkins  해결 방법 결국 Plugin을 초기화할 때 설치하지 않도록 jenkins-values.yaml에서 주석 처리했다.\n jenkins-values.yaml\n 변경 전 installPlugins: - kubernetes:1.27.6 - workflow-aggregator:2.6 - git:4.4.5 - configuration-as-code:1.46 변경 후 installPlugins: # - kubernetes:1.27.6 # - workflow-aggregator:2.6 # - git:4.4.5 # - configuration-as-code:1.46  그리고 다시 helm release upgrade하면 정상 동작을 확인할 수 있다.\n# helm upgrade -f jenkins-values.yaml jenkins jenkins/jenkins --namespace=jenkins  # kubectl get all -n jenkins NAME READY STATUS RESTARTS AGE pod/jenkins-0 2/2 Running 0 6m20s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/jenkins NodePort 10.109.131.192 \u0026lt;none\u0026gt; 8080:31133/TCP 178m service/jenkins-agent ClusterIP 10.99.253.235 \u0026lt;none\u0026gt; 50000/TCP 178m NAME READY AGE statefulset.apps/jenkins 1/1 178m  대신 필요한 플러그인을 설치하고 보안 설정도 해줘야 한다.\n kubernetes workflow-aggregator git configuration-as-code  참고  https://stackoverflow.com/questions/57790463/cans-initiate-jenkins-using-stable-helm  ","id":2,"section":"posts","summary":"이슈 Jenkins를 Helm으로 설치하는 과정에서 이슈가 하나 발생했다. 컨테이너 초기화가 계속 실패했는데, 아래와 같이 STATUS가 Running이","tags":["docker","kubernetes","troubleshooting"],"title":"Kubernets(쿠버네티스) Troubleshooting (3) - Pod STATUS가 Running되지 않고 Error 또는 CrashLoopBackOff을 반복할 때","uri":"https://healinyoon.github.io/2020/12/20201214_k8s_troubleshooting_3/","year":"2020"},{"content":"요약  k8s cluster의 특정 Node가 NotReady됐는데 docker가 stop 상태였다. 2개의 원인이 만나서 발생: apt 자동 업데이트 + docker.io 패키지 버그 해결 방법: systemd에 패키지를 추가해줘야 하는데 -\u0026gt; 문제는 이게 버그를 무조건 한번은 다시 트리거 한다. -\u0026gt; k8s cluster의 모든 도커가 발생 가능한 이슈이므로 -\u0026gt; 한번씩은 다 죽다 살아나야 한다는 이야기..(근데 어자피 설정 안하면 도커가 언제 죽어도 안 이상한 상태가 된다.)  이슈 docker가 혼자서 죽었다. 죽은 원인을 확인해보자.\n# journalctl -u docker --since \u0026quot;2020-11-01 00:00:00\u0026quot; -- Logs begin at Tue 2020-10-13 18:15:48 KST, end at Wed 2020-12-09 14:31:19 KST. -- 12월 02 06:56:02 test-server systemd[1]: Stopping Docker Application Container Engine... 12월 02 06:56:02 test-server dockerd[16376]: time=\u0026quot;2020-12-02T06:56:02.357507251+09:00\u0026quot; level=info msg=\u0026quot;Processing signal 'terminated'\u0026quot; (중략)  12월 02 06:56:02 test-server systemd[1]: Stopping Docker Application Container Engine... systemd에 의해서 docker가 stop되고 다시 start 되지 않는다.\n원인(ubuntu package bug 페이지) 2개의 이슈가 만나서 발생했다. apt 자동 업데이트 + docker.io 패키지 버그\n1. apt 자동 업데이트 apt는 unattended updates 라는 것이 있다. 확인해보자.\n# journalctl -u apt* --since \u0026quot;2020-12-01 00:00:00\u0026quot; (중략) 12월 02 06:55:52 test-server systemd[1]: Starting Daily apt upgrade and clean activities... 12월 02 06:56:25 test-server systemd[1]: Started Daily apt upgrade and clean activities.  이게 자동으로 패키지를 업데이트 하는데.. 이것이 아래의 docker.io 패키지 버그와 만나서 docker를 죽여놓고 살려놓지 않는 참사가 발생한다.\n2. docker.io 패키지 버그 \u0026ldquo;bug in the docker.io package causing the service to stop on package upgrade\u0026quot;의 소스코드는 /var/lib/dpkg/info/docker.io.prerm 여기에 있다. 이렇게 생겼다.\n# because we had to use \u0026quot;dh_installinit --no-start\u0026quot;, we get to make sure the daemon is stopped on uninstall # (this is adapted from debhelper's \u0026quot;autoscripts/prerm-init-norestart\u0026quot;) if [ \u0026quot;$1\u0026quot; = remove ]; then invoke-rc.d docker stop fi  결론적으로는 stop만 하고 start는 안한다는 것\u0026hellip;\n해결 방법  https://launchpad.net/~bryce/+archive/ubuntu/containerd-sru-lp1870514-docker-dh/  sudo add-apt-repository ppa:bryce/containerd-sru-lp1870514-docker-dh sudo apt-get update  근데 문제가 있다. 이거 업데이트 할 때 위의 prerm 스크립트를 한번은 더 실행해야 하고 이건 버그를 한번 더 발생 시킨다 = 도커가 한 번 더 죽는다(unfortunately any change we make to docker.io requires the running of the prerm script (the version of the script already present on your system, not the one we\u0026rsquo;d be installing), and thus triggers the bug). 다른 서버의 도커에서도 발생할 수 있다 -\u0026gt; k8s cluster의 모든 노드가 설정을 위해서 한번씩은 죽었다가 살아나야 한다..ㅠㅠ\n","id":3,"section":"posts","summary":"요약 k8s cluster의 특정 Node가 NotReady됐는데 docker가 stop 상태였다. 2개의 원인이 만나서 발생: apt 자동 업데이트 + docker.io 패키지 버그 해결","tags":["docker","kubernetes","troubleshooting"],"title":"Kubernets(쿠버네티스) Troubleshooting (2) - Node가 NotReady됐는데 docker가 죽어있을 때","uri":"https://healinyoon.github.io/2020/12/20201209_k8s_troubleshooting_2/","year":"2020"},{"content":"에러 PV를 delete 하는 과정에서 계속 \u0026ldquo;Terminating\u0026rdquo; 상태에서 멈춰 있다.\n# kubectl delete pv jenkins2-pv persistentvolume \u0026quot;jenkins2-pv\u0026quot; deleted  # kubectl describe pv jenkins2-pv Name: jenkins2-pv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Terminating (lasts 27m) Claim: jenkins/jenkins Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 10.231.238.239 Path: /mnt/nfs_server/pv_jenkins2 ReadOnly: false Events: \u0026lt;none\u0026gt;  해결 방법 제거하려는 pv를 사용 중인 k8s resource를 먼저 제거해야 한다.\n # kubectl delete namespace jenkins namespace \u0026quot;jenkins\u0026quot; deleted  그 후 pv도 바로 제거 된다.\n# kubectl delete pv jenkins2-pv persistentvolume \u0026quot;jenkins2-pv\u0026quot; deleted  ","id":4,"section":"posts","summary":"에러 PV를 delete 하는 과정에서 계속 \u0026ldquo;Terminating\u0026rdquo; 상태에서 멈춰 있다. # kubectl delete pv jenkins2-pv persistentvolume \u0026quot;jenkins2-pv\u0026quot; deleted # kubectl describe pv jenkins2-pv Name: jenkins2-pv Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Terminating (lasts 27m) Claim: jenkins/jenkins Reclaim Policy: Retain Access Modes: RWO VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: NFS","tags":["docker","kubernetes","troubleshooting"],"title":"Kubernets(쿠버네티스) Troubleshooting (1) - PV Terminating에서 멈출 때","uri":"https://healinyoon.github.io/2020/12/20201209_k8s_troubleshooting_1/","year":"2020"},{"content":"소개 서버를 관리하다보면 네트워크에 대한 지식에 아쉬움을 느낄 때가 종종 있다. Understanding IP Addresses, Subnets, and CIDR Notation for Networking의 내용을 정리하면서 네트워크의 기본 개념을 살펴보고자 한다. 이번 글은 IP 주소 그룹화를 위한 네트워크 클래스, 서브넷 및 CIDR 표기법을 줄 살펴본다.\nUnderstanding IP addresses 네트워크 상의 모든 장소 또는 장치는 \u0026ldquo;주소로 지정 가능해야 한다\u0026rdquo;. 즉, 주소를 통해(by referencing under a predefined system of addresses) 네트워크 상에서 접근 가능해야한다. 일반적으로 네트워크에서 언급되는 \u0026lsquo;address\u0026rsquo;는 IP 주소를 의미한다.\nIP 주소를 사용하면 네트워크 인터페이스를 통해서 네트워크 리소스에 접근 가능해진다. 한 컴퓨터가 다른 컴퓨터와 통신하려면 원격 컴퓨터의 IP 주소를 사용한다. 두 컴퓨터가 동일한 네트워크에 있거나, 혹은 두 컴퓨터 사이의 다른 장치가 네크워크 요청을 변환하여 전달하면 두 컴퓨터는 서로 통신 가능하다.\n네트워크 내에서의 각 IP 주소는 고유한 값이여야 한다. 네트워크는 다른 네트워크와 격리될 수 있으며, 서로 다른 네트워크 간의 연결(bridged)과 변환(translation)이 가능하다. **네트워크 주소 변환(Network Address Translation)**은 packet(= 네트워크를 통해 전송되는 데이터의 가장 기본적인 단위)이 다른 네트워크에 전달되기 위해 주소를 변환하여 원하는 목적지로 도달할 수 있도록 한다. 물론 격리된 네트워크는 동일한 IP를 가질 수 있다.\nThe difference between IPv4 and IPv6 오늘날 가장 널리 사용되는 IP Protocol은 Ipv4와 IPv6이다. IP Protocol의 4번째 버전인 IPv4는 현재 대부분의 시스템에서 지원된다. 새로운 6번째 버전인 IPv6는 IPv4 주소 공간의 부족과 protocol의 개선으로 더 자주 배포되고 있다. 인터넷에 연결된 장치가 IPv4로 표현할 수 있는 주소 공간의 한계보다 많기 때문이다.\nIPv4는 32bit으로 표현되는 주소이다. 주소의 각 8bit segment는 .(dot)으로 나뉘고 0~255 사이의 값으로 표현 가능하다. 각 segment는 사람에게 읽기 쉽게 십진수로 표현되지만, 8bit의 배열 표기이므로 일반적으로 octet이라고 한다.\n일반적인 IPv4 주소는 다음과 같다.\n192.168.0.5  각 octet의 가장 낮은 값은 0이고, 가장 높은 값은 255이다. 위의 표현을 바이너라로 표현하면 다음과 같다(가독성을 위해 4bit을 공백으로 구분하고 \u0026lsquo;.\u0026lsquo;은 \u0026lsquo;-\u0026lsquo;로 대체한다).\n1100 0000 - 1010 1000 - 0000 0000 - 0000 0101  IPv4와 IPv6은 protocol 및 background 기능에 차이가 있지만 가장 큰 차이점은 주소 공간이다. IPv6는 128bit으로 주소를 표현한다. 즉 IPv4에 비해 7.9*10^28배 더 많은 주소 공간 표현이 가능하다.\nIPv6는 일반적으로 4개의 16진수로 이루어진 8개의 segment로 표현된다. 일반적인 IPv6의 주소는 다음과 같다.\n1203:8fe0:fe80:b897:8990:8a7c:99bf:323d  IPv4 Addresses Classes and Reserved Ranges Reserved Private Ranges Netmasks and Subnets CIDR Notation 참고  https://www.digitalocean.com/community/tutorials/understanding-ip-addresses-subnets-and-cidr-notation-for-networking  ","id":5,"section":"posts","summary":"소개 서버를 관리하다보면 네트워크에 대한 지식에 아쉬움을 느낄 때가 종종 있다. Understanding IP Addresses, Subnets, and CIDR Notation for Networking의 내용을 정리하면서 네트워크의 기본 개념","tags":null,"title":"네트워킹을 위한 IP address, Subnet, CIDR 표기법 이해하기","uri":"https://healinyoon.github.io/2020/12/%EC%9E%91%EC%84%B1%EC%A4%9120201201_ip_address_subnets_cidr/","year":"2020"},{"content":"소개 서버를 관리하다보면 네트워크에 대한 지식에 아쉬움을 느낄 때가 종종 있다. An Introduction to Networking Terminology, Interfaces, and Protocols의 내용을 정리하면서 네트워크의 기본 개념을 살펴보고자 한다.\nNetworking Glossary 네트워크 기본 지식을 살펴보기 앞서, 자주 사용되는 용어를 정의하면 다음과 같다.\n Connection  데이터 블록을 전달하기 위한 통로 일반적으로 데이터 전송 전 \u0026lsquo;connection\u0026rsquo; 준비 / 데이터 전송 후 \u0026lsquo;connection\u0026rsquo; 해제   Packet  네트워크를 통해 전송되는 데이터의 가장 기본적인 단위(블록)  인터넷 내에서 데이터를 보내기 위한 라우팅을 효율적으로 하기 위해 데이터를 여러 조각으로 나누어 전송: 조각 = packet   Packet = Header + Data + Tail Header = packet의 송수신 주소, 타임 스탬프, network hop 등의 정보 포함 Body(or Payload) = 전송되는 데이터를 포함   Network Interface  네트워크 하드웨어와 통신하기 위한 네트워크 고유 소프트웨어   WAN(Wide Area Network)  광역 통신망 일반적으로 인터넷 전체를 의미 =\u0026gt; interface가 WAN에 연결 = 인터넷을 통해 연결 가능하다고 가정 LAN과 LAN 사이를 연결\n그림 출처: https://ledgku.tistory.com/17   LAN(Local Area Network)  근거리 통신망 가정 또는 회사 네트워크 주로 이더넷 프로토콜 사용   Protocol  데이터 교환 방법 및 순서에 대해 정의한 의사소통 약속, 규약 혹은 규칙 체계를 의미 Protocol의 종류는 매우 다양   Port  네트워크를 통해 데이터를 주고 받는 프로세스를 식별하기 위해 호스트 내부적으로 프로세스가 할당 받는 고유한 주소 값(number)   Firwall  서버의 in/out 트래픽의 허용 여부를 결정하는 프로그램   NAT(Network Address Translation)  네트워크 주소를 변환해주는 기능 public outside address와 private inside address의 사이에서 border router로서의 역할 내부 망에서는 사설 IP 주소를 사용하여 통신을 하고, 외부망과의 통신시에는 NAT를 거쳐 공인 IP 주소로 자동 변환   VPN(Virtual Private Network)  가상 사설망 보안을 유지하면서 인터넷을 통해 LAN에 연결    Network Layers 출처: https://shlee0882.tistory.com/110\nOSI Model과 TCP/IP Model의 차이를 자세히 알고 싶다면 여기를 참고한다.\nOSI Model 네트워크 통신의 여러 계층을 설명하는 일반적인 방법으로, Open System Interconnect를 의미한다. OSI는 7계층을 정의한다.\n Application:(7 layer)  최종 사용자에게 가장 가까운 계층으로 =\u0026gt; 7계층에서 작동하는 응용프로그램은 사용자와 직접적으로 상호 작용 예) Chrome, Firefox, Safari, Skype, Outlook, Office 등등   Presentation(6 layer)  응용프로그램이나 네트워크를 위해 낮은 수준의 데이터를 \u0026ldquo;표현(변환)\u0026rdquo; 하는 기능 데이터를 어떻게 표현할 지 결정 3가지 기능 수행  송신자에서 온 데이터를 해석하기 위한 응용 계층 데이터 부호화, 변화 수신자에서 데이터의 압축을 풀 수 있는 방식으로 된 데이터 압축 데이터 암호화/복호화     Session(5 layer)  서로 다른 노드(device, 컴퓨터 or 서버) 간의 대화를 위한 session을 제공하는 역할 지속적인 방식으로 노드 간의 연결을 생성, 유지(조율, 예: 시스템의 응답 대기 시간), 파괴   Transport(4 layer)  상위 계층의 안정적인 연결을 제공 최종 시스템 및 호스트 간의 신뢰성있는 데이터 송수신을 위한 오류 검출, 복구, 흐름 제어, 중복 검사 등 수행 프로토콜은 UDP와 TCP로 구분 port를 사용하여 다른 애플리케이션 처리 Data unit: Segment   Network(3 layer)  서로 다른 노드 간에 데이터(packet) 라우팅 담당 IP adress 정보를 가짐 Data unit: Packet Device: 라우터, L3 스위치   Data Link(2 layer)  Physical 계층에서 송수신되는 데이터의 오류 수정 서로 다른 노드 또는 장치 간의 안정적인 링크를 설정하고 유지 MAC adress 정보를 가짐 Data unit: Frame Device: 스위치, 브릿지   Physical(1 layer)  연결에 사용되는 실제 물리적 장치를 처리 전기적, 기계적, 기능적인 특성을 이용하여 데이터를 전송 Data unit: Bit(on/off) Device: 케이블, 허브, 리피터    TCP/IP Model TCP protocol과 IP protocol을 OSI 7계층에 맞추어 간략화 시킨 모델이다.\n Application  사용자 응용 프로그램으로부터 요청을 받아서 사용자 데이터를 생성하고 하위 계층으로 전달하는 역할 OSI 7계층의 Application + Presentation + Session   Transport  프로세스 간의 통신 담당 프로토콜은 UDP와 TCP로 구분 port를 사용하여 다른 애플리케이션 처리   Internet  Transport 계층에서 받은 packet을 목적지까지 효율적으로 전달하는 기능 데이터의 주소를 판독하고 주소에 맞는 네트워크 탐색, 해당 host가 데이터를 수신할 수 있도록 송신   Link  Internet 계층에 주소로 사용 가능한 인터페이스를 제공하기 위해 로컬 네트워크의 실제 topology 구현 인접 node간의 connection을 구축하고 데이터 전송    Interfaces  컴퓨터의 네트워크 통신 point로, 각 interface는 물리 또는 가상 네트워크 장치와 연결 일반적으로 서버는 Ethernet 또는 무선 인터넷 카드에 대한 구성 가능한 network interface를 하나씩 가지고 있음 loopback 또는 local host interface 라고 하는 가상 network interface를 정의  단일 컴퓨터의 응용 프로그램 및 프로세스를 다른 응용 프로그램 및 프로세스에 연결하는 interface로 사용 많은 tool에서 \u0026ldquo;IO\u0026rdquo; interface로 참조되는 것을 확인 가능   일반적으로 관리자는 인터넷 트래픽을 처리하기 위한 interface와 LAN 또는 prive network를 위한 interface로 각각 구성 DigitalOcean(사설 네트워트를 사용하는 데이터 센터)에서 VPS는 local interface 외에도 2개의 interface를 가짐  \u0026ldquo;eth0\u0026rdquo;: 인터넷 트래픽 처리 \u0026ldquo;eth1\u0026rdquo;: private network와 통신    Protocols 네트워크에서 데이터 교환 방법 및 순서에 대해 정의한 의사소통 약속, 규약 혹은 규칙 체계를 의미한다. 낮은 네트워크 계층에서 높은 네트워크 계층까지 사용되는 다양한 프로토콜을 살펴보자.\nMAC(Media Access Control)  Device를 구별하기 위한 protocol로 모든 device는 제조 과정에서 고유한 MAC adress 부여 Link 계층  IP(Internet Protocol)  Internet Protocol Suite IP 주소는 각 네트워크에서 고유하며 이를 통해 컴퓨터 네트워크를 통해 서로 주소를 지정 TCP/IP 모델의 Internet 계층 또는 OSI의 Network 계층  ICMP(Internet Control Message Protocol)  Internet Protocol Suite 가용성 또는 오류 조건을 표시하기 위해 장치간에 메시지를 보내는 데 사용  ping, tracerroute와 같은 네트워크 진단 도구에 사용   OSI의 Network 계층  TCP(Transmission Control Protoco)  Internet Protocol Suite 데이터를 메세지의 형태로 보내기 위해 IP와 함께 사용하는 protocol  TCP의 역할: 전송하는 데이터를 추적 및 관리 IP의 역할: 데이터의 전송을 담당   연결형 protocol  신뢰성 있는 데이터 전송   Transport 계층  UDP(User Datagram Protocol)  Internet Protocol Suite 데이터를 메세지의 형태로 보내기 위해 사용되는 protocol 비연결형 protocol  낮은 신뢰성   Transport 계층  HTTP  W3(World Wide Web, WWW) 상에서 정보를 주고 받기 위한 protocol Application 계층  FTP(File Transfer Protocol)  서버와 클라이언트 사이의 파일 전송을 위한 protocol Application 계층  DNS(Domain Name System)  도메인 이름 \u0026lt;-\u0026gt; IP 주소로 변환하는 protocol TCP/IP 모델의 Application 계층  SSH(Secure SHell)  네트워크 상의 컨퓨터 또는 device와 통신하기 위한 암호화된 protocol Application 계층  참고  https://www.ibm.com/support/knowledgecenter/ko/ssw_aix_71/network/tcpip_interfaces.html https://ledgku.tistory.com/17 https://m.blog.naver.com/PostView.nhn?blogId=on21life\u0026amp;logNo=221509574568\u0026amp;proxyReferer=https:%2F%2Fwww.google.com%2F https://bmind305.tistory.com/25 https://m.blog.naver.com/PostView.nhn?blogId=ssdyka\u0026amp;logNo=221376674886\u0026amp;proxyReferer=https:%2F%2Fwww.google.com%2F https://shlee0882.tistory.com/110 https://medium.com/harrythegreat/osi%EA%B3%84%EC%B8%B5-tcp-ip-%EB%AA%A8%EB%8D%B8-%EC%89%BD%EA%B2%8C-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-f308b1115359  ","id":6,"section":"posts","summary":"소개 서버를 관리하다보면 네트워크에 대한 지식에 아쉬움을 느낄 때가 종종 있다. An Introduction to Networking Terminology, Interfaces, and Protocols의 내용을 정리하면서 네트워크의 기본 개념을 살","tags":null,"title":"네트워크 용어, 인터페이스 및 프로토콜 기초 이해하기","uri":"https://healinyoon.github.io/2020/11/20201127_network_basic/","year":"2020"},{"content":"Docker image는 container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하는데 필요한 단계를 정의하는 Dockerfile를 작성하여 고유한 image를 빌드할 수도 있다. Dockerfile의 각 단계에 정의된 명령어는 image에 layer를 만든다. Docker image를 이해하기 위해서는 이 Layer에 대한 이해가 필요하다.\nImage Layer 앞서 설명한 바와 같이 image는 Dockerfile로 빌드된다. Dockerfile을 작성하고 빌드하는 과정을 통해 image layer에 대해 이해해보자. Dockerfile은 다음과 같이 단계별로 구성된다.\n// step1 FROM alpine:3.10 // step2 ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;hello\u0026quot;]  Dockerfile을 빌드하면 아래와 같이 각 단계별로 image가 생성되는 것을 확인할 수 있다. 즉 Dockerfile을 빌드하면 Dockerfile의 단계별로 image layer가 생성되며, 이들이 계층적으로 하나씩 쌓이며 image를 이루게 된다.\n$ sudo docker build --tag echo:1.0 . Sending build context to Docker daemon 7.168kB FROM alpine:3.10 Step 1/2 : FROM alpine:3.10 3.10: Pulling from library/alpine 21c83c524219: Already exists Digest: sha256:f0e9534a598e501320957059cb2a23774b4d4072e37c7b2cf7e95b241f019e35 Status: Downloaded newer image for alpine:3.10 ---\u0026gt; be4e4bea2c2e Step 2/2 : ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;hello\u0026quot;]` ---\u0026gt; Running in 2dbb42b167e8 FROM ubuntu:18:04 Removing intermediate container 2dbb42b167e8 ---\u0026gt; 75b05f96c44d Successfully built 75b05f96c44d Successfully tagged echo:1.0  Container Layer Image를 빌드한 후 docker container run 명령을 수행하면 아래와 같이 가장 마지막에 container layer를 생성한다(이 container layer는 container를 삭제하면 같이 삭제 된다).\n그런데 실제로 container를 사용할 때는 하나의 파일 시스템으로 보이는데, 이렇게 계층적으로 나눠진 image들이 어떻게 하나의 파일 시스템으로 보이는 것일까? 바로 Union FS 덕분이다.\nUnion FS Union Mount Union Mount는 복수의 파일 시스템을 하나의 파일 시스템으로 마운트하는 기능으로 두 파일 시스템에서 동일한 파일이 있다면 나중에 마운트 된 파일 시스템의 파일을 Overlay한다. 하위 파일에 대한 쓰기 작업은 CoW(Copy on Write) 전략에 따라 복사본을 생성하여 수행하므로 원본 파일 시스템은 변하지 않는 것이 특징이다.\nDocker Image: Union File System Docker image는 Union File System 기반으로 동작한다. Union File Systme의 특성에 따라 하위 layer는 읽기 전용이며, CoW 전략에 의해 쓰기 작업은 상위 레이어로 복사해서 이루어지기 때문에 하나의 image로 부터 복수의 container가 실행 가능한 것이다.\n  Container Layer\n Write layer 각 container마다 최 상단 layer에 생성되어, container마다 자신만의 상태를 가질 수 있게 해준다. container가 생성된 후 모든 변경 작업은 여기서 이루어진다. R/W 속도가 느리다.    Image Layer\n Read only layer 다른 container와 공유 가능하다.    Image Layer 디렉토리 파악하기 Image 정보 확인 nginx image를 다운받아보자.\n Image pull  # sudo docker pull nginx Using default tag: latest latest: Pulling from library/nginx 852e50cd189d: Already exists 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest   Image 정보 확인  Image의 정보는 docker inspect {image} 명령어로 확인할 수 있다.\n# docker inspect nginx [ { \u0026quot;Id\u0026quot;: \u0026quot;sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221c\u0026quot;, \u0026quot;RepoTags\u0026quot;: [ \u0026quot;nginx:latest\u0026quot; ], (중략)  Image 저장소 위치 확인 docker image 저장소 위치는 docker info 명령어로 확인할 수 있다.\n# docker info Client: Debug Mode: false Server: Containers: 57 Running: 16 Paused: 0 Stopped: 41 Images: 149 Server Version: 19.03.13 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: false Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 8fba4e9a7d01810a393d5d25a3621dc101981175 runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd init version: fec3683 Security Options: apparmor seccomp Profile: default Kernel Version: 5.4.0-1031-azure Operating System: Ubuntu 18.04.5 LTS OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 7.749GiB Name: master ID: 5HUW:6SVP:6Q4Q:LDGN:YMF2:RBDM:VEXF:3UKJ:XVTV:5SRK:SS7R:TPJ2 Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support  Layer 디렉토리  디렉토리 구조  Image layer 정보를 확인하기 위한 디렉토리만 살펴보면 다음과 같다.\n/var/lib/docker# tree docker . docker ├── containers: docker container 정보를 저장한다. ├── image: docker image 정보를 저장한다. │ └── overlay2 │ ├── imagedb: imagedb에 대한 정보는 layerdb에 저장된다. │ └── layerdb: layerdb에 대한 정보는 overlay2에 저장된다. ├── overlay2: docker image의 파일 시스템이 저장된다. 실질적으로 image layer 데이터가 저장되는 경로이다. │ ├── 0090fbeed32cba3aed09c2459d4a5f59144be127dfed23cbe8c7f47982dd3c12 │ ├── 014997dffd51a7e9a1418e7f888097c360c592ddb791c0973b35b9935a1eea9d   각 디렉토리별 용량  위의 각 경로의 데이터 용량을 조회해보면 다음과 같다. 실제로 docker 데이터가 저장되는 root 경로인 /var/lib/docker 디렉토리 데이터 용량과 /var/lib/docker/overlay2 디렉토리 데이터 용량이 가장 근접한 것을 볼 수 있다(즉 실질적인 image layer 데이터가 여기에 저장된다는 것).\n# du -sh /var/lib/docker 9.1G /var/lib/docker # du -sh /var/lib/docker/containers/ 39M /var/lib/docker/containers/ # du -sh /var/lib/docker/image/ 11M /var/lib/docker/image/ # du -sh /var/lib/docker/overlay2/ 6.0G /var/lib/docker/overlay2/  참고  https://docs.docker.com/get-started/overview/ https://nirsa.tistory.com/63 https://devaom.tistory.com/5  ","id":7,"section":"posts","summary":"Docker image는 container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하","tags":["docker"],"title":"Docker Image의 핵심 기술: Layer","uri":"https://healinyoon.github.io/2020/11/20201125_docker_image_layer/","year":"2020"},{"content":"The Docker platform Docker는 어플리케이션을 패키징하고 실행할 수 있는 Container라는 격리된 환경을 제공한다. Container는 hypervisor의 추가 로드가 필요하지 않기 때문에 경량이지만 호스트 머신의 커널 내에서 직접 실행된다.\nDocker architecture Docker는 client-server 아키텍처를 사용한다. Docker client는 Docker daemon과 통신한다. Docker daemon은 cotainer를 빌드, 실행, 배포하는 작업을 수행한다. Docker client와 daemon은 동일한 시스템에서 실행되거나, 원격으로 연결할 수도 있습니다. Docker client와 daemon은 UNIX 소켓 또는 네트워크 인터페이스를 통해 REST API로 통신한다.\nDocker daemon Docker daemon(=dockerd)은 Docker API 요청을 수신하고 image, container, network, volume과 같은 Docker 객체를 관리한다. Docker daemon의 상세 내용은 여기를 참고하자.\nDocker client Docker client(=docker)는 사용자가 입력한 명령어를 Docker daemon에 전달한다. 이때의 동작 흐름은 다음과 같다.\n 사용자가 docker 명령어 입력 Docker client는 Docker deamon에게 명령어 전달 Docker daemon은 명령어를 파싱하고 해당하는 작업 수행 Docker daemon은 수행 결과를 Docker client에게 반환 Docker client는 사용자에게 결과를 출력  Docker registries Docker registry는 Docker image를 저장한다. Docker Hub는 누구나 사용 가능한 public registry로, Docker는 기본적으로 Docker Hub에서 image를 찾도록 구성된다. private registry 구축도 가능하다.\ndocker pull 또는 docker run 명령어를 사용하면 registry에서 image를 가져온다. docker push 명령을 사용하면 image가 구성된 registry로 push 된다.\nDocker objects Docker를 사용하면 image, container, network, volume, plugin 및 기타 object를 생성하고 사용하게 된다.\nImages Docker container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하는데 필요한 단계를 정의하는 Dockerfile를 작성하여 고유한 image를 빌드할 수도 있다.\nContainers Docker container는 image의 실행 가능한 인스턴스이다. Docker API 또는 CLI를 사용하여 container를 생성, 시작, 중지, 이동 또는 삭제 할 수 있다. container는 image와 사용자가 생성하거나 시작할 때 제공하는 구성 옵션에 의해 정의된다. container가 제거되면 영구 저장소에 저장되지 않은 데이터의 변경 사항들은 함께 제거된다.\n 예제 docker run 명령\n 다음 명령어는 ubuntu container를 실행하고 container 내부와 대화형 command 창을 연결한다.\n$ docker run -i -t ubuntu /bin/bash  위의 명령어를 입력하면 다음 절차가 순차적으로 진행된다.\n ubuntu image가 local에 없는 경우, Docker는 자동으로 docker pull ubuntu 명령을 수행하여 registry에서 image를 가져온다. Docker는 docker container create 명령을 수행하여 새로운 container를 생성한다. Docker는 container layer(Read/Write 가능)를 최종 layer로 container에 할당한다. 이를 통해 실행 중인 container는 container 내부의 로컬 파일 시스템에서 파일과 디렉토리를 읽고 쓰는 것이 가능하다. 네트워크 옵션을 지정하지 않았으므로, Docker는 container를 기본 네트워크에 연결하는 인터페이스를 만든다. 여기에는 container에 IP 주소를 할당하는 것도 포함된다. Docker는 container를 시작하고 /bin/bash 명령을 실행시킨다. container가 실행중이며 -i와 -t 옵션을 사용하였기 때문에 container 내부에서 명령 입력이 가능하다. exit 명령을 사용하여 container를 빠져나오면 container가 중지되지만 제거되지는 않는다. 다시 시작하거나 제거할 수 있다.  Container technology Docker는 Go 프로그래밍 언어로 작성되었으며, Linux 커널의 여러 기능을 활용하도록 제공한다.\nNamespace Linux는 격리된 작업 공간을 제공하기 위해 namespace 라는 기능을 커널에 내장하고 있다. 현재 Docker Engine은 Linux에서 다음과 같은 namespace를 사용한다.\n mnt(MNT; 파일 시스템 마운트): host 파일 시스템에 구애받지 않고 독립적으로 파일 시스템을 mount or unmount 가능하다. pid(PID; 프로세스 ID): 독립적인 프로세스 공간 할당 net(NET; 네트워킹): namespace간의 네트워크 충돌 방지(중복 port 바인딩 등) ipc(IPC; 프로세스간 통신): 프로세스간의 독립적인 통신 통로 할당 uts(UTS; Unix Timesharing System): 독립적인 hostname 할당  PID namespace 밖의 공간(regular namespace)에서도 독립적인 프로세스 확인이 가능하다. 즉 namespace 기능은 같은 공간을 공유하되, 좀 더 제한된 공간을 할당하는 원리이다. namespace를 통해 독립적인 공간을 할당한 후에는 nsenter(namespace enter)라는 명령어를 통해 이미 실행 중인 프로세스의 namespace 공간에 접근할 수 있다.\nnsenter는 docker의 exec와 비슷한 역할을 한다. 다만 nsenter는 exce와 다르게 cgroups에 들어가지 않기 때문에 리소스 제한의 영향을 받지 않는다.\nCgroups 자원(resource)에 대한 제어를 가능하게 해주는 Linux 커널 기능이다. Croups은 다음 리소스를 제어할 수 있다.\n 메모리 CPU I/O 네트워크 device 노드(/dev/)  참고  https://docs.docker.com/get-started/overview/ https://velog.io/@labyu/docker-3 https://tech.ssut.me/what-even-is-a-container/  ","id":8,"section":"posts","summary":"The Docker platform Docker는 어플리케이션을 패키징하고 실행할 수 있는 Container라는 격리된 환경을 제공한다. Container는 hypervisor","tags":["docker"],"title":"Docker 개요","uri":"https://healinyoon.github.io/2020/11/20201125_docker_overview/","year":"2020"},{"content":"jdk8 설치 버전 확인 $ java -version  repository 업데이트 $ sudo apt-get update  openjdk 설치 $ sudo apt-get install openjdk-8-jdk  설치 확인 $ java -version openjdk version \u0026quot;1.8.0_275\u0026quot; OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01) OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)  환경 변수 설정 javac 설치 경로 확인 # javac -version javac 1.8.0_275 # which javac /usr/bin/javac # readlink -f /usr/bin/javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac  환경 변수 설정 # vi /etc/profile 맨 아래에 추가 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/javac  적용 # source /etc/profile  확인 # echo $JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/bin/javac  ","id":9,"section":"posts","summary":"jdk8 설치 버전 확인 $ java -version repository 업데이트 $ sudo apt-get update openjdk 설치 $ sudo apt-get install openjdk-8-jdk 설치 확인 $ java -version openjdk version \u0026quot;1.8.0_275\u0026quot; OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01) OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode) 환경 변수 설정 javac 설치 경로 확인 # javac -version javac 1.8.0_275 # which","tags":["java","openjdk"],"title":"java(openjdk) 설치 및 환경 변수 설정","uri":"https://healinyoon.github.io/2020/11/20201113_install_java/","year":"2020"},{"content":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자.\n사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은 조건이 있다면 선행해주자.\n1. kubernetes cluster 구축 아직 구축이 되어 있지 않다면 Kubernetes Cluster 설치하기(ubuntu18.04)를 참고하여 진행\n2. nvidia-docker 설치 아직 설치가 되어 있지 않다면 nvidia-docker install guide를 참고하여 진행\n1. Nvidia Plugin Pod 생성 ref)  Nvidia k8s-device-plugin 공식 사이트 Nvidia docker 공식 사이트  1.1 가장 정보가 많았던 YAML으로 설치, 그러나 실패 처음에 여기 링크를 참고하여 진행했다.\n다만 아래와 같이 버전만 변경하여 실행했다.\n$ kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml daemonset.extensions/nvidia-device-plugin- daemonset-1.12 created  하지만 아래 이슈 발생해서 실패했다 ↓ ↓ ↓\n1.2. 이슈 1.2.1. 이슈 내용 쿠버네티스 1.15 버전 이하를 설치했을 경우 문제 없겠지만, 1.16 버전 이상을 설치했을 경우 다음과 같은 에러가 발생한다.\nerror: unable to recognize \u0026quot;https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml\u0026quot;: no matches for kind \u0026quot;DaemonSet\u0026quot; in version \u0026quot;extensions/v1beta1\u0026quot;  이는 쿠버네티스 버전이 업그레이드 되면서, Daemonset의 extensions/v1beta1 버전을 더이상 지원하지 않기 때문이다.\n→ 1) 따라서 버전을 apps/v1 으로 변경하고 selector object를 추가한 후\n→ 2) k8s-device-plugin을 다시 설치하고\n→ 3) 매니패스트 파일도 적절하게 수정해주었다.\nref)  Kubectl convert 참고 자료 No matches 이슈 해결 자료   (참고) Pod 생성 실패시 에러 정보 확인\n $ kubectl describe pod {pod 명}  1.2.2. 변경한 YAML 파일을 사용하여 DaemonSet Pod 생성 이제 커스터 마이징한 YAML 파일로 Pod를 생성해보자.\n gpu-plugin.yaml\n apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-device-plugin-daemonset-1.12 namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: name: nvidia-device-plugin-ds template: metadata: # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler # reserves resources for critical add-on pods so that they can be rescheduled after # a failure. This annotation works in tandem with the toleration below. annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; labels: name: nvidia-device-plugin-ds spec: tolerations: # Allow this pod to be rescheduled while the node is in \u0026quot;critical add-ons only\u0026quot; mode. # This, along with the annotation above marks this pod as a critical add-on. - key: CriticalAddonsOnly operator: Exists - key: nvidia.com/gpu operator: Exists effect: NoSchedule containers: - image: nvidia/k8s-device-plugin:1.11 name: nvidia-device-plugin-ctr securityContext: allowPrivilegeEscalation: false capabilities: drop: [\u0026quot;ALL\u0026quot;] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins nodeSelector: gpus: \u0026quot;true\u0026quot;  위의 매니패스트 주요 사항은 다음과 같다.\n① 리소스 유형 = DaemonSet\nkind: DaemonSet  따라서 기본적으로는 모든 worker 노드 하나씩 동작하게 한다.  ② RollingUpdate\nspec.selector.matchLabels.name\nname: nvidia-device-plugin-ds  spec.template.matadata.labels.name\nlabels: name: nvidia-device-plugin-ds  name object가 nvidia-device-plugin-ds 인 리소스에 대하여 RollingUpdate를 설정한다.\n③ node Label 지정\nspec.template.spec.nodeSelector 로 어느 노드의 DaemonSet으로 띄워줄 것인지 레이블링해준다.\nnodeSelector: gpus: \u0026quot;true\u0026quot;  1.3. gpu-plugin DeamonSet Pod 정상 동작 확인 $ kubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-d7t2g 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-jmcg9 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-zhsqm 1/1 Running 0 4h8mubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-7kh27 1/1 Running 0 27m $ kubectl -n kube-system logs -l name=nvidia-device-plugin-ds 2020/07/01 05:02:30 Loading NVML 2020/07/01 05:02:30 Fetching devices. 2020/07/01 05:02:30 Starting FS watcher. 2020/07/01 05:02:30 Starting OS watcher. 2020/07/01 05:02:30 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock 2020/07/01 05:02:30 Registered device plugin with Kubelet  🌟🌟 여기서 잠깐! 🌟🌟 중요한 사항은 gpu를 사용하려는 Worker node가 gpus: \u0026quot;true\u0026quot; 레이블링이 되어 있어야 한다는 것이다.\n 만약 GPU가 있는 node인데 해당 DeamonSet이 올라가있지 않거나 신규 Worker node를 추가하려는 경우  ⇒ kubectl label nodes {Worker node 명} gpus=true 로 레이블링을 해주자.\n2. GPU 개수 확인 이제 쿠버네티스가 사용 가능한 GPU 개수를 확인해보자.\nmaster node에서 아래의 명령어를 실행하면 각 worker node에서 사용 가능한 GPU 개수가 출력된다.\n$ kubectl get nodes \u0026quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\u0026quot; NAME GPU gpu-1080ti-XX 9 gpu-1080ti-XX 9  3. Pod에서 그래픽 카드 명령어 테스트 3.1. GPU를 사용하는 Pod 생성하기 3.1.1. YAML 파일  gpu-k8s.yaml\n apiVersion: v1 kind: Pod metadata: name: gpu-k8s spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null resources: requests: nvidia.com/gpu: 2 limits: nvidia.com/gpu: 2  ref) nvidia/cuda 도커 이미지 버전이 맞지 않은 이슈 발생 시 ⇒ 도커 허브에서 맞는 이미지 버전을 찾아서 사용해주면 된다.\n Docker Hub Kubernetes Resource Request와 Limit의 이해 Schedule GPUs  3.1.2. Pod 생성 및 확인 $ kubectl apply -f gpu-k8s.yaml pod/gpu-k8s created $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 12s  3.1.3. nvidia-smi 확인 $ kubectl logs gpu-k8s Thu Jul 2 06:00:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 30C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.2. 2개의 Pod를 띄워서 gpu 4개를 모두 사용하기 gpu-k8s2.yaml 매니패스트 파일을 하나 더 만들어서 위와 동일하게 실행해보자.\n3.2.1. 결과 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 2m44s gpu-k8s2 1/1 Running 0 2  3.2.2. nvidia-smi 확인 $ kubectl logs gpu-k8s2 Thu Jul 2 06:03:06 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 31C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.3. 컨테이너가 노드의 모든 GPU를 사용 가능하게 하고 싶다면  request와 limit 설정 부분을 없애주면 된다. 특이한 점은 이미 다른 파드에 GPU를 모두 할당 해준 상태에서도 파드 생성 가능하다.  3.3.1. YAML 파일 apiVersion: v1 kind: Pod metadata: name: gpu-all spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime env: - name: DP_DISABLE_HEALTHCHECKS value: \u0026quot;xids\u0026quot; command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null  3.3.2. Pod 실행 $ kubectl apply -f gpu-all.yaml pod/gpu-all created  3.3.3. 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-all 1/1 Running 0 50s gpu-k8s 1/1 Running 0 42m gpu-k8s2 1/1 Running 0 40m $ kubectl logs gpu-all Thu Jul 2 06:42:25 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 32C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  ","id":10,"section":"posts","summary":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자. 사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은","tags":["docker","kubernetes"],"title":"Kubernets(쿠버네티스) with GPU 구축하기","uri":"https://healinyoon.github.io/2020/10/20201023_kubernetes_with_gpu/","year":"2020"},{"content":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주하는 질문이므로, 이번 기회에 정리를 해보려고 한다.\n1. Host vs Container GPU 성능 테스트 요약 1.1. 결론 GPU on Host VS Container를 비교하였을 때 속도의 성능 차이가 없었음\n1.2. 상세 내용  GPU Performance on Host VS Container 비교시 정확도에는 차이가(모델이 같으므로) 없으므로, 속도만 비교하였다. GPU를 1개 사용하였을 경우와 2개 사용하였을 경우를 나누어 비교하였다. 동일한 조건에 대해 테스트 케이스(횟수)를 총 2회씩 수행하였다.  2. GPU 성능 비교를 위한 Host 환경 설정 2.1. 가상환경 생성 # pip3 install virtualenv # virtualenv /home/rin_gu/PerformTestEnv/performTestEnv  2.2. Tensorflow 설치 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # python -m pip install --upgrade pip # sudo -H pip install --upgrade tf-nightly-gpu  2.3. 성능 테스트 코드 실행 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # cd /home/rin_gu/PerformanceTest/ # git clone https://github.com/tensorflow/benchmarks.git # cd benchmarks  3. GPU 성능 비교를 위한 Container 환경 설정 3.1. 컨테이너 이미지 다운로드 root@ubuntu:~# docker pull tensorflow/tensorflow:nightly-gpu  3.2. 컨테이너 run with GPU root@ubuntu:~# docker container run -d --gpus all -it tensorflow/tensorflow:nightly-gpu 94515f052adbddf25bb9c66e5d5d7ee6a6010cfc74c6936f3b01bdf764202488  3.3. 컨테이너 접속 root@ubuntu:~# docker exec -it 94515f052adb /bin/bash \u0026quot;docker exec\u0026quot; requires at least 2 arguments. See 'docker exec --help'. Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container root@ubuntu:~# docker container exec -it 94515f052adb /bin/bash ________ _______________ ___ __/__________________________________ ____/__ /________ __ __ / _ _ \\_ __ \\_ ___/ __ \\_ ___/_ /_ __ /_ __ \\_ | /| / / _ / / __/ / / /(__ )/ /_/ / / _ __/ _ / / /_/ /_ |/ |/ / /_/ \\___//_/ /_//____/ \\____//_/ /_/ /_/ \\____/____/|__/ WARNING: You are running this container as root, which can cause new files in mounted volumes to be created as the root user on your host machine. To avoid this, run the container by specifying your user's userid: $ docker run -u $(id -u):$(id -g) args...  3.4. 파이썬 버전 확인 root@282efcfdc3c8:/# python Python 3.6.9 (default, Apr 18 2020, 01:56:04) [GCC 8.4.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import tensorflow as tf 2020-06-22 11:16:19.882107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 \u0026gt;\u0026gt;\u0026gt; tf.__version__ '2.3.0-dev20200621'  3.5. git 설치 / 리포지토리 다운로드 root@94515f052adb:/# apt-get install git root@94515f052adb:/# git clone https://github.com/tensorflow/benchmarks.git  4. Host vs Container 성능 비교 4.1. GPU 1개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스} python tf_cnn_benchmarks.py --num_gpus=1 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 124.77 126.95 model: Resnet50   2 125.76 125.07 model: Resnet50    4.2. GPU 2개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스1, GPU 디바이스2} python tf_cnn_benchmarks.py --num_gpus=2 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 242.60 240.64 model: Resnet50   2 241.52 236.72 model: Resnet50    5. GPU 성능 측정 상세 - Host 5.1. 테스트 1 5.1.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 127.9 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 126.4 +/- 0.2 (jitter = 0.4)\t7.849 20\timages/sec: 126.3 +/- 0.1 (jitter = 0.5)\t8.013 30\timages/sec: 126.2 +/- 0.1 (jitter = 0.7)\t7.940 40\timages/sec: 126.0 +/- 0.1 (jitter = 0.7)\t8.137 50\timages/sec: 125.8 +/- 0.1 (jitter = 0.6)\t8.052 60\timages/sec: 125.6 +/- 0.1 (jitter = 0.7)\t7.782 70\timages/sec: 125.4 +/- 0.1 (jitter = 0.9)\t7.856 80\timages/sec: 125.3 +/- 0.1 (jitter = 1.0)\t8.011 90\timages/sec: 125.1 +/- 0.1 (jitter = 1.2)\t7.843 100\timages/sec: 124.8 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 124.77 ----------------------------------------------------------------  5.2. 테스트 2 5.2.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중량) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 246.2 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 244.9 +/- 0.6 (jitter = 1.7)\t7.892 20\timages/sec: 244.3 +/- 0.4 (jitter = 2.6)\t7.968 30\timages/sec: 244.5 +/- 0.4 (jitter = 2.3)\t7.934 40\timages/sec: 244.2 +/- 0.3 (jitter = 2.6)\t8.016 50\timages/sec: 243.8 +/- 0.3 (jitter = 2.6)\t7.922 60\timages/sec: 243.6 +/- 0.3 (jitter = 2.0)\t7.872 70\timages/sec: 243.5 +/- 0.2 (jitter = 1.9)\t7.837 80\timages/sec: 243.3 +/- 0.2 (jitter = 1.8)\t7.850 90\timages/sec: 243.0 +/- 0.2 (jitter = 2.1)\t7.859 100\timages/sec: 242.7 +/- 0.2 (jitter = 2.2)\t7.946 ---------------------------------------------------------------- total images/sec: 242.60 ----------------------------------------------------------------  6. GPU 성능 측정 상세 - Container 6.1. 테스트 1 6.1.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 129.2 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 128.7 +/- 0.4 (jitter = 0.9)\t7.849 20\timages/sec: 128.4 +/- 0.2 (jitter = 1.1)\t8.013 30\timages/sec: 128.4 +/- 0.2 (jitter = 0.8)\t7.940 40\timages/sec: 128.3 +/- 0.1 (jitter = 0.9)\t8.136 50\timages/sec: 128.1 +/- 0.1 (jitter = 0.9)\t8.053 60\timages/sec: 127.9 +/- 0.1 (jitter = 1.1)\t7.784 70\timages/sec: 127.7 +/- 0.1 (jitter = 1.4)\t7.859 80\timages/sec: 127.5 +/- 0.1 (jitter = 1.4)\t8.014 90\timages/sec: 127.3 +/- 0.1 (jitter = 1.4)\t7.842 100\timages/sec: 127.1 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 126.95 ----------------------------------------------------------------  6.2. 테스트 2 6.2.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 240.6 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 243.4 +/- 0.8 (jitter = 3.5)\t7.892 20\timages/sec: 243.3 +/- 0.5 (jitter = 2.7)\t7.968 30\timages/sec: 243.1 +/- 0.4 (jitter = 2.8)\t7.934 40\timages/sec: 242.6 +/- 0.3 (jitter = 2.3)\t8.019 50\timages/sec: 242.4 +/- 0.3 (jitter = 2.0)\t7.923 60\timages/sec: 242.1 +/- 0.3 (jitter = 1.8)\t7.878 70\timages/sec: 241.8 +/- 0.2 (jitter = 1.6)\t7.834 80\timages/sec: 241.5 +/- 0.2 (jitter = 1.6)\t7.861 90\timages/sec: 241.1 +/- 0.2 (jitter = 2.0)\t7.852 100\timages/sec: 240.8 +/- 0.3 (jitter = 2.1)\t7.948 ---------------------------------------------------------------- total images/sec: 240.64 ----------------------------------------------------------------  ","id":11,"section":"posts","summary":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주","tags":["docker"],"title":"GPU 성능 비교하기: Host vs Container(+Container에서 GPU 사용하기)","uri":"https://healinyoon.github.io/2020/10/20201023_gpu_performance_host_vs_container/","year":"2020"},{"content":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다.\n문제는 apt-get 등 기본 패키지 관리 도구를 사용하여 생각 없이 설치하면 기존에 운영하던 k8s 클러스터 버전과 맞지 않은 최신 버전이 설치 된다는 것입니다\u0026hellip;(그러면 저처럼 작업을 2번 하게 됩니다)\n그런데 바이너리 파일을 사용해서 설치하기는 또 귀찮고..\n따라서 apt를 사용하되, 버전을 옵션으로 주는 방식으로 설치를 하기로 했습니다.\n나중에 또 2번 작업하지 않기 위해서, 그리고 중간에 발생한 이슈도 기록해둘겸 내용을 정리하였습니다.\n설치하기 사실 기존 k8s 클러스터 설치 프로세스와 다른 점은 거의 없습니다. 기존의 프로세스는 여기를 참고 바랍니다.\n특정 버전 설치 옵션을 주는 부분만 신경써서 진행하면 됩니다.\n저장소 추가 # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF # sudo apt-get update  원래는 아래와 같이 그냥 최신 버전을 설치했다면 이번엔 옵션으로 버전을 줘야 합니다. sudo apt-get install -y kubelet kubeadm kubectl  운영 중인 클러스터의 버전 확인하기 먼저 기존에 운영 중인 k8s 클러스터의 버전을 확인합니다.\nroot@hci-k8s-master-01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-worker-01 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-worker-02 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-master-01 Ready master 76d v1.18.6  클러스터의 node들은 1.18.x 버전을 사용하는 것을 알 수 있습니다.\n버전을 확인했으니 이제 설치를 진행합니다.\n설치 가능한 버전 확인하기 저는 sudo apt-get install -y kubelet=1.18.8 이렇게 옵션을 주고 설치하려고 했는데, 애석하게도 실패했습니다.\n아래와 같은 로그가 발생합니다.\n# sudo apt-get install kubelet=1.18.8 Reading package lists... Done Building dependency tree Reading state information... Done E: Version '1.18.8' for 'kubelet' was not found  정확한 버전 옵션을 확인해보도록 합니다.\n# apt-cache madison kubeadm  출력 결과에 제가 사용하려던 1.18.8 버전은 다음과 같이 나와있습니다.\nkubeadm | 1.18.8-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages  1.18.8-00 버전으로 설치하기 이제 다시 설치를 진행해봅니다.\n원래 kubeadm만 설치해도 kubectl과 kubelet이 의존적으로 설치됩니다.\n그런데 말입니다\u0026hellip; 특이점이 발생합니다.\n# sudo apt-get install kubeadm=1.18.8-00 Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: kubectl The following NEW packages will be installed: kubeadm kubectl 0 upgraded, 2 newly installed, 0 to remove and 5 not upgraded. Need to get 0 B/16.5 MB of archives. After this operation, 82.8 MB of additional disk space will be used. Do you want to continue? [Y/n] Y Selecting previously unselected package kubectl. (Reading database ... 128167 files and directories currently installed.) Preparing to unpack .../kubectl_1.19.2-00_amd64.deb ... Unpacking kubectl (1.19.2-00) ... Selecting previously unselected package kubeadm. Preparing to unpack .../kubeadm_1.18.8-00_amd64.deb ... Unpacking kubeadm (1.18.8-00) ... Setting up kubectl (1.19.2-00) ... Setting up kubeadm (1.18.8-00) ...  위와 같이 kubectl이 1.19.2-00 버전으로 설치가 됩니다.\n이런 경우, kubectl을 downgrade 해주는 방법도 있지만..\n# apt-get install kubectl=1.18.8-00  애초에 처음부터 모두 버전을 지정해주면 됩니다.\n# sudo apt-get install -y kubelet=1.18.8-00 kubeadm=1.18.8-00 kubectl=1.18.8-00  설치된 버전 확인 하기 마지막으로 설치된 버전을 확인해보겠습니다.\n# dpkg -l kube* Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-=========================-=================-=================-======================================================== ii kubeadm 1.18.8-00 amd64 Kubernetes Cluster Bootstrapping Tool ii kubectl 1.18.8-00 amd64 Kubernetes Command Line Tool ii kubelet 1.18.8-00 amd64 Kubernetes Node Agent ii kubernetes-cni 0.8.7-00 amd64 Kubernetes CNI  Master에 join하기 참고와 동일하게 진행합니다.\nmaster node에서 join 명령어 발급받기 # kubeadm token create --print-join-command  worker node에서 join 명령어 수행하기 신규로 join하려는 worker node에서 위에서 발급받은 명령어를 수행합니다.\n# kubeadm join X.X.X.X:XX --token xxxx --discovery-token-ca-cert-hash sha256:xxxx W1113 13:04:14.543859 83613 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set. [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.  master node에서 cluster로 join된 것 확인하기 STATUS가 NotReady -\u0026gt; Ready로 변경되는데 시간이 걸릴 수 있습니다.\n# kubectl get nodes  ","id":12,"section":"posts","summary":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다. 문제는 apt-get 등 기본 패키지 관리 도구를 사용하","tags":["kubernetes"],"title":"Kuberenets 특정 버전으로 설치하기","uri":"https://healinyoon.github.io/2020/10/20201009_k8s_install_specific_version/","year":"2020"},{"content":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다.\n$ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $ cd azure-cli-with-python3 $ virtualenv -p /usr/bin/python3.5. $ source ./bin/activate $ sudo apt install python3-dev $ pip install azure-cli $ python --version  ref)  The way to configure Azure CLI to use Python 3.5 on system where the default Python version is 2.x · Issue #2529 · Azure/azure-cli  ","id":13,"section":"posts","summary":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다. $ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $","tags":["azure"],"title":"Azure CLI 설치하기 with virtualenv","uri":"https://healinyoon.github.io/2020/09/20201023_azure_cli_with_virtualenv/","year":"2020"},{"content":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다.\n구성 H/W 구성하려는 kubernetes cluster는 다음과 같습니다.\n   노드 vCPU RAM Disk     master01 2 8GiB    worker01 2 8GiB    worker02 2 8GiB     Required ports 1) Control-plane node(s)\n2) Worker node(s)\nDocker 설치(모든 node) # curl -fsSL https://get.docker.com/ | sudo sh # systemctl start docker # systemctl enable docker  Kubernetes 클러스터 구성 쿠버네티스 공식 사이트 kubeam 설치\n모든 노드에 아래의 패키지를 설치한다.\n kubeadm: 클러스터를 부트스트랩하는 명령(쿠버네티스 관리) kubelet: 클러스터의 모든 시스템에서 실행되는 구성 요소로, 포트 및 컨테이너 시작과 같은 작업을 수행(쿠버네티스 서비스) kubectl: 클러스터와 통신하기 위한 command line util(쿠버네티스 클라이언트 프로그램, 클러스터 구성과는 전혀 상관 없음)  1) Kubernetes 리포지토리 구성(모든 node) # sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -  2) Kubeadm, Kubelet, Kubectl 설치(모든 node) # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl // 패키지가 자동으로 업그레이드 되지 않도록 설정 sudo apt-mark hold kubelet kubeadm kubectl // 데몬 재시작 systemctl daemon-reload systemctl restart kubelet  3) hostname 등록(모든 node) 이때 주의할 점은 hostname에 알파벳 소문자와 숫자, \u0026lsquo;-\u0026rsquo; 기호만 가능하다.\n# sudo hostnamectl set-hostname master01 또는 # sudo hostnamectl set-hostname worker01  4) /etc/hosts 파일 수정(모든 node) # vi /etc/hosts 아래에 추가 {IP} master01 {IP} worker01 {IP} worker02  5) Iptables 설정(모든 node) 브릿지 되어있는 IPv4 트래픽을 iptables 체인으로 전달될 수 있도록 한다.\n# cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF # sudo sysctl --system  6) 스왑 기능 비활성화(모든 node) swap 끄기 # sudo swapoff -a 재부팅 후에도 swap 설정 유지 # sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab  7) 마스터 노드 초기화 # kubeadm init  kubeadm init 명령어 실행시 아래와 같이 출력된다.\nTo start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 아래 명령어를 수행하고\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config  worker node에서 아래의 명령어를 수행하여 master node와 join한다.\nkubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 kubectl get nodes 명령어를 입력하면 다음과 같이 출력된다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 NotReady master 10m v1.19.0 healin-k8s-worker01 NotReady \u0026lt;none\u0026gt; 30s v1.19.0 healin-k8s-worker02 NotReady \u0026lt;none\u0026gt; 29s v1.19.0  8) 네트워크 애플리케이션 설치 pod 네트워크 애플리케이션을 설치해야 클러스터 내의 node간 통신이 가능하다.\n사용 가능한 네트워크 옵션은 여기에서 확인할 수 있다.\n다음 명령을 master node에서 수행하여 weave pod 네트워크 애플리케이션을 설치한다.\nkubectl apply -f \u0026quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\u0026quot; 이런 오류 발생시 The connection to the server localhost:8080 was refused - did you specify the right host or port? export KUBECONFIG=/etc/kubernetes/admin.conf 을 적용해보자  master node에서 kubectl get nodes 명령어를 잠시 후 다시 입력하면 다음과 같이 STATUS가 NotReady =\u0026gt; Ready로 변경된 것을 확인할 수 있다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 Ready master 17m v1.19.0 healin-k8s-worker01 Ready \u0026lt;none\u0026gt; 7m42s v1.19.0 healin-k8s-worker02 Ready \u0026lt;none\u0026gt; 7m41s v1.19.0  9) master node를 worker node로도 사용하고 싶다면, 쿠버네티스 클러스터의 control-plane 노드는 보안상의 이유로 격리되어 있다(기본값).\nmaster node에서는 pod 가 스케줄링 되지 않으므로, 1대의 머신으로만 쿠버네티스 클러스터를 구축할 경우 격리 해제해야 한다.\n$ kubectl taint nodes –all node-role.kubernetes.io/master-  Control plane node isolation에 대한 자세한 내용은 아래 경로를 참고한다.\n* 쿠버네티스 공식 문서 - Control plane node isolation\n* Kubernetes Control-Plane Node에 Pod 띄울수 있는 방법 (Taints)\n참고 쿠버네티스(kubernetes) 설치 및 환경 구성하기\n","id":14,"section":"posts","summary":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다. 구성 H/W 구성하","tags":["docker","kubernetes"],"title":"Kubernetes Cluster 설치하기(ubuntu18.04)","uri":"https://healinyoon.github.io/2020/09/20200828_install_kubernetes_cluster_ubuntu/","year":"2020"},{"content":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지\n공식 사이트의 설명: Installing a Pod network add-on\n","id":15,"section":"posts","summary":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지 공식 사이트의 설명: Installing a Pod network add-on","tags":["kubernetes"],"title":"쿠버네티스 POD Network","uri":"https://healinyoon.github.io/2020/09/%EB%AF%B8%EC%99%8420200901_k8s_pod_network/","year":"2020"},{"content":"목적 Jenkins node heap memory 사이즈 변경 방법 정리\n참고 자료  Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양  Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인수로 받을 수 있음\nJenkins node heap memory 사이즈를 변경하기 위한 인수: -Xmx와 -Xms\n-Xms\u0026lt;size\u0026gt; set initial Java heap size -Xmx\u0026lt;size\u0026gt; set maximum Java heap size  예시 1) master의 /etc/default/jenkins에서 설정 AVA_ARGS=\u0026quot;-Xmx256m\u0026quot; # default value JAVA_ARGS=\u0026quot;-Xmx2048m\u0026quot; # 2048MB size  2) Jenkins UI에서 slave 설정 [Jenkins 관리] \u0026gt; [노드 관리] \u0026gt; \u0026lsquo;노드 선택 후 ' [설정] \u0026gt; [고급] Jenkins 권장 사양(자세히) 참고  It is recommended to define the same value for both -Xms and -Xmx so that the memory is allocated on startup rather than runtime. 대/소문자는 상관 없음(예: -Xmx10G는 -Xmx10g와 동일함) Java processes 전역에 적용하고 싶으면 JAVA_TOOL_OPTIONS 환경 변수 사용(예: export JAVA_TOOL_OPTIONS=\u0026quot;-Xmx6g\u0026quot;)  ","id":16,"section":"posts","summary":"목적 Jenkins node heap memory 사이즈 변경 방법 정리 참고 자료 Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양 Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인","tags":["jenkins"],"title":"Jenkins Node Heap Memory 사이즈 설정하기(-Xmx/-Xms 옵션)","uri":"https://healinyoon.github.io/2020/08/20200831_jenkins_node_heap_size/","year":"2020"},{"content":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다.\n 빌드 테스트를 위한 다양한 환경이 필요한 경우 크고 무거운 프로젝트의 작업 부하를 분산해야하는 경우  위의 요구사항을 충족하기 위해 Jenknis 분산 아키텍처인 Master-Slave 구성이 도입되었습니다. Jenkins Master와 Slave의 역할    구분 역할     Master * Build 작업 예약* Build 실행을 위한 작업 분배* Slave node 모니터링(필요에 따라 on/offline 전환 가능)* Build 결과 기록   Slave * Jenkins Master의 요청 수신* Build 실행    Jenkins Master-Slave 연동 방법 Jenkins Master-Slave는 다음의 요구사항을 만족시키면 매우 간단하게 구성할 수 있습니다.\n Master 서버에서 Slave 서버에 접근 가능하도록 설정 Slave 서버의 Jenkins Java 요구사항 충족  1. Jenkins 사용자 생성(모든 Slave node에서 진행) # adduser jenkins 사용자 생성 후 다음과 같이 홈디렉토리의 권한을 변경해준다 # chown ldccai:jenkins /home/jenkins # chmod 775 /home/jenkins  2. Java 8 설치(모든 Slave node에서 진행) # apt-get install openjdk-8-jdk  3. Jenkins에서 Slave node 등록 등록된 노드의 [로그]를 확인하면 다음과 같이 Master 서버와 잘 연동된 것을 확인할 수 있습니다. ","id":17,"section":"posts","summary":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다. 빌드 테스트를 위한 다양한 환경이 필","tags":["jenkins"],"title":"Jenkins Master-Slave 구성하기","uri":"https://healinyoon.github.io/2020/08/20200827_jenkins_master_slave/","year":"2020"},{"content":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미합니다.\nCI(Continuous Integration): 지속적 통합 개발자의 변경 사항이 정기적으로(최상의 경우 하루 여러번) 빌드 및 테스트 되고 공유 리포지토리에 병합되는 프로세스입니다.\nCD(Continous Deploy or ontinuous Delivery): 지속적 배포 Jez Humble의 정의\nContinuous Deployment is about automating the release of a good build to the production environment. In fact, Humble thinks it might be more accurate to call it “continuous release.”\nContinuous Delivery is about 1) ensuring that every good build is potentially ready for production release. At the very least, 2) it’s sent to the user acceptance test (UAT) environment. 3) Your business team can then decide when a successful build in UAT can be deployed to production —and they can do so at the push of a button.\n상황에 따라 이미 운영되고 있는 프로덕션 환경에 바로 release 하는 것은 문제가 될 수 있습니다. 이러한 경우로 임베디드 소프트웨어 등이 해당됩니다.\n따라서 잠재적으로는 release 가능하지만, 프로덕션 환경에 자동으로 release 되지 않는 것이 Continuous Delivery 입니다. 정리  CI: 지속적인 빌드 / 테스트 / 통합 CD: CI의 연장선 ~ Release 준비 완료(Delivery) or 제품 출시(Deploy)   Jenkins for CI/CD 다양한 CI/CD tool CI/CD 구현을 위한 다양한 tool이 있습니다. tool에 대한 자세한 정보 참고\n왜 Jenkins인가 Jenkins는 아래와 같은 강점을 가지고 있습니다.\n It is an open-source tool with great community support. It is easy to install. It has 1000+ plugins to ease your work. If a plugin does not exist, you can code it and share it with the community. It is free of cost. It is built with Java and hence, it is portable to all the major platforms.  하지만 단점 역시 존재합니다. CI/CD tool 중에서 자주 사용되는 Jenkins vs Gitlab vs Travis 비교 글을 참고하시면 Jenkins의 장단점을 이해하는데 도움이 됩니다.\n","id":18,"section":"posts","summary":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미","tags":["jenkins"],"title":"CI/CD와 Jenkins","uri":"https://healinyoon.github.io/2020/08/20200827_cicd_and_jenkins/","year":"2020"},{"content":"HUGO 글 생성하기 $ hugo new {파일명}  HUGO 블로그 빌드 $ hugo -t {테마명}  Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master  ","id":19,"section":"posts","summary":"HUGO 글 생성하기 $ hugo new {파일명} HUGO 블로그 빌드 $ hugo -t {테마명} Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메","tags":["go","hugo"],"title":"HUGO 블로그 새글 업로드하기","uri":"https://healinyoon.github.io/2020/08/20200827_hugo_blog/","year":"2020"},{"content":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부한 내용을 정리하였다.\n 1. 쉘(shell) 이란? 쉘은 사용자가 입력한 명령을 해석해서 커널에 전달하거나, 커널의 처리 결과를 사용자에게 전달하는 역할을 한다.\n1-1. 쉘 명령문 형식 쉘 명령문의 형식은 다음과 같다.\n{프롬프트} {명령어} {옵션} {인자} ex) # cd ~ ex) # ls -al ex) # rm -rf testdir ex) # cat test.txt  1-2. 쉘 환경변수 쉘은 다양한 환경변수 값을 설정할 수 있는데, 리눅스의 주요 환경변수 목록은 아래와 같다.\n   환경변수 설명     HOME 현재 사용자의 홈디렉토리   PATH 실행 파일을 찾는 디렉토리 경로   LANG 기본 지원 언어   PWD 사용자의 현재 작업 디렉토리   TERM 로그인 터미널 타입   SHELL 로그인시 사용하는 쉘   USER 현재 사용자의 이름   DISPLAY X 디스플레이 이름   HOSTNAME 호스트 이름   USERNAME 현재 사용자 이름   OSTYPE 운영체제 타입    현재 설정된 환경 변수는 echo ${환경변수 이름} 명령어를 통해 확인할 수 있다.\nex) echo $HOME ex) echo $HOSTNAME  환경변수 설정 값을 변경하려면 export {환경변수}={값} 명령어를 실행하면 된다. 그 외의 환경변수는 printenv 명령어로 확인할 수 있다. 단, 일부 환경변수는 printenv 명령을 실행해도 출력되지 않는다.\n 2. 쉘 스크립트 사용해보기 쉘 스크립트도 일반적인 프로그래밍 언어와 비슷하게 변수, 반복문, 제어문 등을 사용할 수 있다.\n2-1. 쉘 스크립트 작성하기 아래와 같이 간단한 쉘 스크립트를 작성하고 script.sh으로 저장해보자.\n#!/bin/sh echo \u0026quot;User Name: \u0026quot; $USER echo \u0026quot;User Home Directory: \u0026quot; $HOME exit 0  위의 코드를 간단히 설명하면 다음과 같다.\n 1행: bash를 사용하겠다는 의미로, 쉘 스크립트 작성시 첫 행에 반드시 입력해야한다. 2, 3행: echo 명령은 화면에 출력하는 명령이며, 먼저 문자를 출력하고 ${환경변수}에 해당하는 값을 출력한다. 4행: 종료 코드를 반환하며, 0은 쉘 스크립트 실행 성공을 의미한다.  2-2. 쉘 스크립트 실행하기 쉘 스크립트를 실행하는 방법에는 크게 두 가지가 있다.\n1) sh 명령으로 실행\nsh {실행파일 이름} ex) sh script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  2) 파일 권한을 실행 가능하게 변경하고 실행\n# chmod +x {실행파일 이름} # ./{실행파일 경로} ex) # chmod +x script.sh # ./script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  3. 변수 쉘 스크립트 변수 사용시 주의해야할 특징은 다음과 같다.\n 변수를 사용하기 전에 미리 선언하지 않으며, 처음 변수에 값이 할당되면 자동으로 변수가 생성된다. 변수에 넣는 모든 값을 \u0026lsquo;문자열\u0026rsquo;로 취급한다. 변수 이름의 대소문자를 구분한다. 변수를 대입할 때 \u0026lsquo;=\u0026rsquo; 좌우에 공백이 없어야 한다.  3-1. 변수의 입출력 \u0026lsquo;$\u0026rsquo; 라는 문자가 들어간 글자를 출력하려면 \u0026lsquo;\u0026lsquo;로 묶어주거나 앞에 \u0026lsquo;\\\u0026lsquo;를 붙여줘야 한다.\n#!/bin/sh var=\u0026quot;Hi Shell\u0026quot; echo $var echo '$var' echo \\$var exit 0 [실행 결과] Hi Shell $var $var  변수의 입력과 출력은 다음과 같이 사용할 수 있다.\n#!/bin/sh echo \u0026quot;값을 입력하세요: \u0026quot; read var echo \u0026quot;입력된 값은 \u0026quot; $var \u0026quot;입니다.\u0026quot; exit 0 [실행 결과] 값을 입력하세요: shell script study 입력된 값은 shell script study 입니다.  3-2. 숫자 계산 변수에 넣은 값은 모두 문자열이 되므로, shell script에서 연산을 하기 위해서는 expr 키워드를 사용해야한다. shell script에서 숫자 계산을 하기 위한 규칙은 다음과 같다.\n 수식과 함께 역따옴표로 묶어줘야 한다. 수식에 괄호를 사용하기 위해서는 그 앞에 역슬래시를 붙여줘야 한다. 곱하기 기호를 사용할 때도 그 앞에 역슬래시를 붙여줘야 한다.  #!/bin/sh n1=10 n2=$n1+20 echo $n2 n3=`expr $n1 + 20` echo $n3 n4=`expr \\( $n1 + 20 \\) / 10 \\* 2` echo $n4 exit 0 [실행 결과] 10+20 30 // ※ 각 단어를 띄어쓰기해야하는 것에 주의 6  3-3. 파라미터 변수 파라미터 변수는 $0, $1, $2 등의 형태를 가지며, 아래와 같이 매칭된다.\n   명령어 파라미터 1 파라미터 2 파라미터 n\u0026hellip;     $0 $1 $2 $n\u0026hellip;    예를 들면 다음과 같다.\n   ./script 1 2     $0 $1 $2    #!/bin/sh echo \u0026quot;실행파일 이름: \u0026lt;$0\u0026gt;\u0026quot; echo \u0026quot;첫 번째 파라미터: \u0026lt;$1\u0026gt;.\u0026quot; echo \u0026quot;두 번째 파라미터: \u0026lt;$2\u0026gt;.\u0026quot; echo \u0026quot;전체 파라미터는 \u0026lt;$*\u0026gt;이다.\u0026quot; exit 0 [실행 결과] 실행파일 이름: \u0026lt;./script.sh\u0026gt; 첫 번째 파라미터: \u0026lt;1\u0026gt;. 두 번째 파라미터: \u0026lt;2\u0026gt;. 전체 파라미터는 \u0026lt;1 2\u0026gt;이다.  4. if문 if문의 기본 사용법과 조건 연산자를 알아보자.\n4-1. if문 기본 문법 if문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 fi  이때 주의할 점은 [ 조건 ] 에서 각 단어에 모두 공백이 있어야 한다.\n#!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;rin_gu\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; fi exit 0 [실행 결과] 참 입니다.  4-2. if-else문 if-else문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 else문 거짓인 경우 실행 fi  #!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;shell script\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; else echo \u0026quot;거짓 입니다.\u0026quot; fi exit 0 [실행 결과] 거짓 입니다.  4-3. 조건문에 사용되는 비교 연산자 조건문에 사용디는 비교 연산자에는 문자열 비교 연산자와 산술 비교 연산자가 있다.\n문자열 비교 연산자\n   문자열 비교 내용 예시     = 같으면 참 \u0026ldquo;문자열\u0026rdquo; = \u0026ldquo;문자열\u0026rdquo;   != 같지 않으면 참 \u0026ldquo;문자열\u0026rdquo; != \u0026ldquo;문자열\u0026rdquo;   -n 문자열이 NULL 값이 아니면 참 -n \u0026ldquo;문자열\u0026rdquo;   -z 문자열이 NULL 값이면 참 -z \u0026ldquo;문자열\u0026rdquo;    산술 비교 연산자\n   산술 비교 내용 예시     -eq 두 수식이 같으면 참 수식 -eq 수식   -ne 두 수식이 같지 않으면 참 수식 -ne 수식   -gt 왼쪽 수식이 크면 참 수식 -gt 수식   -ge 왼쪽 수식이 크거나 같으면 참 수식 -ge 수식   -lt 왼쪽 수식이 작으면 참 수식 -lt 수식   -le 왼쪽 수식이 작거나 같으면 참 수식 -le 수식   ! 수식이 거짓이라면 참 !수식    #!/bin/sh if [ 1024 -eq 2024 ] then echo \u0026quot;1024과 2024는 같다.\u0026quot; else echo \u0026quot;1024과 2024는 다르다.\u0026quot; fi exit 0 [실행 결과] 1024과 2024는 다르다.  4-4. 조건문에 사용되는 파일과 관련된 연산자 if문에서 파일을 처리하는 조건 연산자는 다음과 같다.\n파일 조건 | 내용 | 예시 \u0026mdash; | \u0026mdash; -d | 파일이 디렉토리면 참 | -d 파일명 -e | 파일이 존재하면 참 | -e 파일명 -f | 파일이 일반 파일이면 참 | -f 파일명 -g | 파일에 set-group-id가 설정되면 참 | -g 파일명 -r | 파일이 읽기 가능하면 참 | -r 파일명 -s | 파일 크기가 0이 아니면 참 | -s 파일명 -u | 파일에 set-user-id가 설정되면 참 | -u 파일명 -w | 파일이 쓰기 가능하면 참 | -w 파일명 -x | 파일이 실행 가능하면 참 | -x 파일명\n 5. case문 case문의 기본 사용법을 알아보자\n5-1. case문 기본 문법 case문의 기본 문법은 다음과 같다.\ncase [ 변수 ] in 변수값1) 변수와 변수값1이 일치할 경우 실행 변수값2) 변수와 변수값2가 일치할 경우 실행 ... esac  5-2. case 문 사용해보기 (작성중)\n","id":20,"section":"posts","summary":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부","tags":["shell"],"title":"Shell Scripts 사용하기","uri":"https://healinyoon.github.io/2019/06/20190602_shell_script/","year":"2019"},{"content":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다.\n 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry   2. Docker 실행 및 정보 확인 2-1. Docker 실행하기 # 시스템 부팅 시 docker를 시작하도록 설정 systemctl enable docker.service # Docker 실행 systemctl start docker.service # Docker 상태 확인 systemctl status docker.service  2-2. Docker 명령어 사용하기 Docker 명령어의 기본 형식은 docker {명령어} 입니다.\n# Docker 버전 확인 docker version # Docker 실행 환경 확인 docker system info # Docker 디스크 상태 확인 docker system df # 그 외 Docker 명령어 살펴보기 docker -help   3. Docker 사용해보기 Docker가 정상적으로 동작하는지 확인하기 위해 컨테이너를 실행합니다.\n3-1. Docker 컨테이너 실행하기 # Docker 컨테이너 실행 명령어 docker run {옵션} {컨테이너 명/ID}  docker run 명령어는 컨테이너를 생성하고 실행시키는 명령어 입니다. 이때 dockers는 로컬에 해당 이미지가 있는지 학인하고, 없는 경우 docker hub에서 pull을 먼저 진행하고 컨테이너를 생성합니다.\n3-2. Hello, world! docker run hello-world  hello-world 컨테이너가 실행되면 다음과 같이 메세지가 출력됩니다.\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.  3-3. Nginx 이번에는 Nginx 웹서버를 docker로 설치해보겠습니다.\n# Nginx 이미지 다운로드 docker pull nginx # 다운로드한 이미지 확인 docker images # Nginx 컨테이너 실행 docker run --name nginx-webserver -d -p 80:80 nginx  docker run 명령어에서 자주 쓰이는 옵션은 다음과 같습니다.\n \u0026ndash;name: 컨테이너의 이름을 설정한다. -d: 컨테이너를 백그라운드에서 실행시킨다. -p: 호스트 포트와 컨테이너 내부의 포트를 매핑한다(형식: -p {host 포트}:{컨테이너 포트}).  실행중인 docker 컨테이너를 확인하고 싶을 때는 docker ps 명령어로 확인 할 수 있습니다.\n# 실행중인 컨테이너 확인 docker ps # 컨테이너 상태 확인 docker container stats  마지막으로 http://localhost:80으로 접속해서 Nginx 웹 브라우저를 확인합니다.\n 참고 사이트  https://niceman.tistory.com/36 https://futurecreator.github.io/2018/11/16/docker-container-basics/ http://pyrasis.com/Docker/Docker-HOWTO  ","id":21,"section":"posts","summary":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다. 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry 2. Docker 실행 및","tags":["docker"],"title":"Docker 설치하기(CentOS)","uri":"https://healinyoon.github.io/2019/06/20190611_docker_install/","year":"2019"}],"tags":[{"title":"azure","uri":"https://healinyoon.github.io/tags/azure/"},{"title":"docker","uri":"https://healinyoon.github.io/tags/docker/"},{"title":"gitlab","uri":"https://healinyoon.github.io/tags/gitlab/"},{"title":"go","uri":"https://healinyoon.github.io/tags/go/"},{"title":"helm","uri":"https://healinyoon.github.io/tags/helm/"},{"title":"hugo","uri":"https://healinyoon.github.io/tags/hugo/"},{"title":"java","uri":"https://healinyoon.github.io/tags/java/"},{"title":"jenkins","uri":"https://healinyoon.github.io/tags/jenkins/"},{"title":"kubernetes","uri":"https://healinyoon.github.io/tags/kubernetes/"},{"title":"openjdk","uri":"https://healinyoon.github.io/tags/openjdk/"},{"title":"shell","uri":"https://healinyoon.github.io/tags/shell/"},{"title":"troubleshooting","uri":"https://healinyoon.github.io/tags/troubleshooting/"}]}