{"categories":[{"title":"blog","uri":"https://healinyoon.github.io/categories/blog/"},{"title":"ci/cd","uri":"https://healinyoon.github.io/categories/ci/cd/"},{"title":"cloud","uri":"https://healinyoon.github.io/categories/cloud/"},{"title":"devops","uri":"https://healinyoon.github.io/categories/devops/"},{"title":"gpu","uri":"https://healinyoon.github.io/categories/gpu/"},{"title":"jvm","uri":"https://healinyoon.github.io/categories/jvm/"},{"title":"linux","uri":"https://healinyoon.github.io/categories/linux/"},{"title":"msa","uri":"https://healinyoon.github.io/categories/msa/"},{"title":"programming","uri":"https://healinyoon.github.io/categories/programming/"}],"posts":[{"content":"Docker image는 container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하는데 필요한 단계를 정의하는 Dockerfile를 작성하여 고유한 image를 빌드할 수도 있다. Dockerfile의 각 단계에 정의된 명령어는 image에 layer를 만든다. Docker image를 이해하기 위해서는 이 Layer에 대한 이해가 필요하다.\nImage Layer 앞서 설명한 바와 같이 image는 Dockerfile로 빌드된다. Dockerfile을 작성하고 빌드하는 과정을 통해 image layer에 대해 이해해보자. Dockerfile은 다음과 같이 단계별로 구성된다.\n// step1 FROM alpine:3.10 // step2 ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;hello\u0026quot;]  Dockerfile을 빌드하면 아래와 같이 각 단계별로 image가 생성되는 것을 확인할 수 있다. 즉 Dockerfile을 빌드하면 Dockerfile의 단계별로 image layer가 생성되며, 이들이 계층적으로 하나씩 쌓이며 image를 이루게 된다.\n$ sudo docker build --tag echo:1.0 . Sending build context to Docker daemon 7.168kB FROM alpine:3.10 Step 1/2 : FROM alpine:3.10 3.10: Pulling from library/alpine 21c83c524219: Already exists Digest: sha256:f0e9534a598e501320957059cb2a23774b4d4072e37c7b2cf7e95b241f019e35 Status: Downloaded newer image for alpine:3.10 ---\u0026gt; be4e4bea2c2e Step 2/2 : ENTRYPOINT [\u0026quot;echo\u0026quot;, \u0026quot;hello\u0026quot;]` ---\u0026gt; Running in 2dbb42b167e8 FROM ubuntu:18:04 Removing intermediate container 2dbb42b167e8 ---\u0026gt; 75b05f96c44d Successfully built 75b05f96c44d Successfully tagged echo:1.0  Container Layer Image를 빌드한 후 docker container run 명령을 수행하면 아래와 같이 가장 마지막에 container layer를 생성한다(이 container layer는 container를 삭제하면 같이 삭제 된다).\n그런데 실제로 container를 사용할 때는 하나의 파일 시스템으로 보이는데, 이렇게 계층적으로 나눠진 image들이 어떻게 하나의 파일 시스템으로 보이는 것일까? 바로 Union FS 덕분이다.\nUnion FS Union Mount Union Mount는 복수의 파일 시스템을 하나의 파일 시스템으로 마운트하는 기능으로 두 파일 시스템에서 동일한 파일이 있다면 나중에 마운트 된 파일 시스템의 파일을 Overlay한다. 하위 파일에 대한 쓰기 작업은 CoW(Copy on Write) 전략에 따라 복사본을 생성하여 수행하므로 원본 파일 시스템은 변하지 않는 것이 특징이다.\nDocker Image: Union File System Docker image는 Union File System 기반으로 동작한다. Union File Systme의 특성에 따라 하위 layer는 읽기 전용이며, CoW 전략에 의해 쓰기 작업은 상위 레이어로 복사해서 이루어지기 때문에 하나의 image로 부터 복수의 container가 실행 가능한 것이다.\n  Container Layer\n Write layer 각 container마다 최 상단 layer에 생성되어, container마다 자신만의 상태를 가질 수 있게 해준다. container가 생성된 후 모든 변경 작업은 여기서 이루어진다. R/W 속도가 느리다.    Image Layer\n Read only layer 다른 container와 공유 가능하다.    Image Layer 디렉토리 파악하기 Image 정보 확인 nginx image를 다운받아보자.\n Image pull  # sudo docker pull nginx Using default tag: latest latest: Pulling from library/nginx 852e50cd189d: Already exists 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest   Image 정보 확인 Image의 정보는 docker inspect {image} 명령어로 확인할 수 있다.  # docker inspect nginx [ { \u0026quot;Id\u0026quot;: \u0026quot;sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221c\u0026quot;, \u0026quot;RepoTags\u0026quot;: [ \u0026quot;nginx:latest\u0026quot; ], (중략)  Image 저장소 위치 확인 docker image 저장소 위치는 docker info 명령어로 확인할 수 있다.\n# docker info Client: Debug Mode: false Server: Containers: 57 Running: 16 Paused: 0 Stopped: 41 Images: 149 Server Version: 19.03.13 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: false Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 8fba4e9a7d01810a393d5d25a3621dc101981175 runc version: dc9208a3303feef5b3839f4323d9beb36df0a9dd init version: fec3683 Security Options: apparmor seccomp Profile: default Kernel Version: 5.4.0-1031-azure Operating System: Ubuntu 18.04.5 LTS OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 7.749GiB Name: master ID: 5HUW:6SVP:6Q4Q:LDGN:YMF2:RBDM:VEXF:3UKJ:XVTV:5SRK:SS7R:TPJ2 Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support  Layer 디렉토리  디렉토리 구조 Image layer 정보를 확인하기 위한 디렉토리만 살펴보면 다음과 같다.  /var/lib/docker# tree docker . docker ├── containers: docker container 정보를 저장한다. ├── image: docker image 정보를 저장한다. │ └── overlay2 │ ├── imagedb: imagedb에 대한 정보는 layerdb에 저장된다. │ └── layerdb: layerdb에 대한 정보는 overlay2에 저장된다. ├── overlay2: docker image의 파일 시스템이 저장된다. 실질적으로 image layer 데이터가 저장되는 경로이다. │ ├── 0090fbeed32cba3aed09c2459d4a5f59144be127dfed23cbe8c7f47982dd3c12 │ ├── 014997dffd51a7e9a1418e7f888097c360c592ddb791c0973b35b9935a1eea9d   각 디렉토리별 용량 위의 각 경로의 데이터 용량을 조회해보면 다음과 같다. 실제로 docker 데이터가 저장되는 root 경로인 /var/lib/docker 디렉토리 데이터 용량과 /var/lib/docker/overlay2 디렉토리 데이터 용량이 가장 근접한 것을 볼 수 있다(즉 실질적인 image layer 데이터가 여기에 저장된다는 것).  # du -sh /var/lib/docker 9.1G /var/lib/docker # du -sh /var/lib/docker/containers/ 39M /var/lib/docker/containers/ # du -sh /var/lib/docker/image/ 11M /var/lib/docker/image/ # du -sh /var/lib/docker/overlay2/ 6.0G /var/lib/docker/overlay2/  참고  https://docs.docker.com/get-started/overview/ https://nirsa.tistory.com/63 https://devaom.tistory.com/5  ","id":0,"section":"posts","summary":"Docker image는 container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하","tags":["docker"],"title":"Docker Image의 핵심 기술: Layer","uri":"https://healinyoon.github.io/2020/11/20201125_docker_image_layer/","year":"2020"},{"content":"The Docker platform Docker는 어플리케이션을 패키징하고 실행할 수 있는 Container라는 격리된 환경을 제공한다. Container는 hypervisor의 추가 로드가 필요하지 않기 때문에 경량이지만 호스트 머신의 커널 내에서 직접 실행된다.\nDocker architecture Docker는 client-server 아키텍처를 사용한다. Docker client는 Docker daemon과 통신한다. Docker daemon은 cotainer를 빌드, 실행, 배포하는 작업을 수행한다. Docker client와 daemon은 동일한 시스템에서 실행되거나, 원격으로 연결할 수도 있습니다. Docker client와 daemon은 UNIX 소켓 또는 네트워크 인터페이스를 통해 REST API로 통신한다.\nDocker daemon Docker daemon(=dockerd)은 Docker API 요청을 수신하고 image, container, network, volume과 같은 Docker 객체를 관리한다. Docker daemon의 상세 내용은 여기를 참고하자.\nDocker client Docker client(=docker)는 사용자가 입력한 명령어를 Docker daemon에 전달한다. 이때의 동작 흐름은 다음과 같다.\n 사용자가 docker 명령어 입력 Docker client는 Docker deamon에게 명령어 전달 Docker daemon은 명령어를 파싱하고 해당하는 작업 수행 Docker daemon은 수행 결과를 Docker client에게 반환 Docker client는 사용자에게 결과를 출력  Docker registries Docker registry는 Docker image를 저장한다. Docker Hub는 누구나 사용 가능한 public registry로, Docker는 기본적으로 Docker Hub에서 image를 찾도록 구성된다. private registry 구축도 가능하다.\ndocker pull 또는 docker run 명령어를 사용하면 registry에서 image를 가져온다. docker push 명령을 사용하면 image가 구성된 registry로 push 된다.\nDocker objects Docker를 사용하면 image, container, network, volume, plugin 및 기타 object를 생성하고 사용하게 된다.\nImages Docker container를 만들기 위한 템플릿이다. registry에 존재하는 image를 가져와 사용할 수도 있고, image를 만들고 실행하는데 필요한 단계를 정의하는 Dockerfile를 작성하여 고유한 image를 빌드할 수도 있다.\nContainers Docker container는 image의 실행 가능한 인스턴스이다. Docker API 또는 CLI를 사용하여 container를 생성, 시작, 중지, 이동 또는 삭제 할 수 있다. container는 image와 사용자가 생성하거나 시작할 때 제공하는 구성 옵션에 의해 정의된다. container가 제거되면 영구 저장소에 저장되지 않은 데이터의 변경 사항들은 함께 제거된다.\n 예제 docker run 명령 다음 명령어는 ubuntu container를 실행하고 container 내부와 대화형 command 창을 연결한다.\n $ docker run -i -t ubuntu /bin/bash  위의 명령어를 입력하면 다음 절차가 순차적으로 진행된다.\n ubuntu image가 local에 없는 경우, Docker는 자동으로 docker pull ubuntu 명령을 수행하여 registry에서 image를 가져온다. Docker는 docker container create 명령을 수행하여 새로운 container를 생성한다. Docker는 container layer(Read/Write 가능)를 최종 layer로 container에 할당한다. 이를 통해 실행 중인 container는 container 내부의 로컬 파일 시스템에서 파일과 디렉토리를 읽고 쓰는 것이 가능하다. 네트워크 옵션을 지정하지 않았으므로, Docker는 container를 기본 네트워크에 연결하는 인터페이스를 만든다. 여기에는 container에 IP 주소를 할당하는 것도 포함된다. Docker는 container를 시작하고 /bin/bash 명령을 실행시킨다. container가 실행중이며 -i와 -t 옵션을 사용하였기 때문에 container 내부에서 명령 입력이 가능하다. exit 명령을 사용하여 container를 빠져나오면 container가 중지되지만 제거되지는 않는다. 다시 시작하거나 제거할 수 있다.  Container technology 참고  https://docs.docker.com/get-started/overview/ https://velog.io/@labyu/docker-3  ","id":1,"section":"posts","summary":"The Docker platform Docker는 어플리케이션을 패키징하고 실행할 수 있는 Container라는 격리된 환경을 제공한다. Container는 hypervisor","tags":["docker"],"title":"Docker 개요","uri":"https://healinyoon.github.io/2020/11/20201125_docker_overview/","year":"2020"},{"content":"jdk8 설치 버전 확인 $ java -version  repository 업데이트 $ sudo apt-get update  openjdk 설치 $ sudo apt-get install openjdk-8-jdk  설치 확인 $ java -version openjdk version \u0026quot;1.8.0_275\u0026quot; OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01) OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)  환경 변수 설정 javac 설치 경로 확인 # javac -version javac 1.8.0_275 # which javac /usr/bin/javac # readlink -f /usr/bin/javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac  환경 변수 설정 # vi /etc/profile 맨 아래에 추가 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/javac  적용 # source /etc/profile  확인 # echo $JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/bin/javac  ","id":2,"section":"posts","summary":"jdk8 설치 버전 확인 $ java -version repository 업데이트 $ sudo apt-get update openjdk 설치 $ sudo apt-get install openjdk-8-jdk 설치 확인 $ java -version openjdk version \u0026quot;1.8.0_275\u0026quot; OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01) OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode) 환경 변수 설정 javac 설치 경로 확인 # javac -version javac 1.8.0_275 # which","tags":["java","openjdk"],"title":"java(openjdk) 설치 및 환경 변수 설정","uri":"https://healinyoon.github.io/2020/11/20201113_install_java/","year":"2020"},{"content":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자.\n사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은 조건이 있다면 선행해주자.\n1. kubernetes cluster 구축 아직 구축이 되어 있지 않다면 Kubernetes Cluster 설치하기(ubuntu18.04)를 참고하여 진행\n2. nvidia-docker 설치 아직 설치가 되어 있지 않다면 nvidia-docker install guide를 참고하여 진행\n1. Nvidia Plugin Pod 생성 ref)  Nvidia k8s-device-plugin 공식 사이트 Nvidia docker 공식 사이트  1.1 가장 정보가 많았던 YAML으로 설치, 그러나 실패 처음에 여기 링크를 참고하여 진행했다.\n다만 아래와 같이 버전만 변경하여 실행했다.\n$ kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml daemonset.extensions/nvidia-device-plugin- daemonset-1.12 created  하지만 아래 이슈 발생해서 실패했다 ↓ ↓ ↓\n1.2. 이슈 1.2.1. 이슈 내용 쿠버네티스 1.15 버전 이하를 설치했을 경우 문제 없겠지만, 1.16 버전 이상을 설치했을 경우 다음과 같은 에러가 발생한다.\nerror: unable to recognize \u0026quot;https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml\u0026quot;: no matches for kind \u0026quot;DaemonSet\u0026quot; in version \u0026quot;extensions/v1beta1\u0026quot;  이는 쿠버네티스 버전이 업그레이드 되면서, Daemonset의 extensions/v1beta1 버전을 더이상 지원하지 않기 때문이다.\n→ 1) 따라서 버전을 apps/v1 으로 변경하고 selector object를 추가한 후\n→ 2) k8s-device-plugin을 다시 설치하고\n→ 3) 매니패스트 파일도 적절하게 수정해주었다.\nref)  Kubectl convert 참고 자료 No matches 이슈 해결 자료   (참고) Pod 생성 실패시 에러 정보 확인\n $ kubectl describe pod {pod 명}  1.2.2. 변경한 YAML 파일을 사용하여 DaemonSet Pod 생성 이제 커스터 마이징한 YAML 파일로 Pod를 생성해보자.\n gpu-plugin.yaml\n apiVersion: apps/v1 kind: DaemonSet metadata: name: nvidia-device-plugin-daemonset-1.12 namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: name: nvidia-device-plugin-ds template: metadata: # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler # reserves resources for critical add-on pods so that they can be rescheduled after # a failure. This annotation works in tandem with the toleration below. annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; labels: name: nvidia-device-plugin-ds spec: tolerations: # Allow this pod to be rescheduled while the node is in \u0026quot;critical add-ons only\u0026quot; mode. # This, along with the annotation above marks this pod as a critical add-on. - key: CriticalAddonsOnly operator: Exists - key: nvidia.com/gpu operator: Exists effect: NoSchedule containers: - image: nvidia/k8s-device-plugin:1.11 name: nvidia-device-plugin-ctr securityContext: allowPrivilegeEscalation: false capabilities: drop: [\u0026quot;ALL\u0026quot;] volumeMounts: - name: device-plugin mountPath: /var/lib/kubelet/device-plugins volumes: - name: device-plugin hostPath: path: /var/lib/kubelet/device-plugins nodeSelector: gpus: \u0026quot;true\u0026quot;  위의 매니패스트 주요 사항은 다음과 같다.\n① 리소스 유형 = DaemonSet\nkind: DaemonSet  따라서 기본적으로는 모든 worker 노드 하나씩 동작하게 한다.  ② RollingUpdate\nspec.selector.matchLabels.name\nname: nvidia-device-plugin-ds  spec.template.matadata.labels.name\nlabels: name: nvidia-device-plugin-ds  name object가 nvidia-device-plugin-ds 인 리소스에 대하여 RollingUpdate를 설정한다.\n③ node Label 지정\nspec.template.spec.nodeSelector 로 어느 노드의 DaemonSet으로 띄워줄 것인지 레이블링해준다.\nnodeSelector: gpus: \u0026quot;true\u0026quot;  1.3. gpu-plugin DeamonSet Pod 정상 동작 확인 $ kubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-d7t2g 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-jmcg9 1/1 Running 0 48d nvidia-device-plugin-daemonset-1.12-zhsqm 1/1 Running 0 4h8mubectl -n kube-system get pod -l name=nvidia-device-plugin-ds NAME READY STATUS RESTARTS AGE nvidia-device-plugin-daemonset-1.12-7kh27 1/1 Running 0 27m $ kubectl -n kube-system logs -l name=nvidia-device-plugin-ds 2020/07/01 05:02:30 Loading NVML 2020/07/01 05:02:30 Fetching devices. 2020/07/01 05:02:30 Starting FS watcher. 2020/07/01 05:02:30 Starting OS watcher. 2020/07/01 05:02:30 Starting to serve on /var/lib/kubelet/device-plugins/nvidia.sock 2020/07/01 05:02:30 Registered device plugin with Kubelet  🌟🌟 여기서 잠깐! 🌟🌟 중요한 사항은 gpu를 사용하려는 Worker node가 gpus: \u0026quot;true\u0026quot; 레이블링이 되어 있어야 한다는 것이다.\n 만약 GPU가 있는 node인데 해당 DeamonSet이 올라가있지 않거나 신규 Worker node를 추가하려는 경우  ⇒ kubectl label nodes {Worker node 명} gpus=true 로 레이블링을 해주자.\n2. GPU 개수 확인 이제 쿠버네티스가 사용 가능한 GPU 개수를 확인해보자.\nmaster node에서 아래의 명령어를 실행하면 각 worker node에서 사용 가능한 GPU 개수가 출력된다.\n$ kubectl get nodes \u0026quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\u0026quot; NAME GPU gpu-1080ti-XX 9 gpu-1080ti-XX 9  3. Pod에서 그래픽 카드 명령어 테스트 3.1. GPU를 사용하는 Pod 생성하기 3.1.1. YAML 파일  gpu-k8s.yaml\n apiVersion: v1 kind: Pod metadata: name: gpu-k8s spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null resources: requests: nvidia.com/gpu: 2 limits: nvidia.com/gpu: 2  ref) nvidia/cuda 도커 이미지 버전이 맞지 않은 이슈 발생 시 ⇒ 도커 허브에서 맞는 이미지 버전을 찾아서 사용해주면 된다.\n Docker Hub Kubernetes Resource Request와 Limit의 이해 Schedule GPUs  3.1.2. Pod 생성 및 확인 $ kubectl apply -f gpu-k8s.yaml pod/gpu-k8s created $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 12s  3.1.3. nvidia-smi 확인 $ kubectl logs gpu-k8s Thu Jul 2 06:00:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 30C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.2. 2개의 Pod를 띄워서 gpu 4개를 모두 사용하기 gpu-k8s2.yaml 매니패스트 파일을 하나 더 만들어서 위와 동일하게 실행해보자.\n3.2.1. 결과 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-k8s 1/1 Running 0 2m44s gpu-k8s2 1/1 Running 0 2  3.2.2. nvidia-smi 확인 $ kubectl logs gpu-k8s2 Thu Jul 2 06:03:06 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 31C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  3.3. 컨테이너가 노드의 모든 GPU를 사용 가능하게 하고 싶다면  request와 limit 설정 부분을 없애주면 된다. 특이한 점은 이미 다른 파드에 GPU를 모두 할당 해준 상태에서도 파드 생성 가능하다.  3.3.1. YAML 파일 apiVersion: v1 kind: Pod metadata: name: gpu-all spec: containers: - name: gpu-container image: nvidia/cuda:9.0-runtime env: - name: DP_DISABLE_HEALTHCHECKS value: \u0026quot;xids\u0026quot; command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; args: - nvidia-smi \u0026amp;\u0026amp; tail -f /dev/null  3.3.2. Pod 실행 $ kubectl apply -f gpu-all.yaml pod/gpu-all created  3.3.3. 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE gpu-all 1/1 Running 0 50s gpu-k8s 1/1 Running 0 42m gpu-k8s2 1/1 Running 0 40m $ kubectl logs gpu-all Thu Jul 2 06:42:25 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.95.01 Driver Version: 440.95.01 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 108... Off | 00000000:18:00.0 Off | N/A | | 29% 32C P8 7W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 GeForce GTX 108... Off | 00000000:3B:00.0 Off | N/A | | 29% 33C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 2 GeForce GTX 108... Off | 00000000:86:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 3 GeForce GTX 108... Off | 00000000:AF:00.0 Off | N/A | | 29% 31C P8 8W / 250W | 0MiB / 11178MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+  ","id":3,"section":"posts","summary":"Intro Kubernetes에서 GPU를 사용하도록 환경을 구축하고 사용해보자. 사전 요구 사항 아래의 내용이 완료되었다는 전제로 진행하므로, 아직 충족되지 않은","tags":["docker","kubernetes"],"title":"Kubernets(쿠버네티스) with GPU 구축하기","uri":"https://healinyoon.github.io/2020/10/20201023_kubernetes_with_gpu/","year":"2020"},{"content":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주하는 질문이므로, 이번 기회에 정리를 해보려고 한다.\n1. Host vs Container GPU 성능 테스트 요약 1.1. 결론 GPU on Host VS Container를 비교하였을 때 속도의 성능 차이가 없었음\n1.2. 상세 내용  GPU Performance on Host VS Container 비교시 정확도에는 차이가(모델이 같으므로) 없으므로, 속도만 비교하였다. GPU를 1개 사용하였을 경우와 2개 사용하였을 경우를 나누어 비교하였다. 동일한 조건에 대해 테스트 케이스(횟수)를 총 2회씩 수행하였다.  2. GPU 성능 비교를 위한 Host 환경 설정 2.1. 가상환경 생성 # pip3 install virtualenv # virtualenv /home/rin_gu/PerformTestEnv/performTestEnv  2.2. Tensorflow 설치 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # python -m pip install --upgrade pip # sudo -H pip install --upgrade tf-nightly-gpu  2.3. 성능 테스트 코드 실행 # source /home/rin_gu/PerformanceTest/performTestEnv/bin/activate # cd /home/rin_gu/PerformanceTest/ # git clone https://github.com/tensorflow/benchmarks.git # cd benchmarks  3. GPU 성능 비교를 위한 Container 환경 설정 3.1. 컨테이너 이미지 다운로드 root@ubuntu:~# docker pull tensorflow/tensorflow:nightly-gpu  3.2. 컨테이너 run with GPU root@ubuntu:~# docker container run -d --gpus all -it tensorflow/tensorflow:nightly-gpu 94515f052adbddf25bb9c66e5d5d7ee6a6010cfc74c6936f3b01bdf764202488  3.3. 컨테이너 접속 root@ubuntu:~# docker exec -it 94515f052adb /bin/bash \u0026quot;docker exec\u0026quot; requires at least 2 arguments. See 'docker exec --help'. Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in a running container root@ubuntu:~# docker container exec -it 94515f052adb /bin/bash ________ _______________ ___ __/__________________________________ ____/__ /________ __ __ / _ _ \\_ __ \\_ ___/ __ \\_ ___/_ /_ __ /_ __ \\_ | /| / / _ / / __/ / / /(__ )/ /_/ / / _ __/ _ / / /_/ /_ |/ |/ / /_/ \\___//_/ /_//____/ \\____//_/ /_/ /_/ \\____/____/|__/ WARNING: You are running this container as root, which can cause new files in mounted volumes to be created as the root user on your host machine. To avoid this, run the container by specifying your user's userid: $ docker run -u $(id -u):$(id -g) args...  3.4. 파이썬 버전 확인 root@282efcfdc3c8:/# python Python 3.6.9 (default, Apr 18 2020, 01:56:04) [GCC 8.4.0] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import tensorflow as tf 2020-06-22 11:16:19.882107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 \u0026gt;\u0026gt;\u0026gt; tf.__version__ '2.3.0-dev20200621'  3.5. git 설치 / 리포지토리 다운로드 root@94515f052adb:/# apt-get install git root@94515f052adb:/# git clone https://github.com/tensorflow/benchmarks.git  4. Host vs Container 성능 비교 4.1. GPU 1개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스} python tf_cnn_benchmarks.py --num_gpus=1 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 124.77 126.95 model: Resnet50   2 125.76 125.07 model: Resnet50    4.2. GPU 2개 사용 (performTestEnv) # CUDA_VISIBLE_DEVICES={GPU 디바이스1, GPU 디바이스2} python tf_cnn_benchmarks.py --num_gpus=2 --batch_size={배치사이즈} --model={모델명} --variable_update=parameter_server     테스트 케이스(횟수) Host images/sec Container images/sec 비고     1 242.60 240.64 model: Resnet50   2 241.52 236.72 model: Resnet50    5. GPU 성능 측정 상세 - Host 5.1. 테스트 1 5.1.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 127.9 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 126.4 +/- 0.2 (jitter = 0.4)\t7.849 20\timages/sec: 126.3 +/- 0.1 (jitter = 0.5)\t8.013 30\timages/sec: 126.2 +/- 0.1 (jitter = 0.7)\t7.940 40\timages/sec: 126.0 +/- 0.1 (jitter = 0.7)\t8.137 50\timages/sec: 125.8 +/- 0.1 (jitter = 0.6)\t8.052 60\timages/sec: 125.6 +/- 0.1 (jitter = 0.7)\t7.782 70\timages/sec: 125.4 +/- 0.1 (jitter = 0.9)\t7.856 80\timages/sec: 125.3 +/- 0.1 (jitter = 1.0)\t8.011 90\timages/sec: 125.1 +/- 0.1 (jitter = 1.2)\t7.843 100\timages/sec: 124.8 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 124.77 ----------------------------------------------------------------  5.2. 테스트 2 5.2.1. 조건 (performTestEnv) # CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중량) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  5.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 246.2 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 244.9 +/- 0.6 (jitter = 1.7)\t7.892 20\timages/sec: 244.3 +/- 0.4 (jitter = 2.6)\t7.968 30\timages/sec: 244.5 +/- 0.4 (jitter = 2.3)\t7.934 40\timages/sec: 244.2 +/- 0.3 (jitter = 2.6)\t8.016 50\timages/sec: 243.8 +/- 0.3 (jitter = 2.6)\t7.922 60\timages/sec: 243.6 +/- 0.3 (jitter = 2.0)\t7.872 70\timages/sec: 243.5 +/- 0.2 (jitter = 1.9)\t7.837 80\timages/sec: 243.3 +/- 0.2 (jitter = 1.8)\t7.850 90\timages/sec: 243.0 +/- 0.2 (jitter = 2.1)\t7.859 100\timages/sec: 242.7 +/- 0.2 (jitter = 2.2)\t7.946 ---------------------------------------------------------------- total images/sec: 242.60 ----------------------------------------------------------------  6. GPU 성능 측정 상세 - Container 6.1. 테스트 1 6.1.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=3 python tf_cnn_benchmarks.py --num_gpus=1 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13968 MB memory) -\u0026gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 64 global 64 per device Num batches: 100 Num epochs: 0.00 Devices: ['/gpu:0'] // GPU 1개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.1.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 129.2 +/- 0.0 (jitter = 0.0)\t7.608 10\timages/sec: 128.7 +/- 0.4 (jitter = 0.9)\t7.849 20\timages/sec: 128.4 +/- 0.2 (jitter = 1.1)\t8.013 30\timages/sec: 128.4 +/- 0.2 (jitter = 0.8)\t7.940 40\timages/sec: 128.3 +/- 0.1 (jitter = 0.9)\t8.136 50\timages/sec: 128.1 +/- 0.1 (jitter = 0.9)\t8.053 60\timages/sec: 127.9 +/- 0.1 (jitter = 1.1)\t7.784 70\timages/sec: 127.7 +/- 0.1 (jitter = 1.4)\t7.859 80\timages/sec: 127.5 +/- 0.1 (jitter = 1.4)\t8.014 90\timages/sec: 127.3 +/- 0.1 (jitter = 1.4)\t7.842 100\timages/sec: 127.1 +/- 0.1 (jitter = 1.4)\t8.090 ---------------------------------------------------------------- total images/sec: 126.95 ----------------------------------------------------------------  6.2. 테스트 2 6.2.1. 조건 root@94515f052adb:# CUDA_VISIBLE_DEVICES=2,3 python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --variable_update=parameter_server (중략) Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13968 MB memory) -\u0026gt; physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5) TensorFlow: 2.3 Model: resnet50 Dataset: imagenet (synthetic) Mode: training SingleSess: False Batch size: 128 global 64 per device Num batches: 100 Num epochs: 0.01 Devices: ['/gpu:0', '/gpu:1'] // GPU 2개 사용 NUMA bind: False Data format: NCHW Optimizer: sgd Variables: parameter_server  6.2.2. 결과 Done warm up Step\tImg/sec\ttotal_loss 1\timages/sec: 240.6 +/- 0.0 (jitter = 0.0)\t7.749 10\timages/sec: 243.4 +/- 0.8 (jitter = 3.5)\t7.892 20\timages/sec: 243.3 +/- 0.5 (jitter = 2.7)\t7.968 30\timages/sec: 243.1 +/- 0.4 (jitter = 2.8)\t7.934 40\timages/sec: 242.6 +/- 0.3 (jitter = 2.3)\t8.019 50\timages/sec: 242.4 +/- 0.3 (jitter = 2.0)\t7.923 60\timages/sec: 242.1 +/- 0.3 (jitter = 1.8)\t7.878 70\timages/sec: 241.8 +/- 0.2 (jitter = 1.6)\t7.834 80\timages/sec: 241.5 +/- 0.2 (jitter = 1.6)\t7.861 90\timages/sec: 241.1 +/- 0.2 (jitter = 2.0)\t7.852 100\timages/sec: 240.8 +/- 0.3 (jitter = 2.1)\t7.948 ---------------------------------------------------------------- total images/sec: 240.64 ----------------------------------------------------------------  ","id":4,"section":"posts","summary":"Intro 회사에서 쿠버네티스 업무를 하다가 \u0026ldquo;컨테이너 위에서도 GPU 성능이 보장되는가?\u0026ldquo;라는 질문이 나왔다. 쿠버네티스 운영 중에 자주 마주","tags":["docker"],"title":"GPU 성능 비교하기: Host vs Container(+Container에서 GPU 사용하기)","uri":"https://healinyoon.github.io/2020/10/20201023_gpu_performance_host_vs_container/","year":"2020"},{"content":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다.\n문제는 apt-get 등 기본 패키지 관리 도구를 사용하여 생각 없이 설치하면 기존에 운영하던 k8s 클러스터 버전과 맞지 않은 최신 버전이 설치 된다는 것입니다\u0026hellip;(그러면 저처럼 작업을 2번 하게 됩니다)\n그런데 바이너리 파일을 사용해서 설치하기는 또 귀찮고..\n따라서 apt를 사용하되, 버전을 옵션으로 주는 방식으로 설치를 하기로 했습니다.\n나중에 또 2번 작업하지 않기 위해서, 그리고 중간에 발생한 이슈도 기록해둘겸 내용을 정리하였습니다.\n설치하기 사실 기존 k8s 클러스터 설치 프로세스와 다른 점은 거의 없습니다. 기존의 프로세스는 여기를 참고 바랍니다.\n특정 버전 설치 옵션을 주는 부분만 신경써서 진행하면 됩니다.\n저장소 추가 # curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF # sudo apt-get update  원래는 아래와 같이 그냥 최신 버전을 설치했다면 이번엔 옵션으로 버전을 줘야 합니다. sudo apt-get install -y kubelet kubeadm kubectl  운영 중인 클러스터의 버전 확인하기 먼저 기존에 운영 중인 k8s 클러스터의 버전을 확인합니다.\nroot@hci-k8s-master-01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-worker-01 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-worker-02 Ready \u0026lt;none\u0026gt; 74d v1.18.8 k8s-master-01 Ready master 76d v1.18.6  클러스터의 node들은 1.18.x 버전을 사용하는 것을 알 수 있습니다.\n버전을 확인했으니 이제 설치를 진행합니다.\n설치 가능한 버전 확인하기 저는 sudo apt-get install -y kubelet=1.18.8 이렇게 옵션을 주고 설치하려고 했는데, 애석하게도 실패했습니다.\n아래와 같은 로그가 발생합니다.\n# sudo apt-get install kubelet=1.18.8 Reading package lists... Done Building dependency tree Reading state information... Done E: Version '1.18.8' for 'kubelet' was not found  정확한 버전 옵션을 확인해보도록 합니다.\n# apt-cache madison kubeadm  출력 결과에 제가 사용하려던 1.18.8 버전은 다음과 같이 나와있습니다.\nkubeadm | 1.18.8-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages  1.18.8-00 버전으로 설치하기 이제 다시 설치를 진행해봅니다.\n원래 kubeadm만 설치해도 kubectl과 kubelet이 의존적으로 설치됩니다.\n그런데 말입니다\u0026hellip; 특이점이 발생합니다.\n# sudo apt-get install kubeadm=1.18.8-00 Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: kubectl The following NEW packages will be installed: kubeadm kubectl 0 upgraded, 2 newly installed, 0 to remove and 5 not upgraded. Need to get 0 B/16.5 MB of archives. After this operation, 82.8 MB of additional disk space will be used. Do you want to continue? [Y/n] Y Selecting previously unselected package kubectl. (Reading database ... 128167 files and directories currently installed.) Preparing to unpack .../kubectl_1.19.2-00_amd64.deb ... Unpacking kubectl (1.19.2-00) ... Selecting previously unselected package kubeadm. Preparing to unpack .../kubeadm_1.18.8-00_amd64.deb ... Unpacking kubeadm (1.18.8-00) ... Setting up kubectl (1.19.2-00) ... Setting up kubeadm (1.18.8-00) ...  위와 같이 kubectl이 1.19.2-00 버전으로 설치가 됩니다.\n이런 경우, kubectl을 downgrade 해주는 방법도 있지만..\n# apt-get install kubectl=1.18.8-00  애초에 처음부터 모두 버전을 지정해주면 됩니다.\n# sudo apt-get install -y kubelet=1.18.8-00 kubeadm=1.18.8-00 kubectl=1.18.8-00  설치된 버전 확인 하기 마지막으로 설치된 버전을 확인해보겠습니다.\n# dpkg -l kube* Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-=========================-=================-=================-======================================================== ii kubeadm 1.18.8-00 amd64 Kubernetes Cluster Bootstrapping Tool ii kubectl 1.18.8-00 amd64 Kubernetes Command Line Tool ii kubelet 1.18.8-00 amd64 Kubernetes Node Agent ii kubernetes-cni 0.8.7-00 amd64 Kubernetes CNI  Master에 join하기 참고와 동일하게 진행합니다.\nmaster node에서 join 명령어 발급받기 # kubeadm token create --print-join-command  worker node에서 join 명령어 수행하기 신규로 join하려는 worker node에서 위에서 발급받은 명령어를 수행합니다.\n# kubeadm join X.X.X.X:XX --token xxxx --discovery-token-ca-cert-hash sha256:xxxx W1113 13:04:14.543859 83613 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set. [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.18\u0026quot; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.  master node에서 cluster로 join된 것 확인하기 STATUS가 NotReady -\u0026gt; Ready로 변경되는데 시간이 걸릴 수 있습니다.\n# kubectl get nodes  ","id":5,"section":"posts","summary":"배경 회사에서 Kubernetes를 운영하다보면 클러스터에 새로운 worker node를 추가해야하는 일이 종종 생깁니다. 문제는 apt-get 등 기본 패키지 관리 도구를 사용하","tags":["kubernetes"],"title":"Kuberenets 특정 버전으로 설치하기","uri":"https://healinyoon.github.io/2020/10/20201009_k8s_install_specific_version/","year":"2020"},{"content":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다.\n$ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $ cd azure-cli-with-python3 $ virtualenv -p /usr/bin/python3.5. $ source ./bin/activate $ sudo apt install python3-dev $ pip install azure-cli $ python --version  ref)  The way to configure Azure CLI to use Python 3.5 on system where the default Python version is 2.x · Issue #2529 · Azure/azure-cli  ","id":6,"section":"posts","summary":"Azure CLI를 virtualenv 환경에서 설치하기 Host 서버에 그냥 설치하면 파이썬 버전 문제로 실행이 안될 수 있다. virtualenv 를 설치하고 az cli 를 설치하는 방법이다. $ pip install virtualenv $ mkdir azure-cli-with-python3 $ which python3 $","tags":["azure"],"title":"Azure CLI 설치하기 with virtualenv","uri":"https://healinyoon.github.io/2020/09/20201023_azure_cli_with_virtualenv/","year":"2020"},{"content":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다.\n구성 H/W 구성하려는 kubernetes cluster는 다음과 같습니다.\n   노드 vCPU RAM Disk     master01 2 8GiB    worker01 2 8GiB    worker02 2 8GiB     Required ports 1) Control-plane node(s)\n2) Worker node(s)\nDocker 설치(모든 node) # curl -fsSL https://get.docker.com/ | sudo sh # systemctl start docker # systemctl enable docker  Kubernetes 클러스터 구성 쿠버네티스 공식 사이트 kubeam 설치\n모든 노드에 아래의 패키지를 설치한다.\n kubeadm: 클러스터를 부트스트랩하는 명령(쿠버네티스 관리) kubelet: 클러스터의 모든 시스템에서 실행되는 구성 요소로, 포트 및 컨테이너 시작과 같은 작업을 수행(쿠버네티스 서비스) kubectl: 클러스터와 통신하기 위한 command line util(쿠버네티스 클라이언트 프로그램, 클러스터 구성과는 전혀 상관 없음)  1) Kubernetes 리포지토리 구성(모든 node) # sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -  2) Kubeadm, Kubelet, Kubectl 설치(모든 node) # cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl // 패키지가 자동으로 업그레이드 되지 않도록 설정 sudo apt-mark hold kubelet kubeadm kubectl // 데몬 재시작 systemctl daemon-reload systemctl restart kubelet  3) hostname 등록(모든 node) 이때 주의할 점은 hostname에 알파벳 소문자와 숫자, \u0026lsquo;-\u0026rsquo; 기호만 가능하다.\n# sudo hostnamectl set-hostname master01 또는 # sudo hostnamectl set-hostname worker01  4) /etc/hosts 파일 수정(모든 node) # vi /etc/hosts 아래에 추가 {IP} master01 {IP} worker01 {IP} worker02  5) Iptables 설정(모든 node) 브릿지 되어있는 IPv4 트래픽을 iptables 체인으로 전달될 수 있도록 한다.\n# cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF # sudo sysctl --system  6) 스왑 기능 비활성화(모든 node) swap 끄기 # sudo swapoff -a 재부팅 후에도 swap 설정 유지 # sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab  7) 마스터 노드 초기화 # kubeadm init  kubeadm init 명령어 실행시 아래와 같이 출력된다.\nTo start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 아래 명령어를 수행하고\n$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config  worker node에서 아래의 명령어를 수행하여 master node와 join한다.\nkubeadm join 10.1.11.4:6443 --token xxxxxxxxxxxxxxxxxxxxxxx \\ --discovery-token-ca-cert-hash sha256:e184c470296359bc4a35bc57624b03d8c4b3eb2bd46f413f3a68a86f182c9844  master node에서 kubectl get nodes 명령어를 입력하면 다음과 같이 출력된다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 NotReady master 10m v1.19.0 healin-k8s-worker01 NotReady \u0026lt;none\u0026gt; 30s v1.19.0 healin-k8s-worker02 NotReady \u0026lt;none\u0026gt; 29s v1.19.0  8) 네트워크 애플리케이션 설치 pod 네트워크 애플리케이션을 설치해야 클러스터 내의 node간 통신이 가능하다.\n사용 가능한 네트워크 옵션은 여기에서 확인할 수 있다.\n다음 명령을 master node에서 수행하여 weave pod 네트워크 애플리케이션을 설치한다.\nkubectl apply -f \u0026quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\u0026quot; 이런 오류 발생시 The connection to the server localhost:8080 was refused - did you specify the right host or port? export KUBECONFIG=/etc/kubernetes/admin.conf 을 적용해보자  master node에서 kubectl get nodes 명령어를 잠시 후 다시 입력하면 다음과 같이 STATUS가 NotReady =\u0026gt; Ready로 변경된 것을 확인할 수 있다.\n# kubectl get nodes NAME STATUS ROLES AGE VERSION healin-k8s-master01 Ready master 17m v1.19.0 healin-k8s-worker01 Ready \u0026lt;none\u0026gt; 7m42s v1.19.0 healin-k8s-worker02 Ready \u0026lt;none\u0026gt; 7m41s v1.19.0  9) master node를 worker node로도 사용하고 싶다면, 쿠버네티스 클러스터의 control-plane 노드는 보안상의 이유로 격리되어 있다(기본값).\nmaster node에서는 pod 가 스케줄링 되지 않으므로, 1대의 머신으로만 쿠버네티스 클러스터를 구축할 경우 격리 해제해야 한다.\n$ kubectl taint nodes –all node-role.kubernetes.io/master-  Control plane node isolation에 대한 자세한 내용은 아래 경로를 참고한다.\n* 쿠버네티스 공식 문서 - Control plane node isolation\n* Kubernetes Control-Plane Node에 Pod 띄울수 있는 방법 (Taints)\n참고 쿠버네티스(kubernetes) 설치 및 환경 구성하기\n","id":7,"section":"posts","summary":"Kubernetes Cluster 설치하기 이 페이지에서는 ubuntu18.04에 kubeadm tool을 설치하고 이를 사용하여 kubernetes cluster를 구축하는 방법을 정리했습니다. 구성 H/W 구성하","tags":["docker","kubernetes"],"title":"Kubernetes Cluster 설치하기(ubuntu18.04)","uri":"https://healinyoon.github.io/2020/09/20200828_install_kubernetes_cluster_ubuntu/","year":"2020"},{"content":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지\n공식 사이트의 설명: Installing a Pod network add-on\n","id":8,"section":"posts","summary":"Pod Network pod network가 헷갈려서 정리용으로 만든 페이지 공식 사이트의 설명: Installing a Pod network add-on","tags":["kubernetes"],"title":"쿠버네티스 POD Network","uri":"https://healinyoon.github.io/2020/09/%EB%AF%B8%EC%99%8420200901_k8s_pod_network/","year":"2020"},{"content":"목적 Jenkins node heap memory 사이즈 변경 방법 정리\n참고 자료  Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양  Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인수로 받을 수 있음\nJenkins node heap memory 사이즈를 변경하기 위한 인수: -Xmx와 -Xms\n-Xms\u0026lt;size\u0026gt; set initial Java heap size -Xmx\u0026lt;size\u0026gt; set maximum Java heap size  예시 1) master의 /etc/default/jenkins에서 설정 AVA_ARGS=\u0026quot;-Xmx256m\u0026quot; # default value JAVA_ARGS=\u0026quot;-Xmx2048m\u0026quot; # 2048MB size  2) Jenkins UI에서 slave 설정 [Jenkins 관리] \u0026gt; [노드 관리] \u0026gt; \u0026lsquo;노드 선택 후 ' [설정] \u0026gt; [고급] Jenkins 권장 사양(자세히) 참고  It is recommended to define the same value for both -Xms and -Xmx so that the memory is allocated on startup rather than runtime. 대/소문자는 상관 없음(예: -Xmx10G는 -Xmx10g와 동일함) Java processes 전역에 적용하고 싶으면 JAVA_TOOL_OPTIONS 환경 변수 사용(예: export JAVA_TOOL_OPTIONS=\u0026quot;-Xmx6g\u0026quot;)  ","id":9,"section":"posts","summary":"목적 Jenkins node heap memory 사이즈 변경 방법 정리 참고 자료 Java Memory Management for Java Virtual Machine (JVM) Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide CloudBees Jenkins JVM troubleshooting Jenkins 권장 사양 Jenkins Heap Memory 옵션 Jenkins에서는 다양한 JAVA 옵션을 인","tags":["jenkins"],"title":"Jenkins Node Heap Memory 사이즈 설정하기(-Xmx/-Xms 옵션)","uri":"https://healinyoon.github.io/2020/08/20200831_jenkins_node_heap_size/","year":"2020"},{"content":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다.\n 빌드 테스트를 위한 다양한 환경이 필요한 경우 크고 무거운 프로젝트의 작업 부하를 분산해야하는 경우  위의 요구사항을 충족하기 위해 Jenknis 분산 아키텍처인 Master-Slave 구성이 도입되었습니다. Jenkins Master와 Slave의 역할    구분 역할     Master * Build 작업 예약* Build 실행을 위한 작업 분배* Slave node 모니터링(필요에 따라 on/offline 전환 가능)* Build 결과 기록   Slave * Jenkins Master의 요청 수신* Build 실행    Jenkins Master-Slave 연동 방법 Jenkins Master-Slave는 다음의 요구사항을 만족시키면 매우 간단하게 구성할 수 있습니다.\n Master 서버에서 Slave 서버에 접근 가능하도록 설정 Slave 서버의 Jenkins Java 요구사항 충족  1. Jenkins 사용자 생성(모든 Slave node에서 진행) # adduser jenkins 사용자 생성 후 다음과 같이 홈디렉토리의 권한을 변경해준다 # chown ldccai:jenkins /home/jenkins # chmod 775 /home/jenkins  2. Java 8 설치(모든 Slave node에서 진행) # apt-get install openjdk-8-jdk  3. Jenkins에서 Slave node 등록 등록된 노드의 [로그]를 확인하면 다음과 같이 Master 서버와 잘 연동된 것을 확인할 수 있습니다. ","id":10,"section":"posts","summary":"Jenkins Master-Slave Jenkins는 기본적으로 단일 서버로 동작합니다. 그러나 단일 서버는 다음과 같은 상황을 충족하기 충분하지 않습니다. 빌드 테스트를 위한 다양한 환경이 필","tags":["jenkins"],"title":"Jenkins Master-Slave 구성하기","uri":"https://healinyoon.github.io/2020/08/20200827_jenkins_master_slave/","year":"2020"},{"content":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미합니다.\nCI(Continuous Integration): 지속적 통합 개발자의 변경 사항이 정기적으로(최상의 경우 하루 여러번) 빌드 및 테스트 되고 공유 리포지토리에 병합되는 프로세스입니다.\nCD(Continous Deploy or ontinuous Delivery): 지속적 배포 Jez Humble의 정의\nContinuous Deployment is about automating the release of a good build to the production environment. In fact, Humble thinks it might be more accurate to call it “continuous release.”\nContinuous Delivery is about 1) ensuring that every good build is potentially ready for production release. At the very least, 2) it’s sent to the user acceptance test (UAT) environment. 3) Your business team can then decide when a successful build in UAT can be deployed to production —and they can do so at the push of a button.\n상황에 따라 이미 운영되고 있는 프로덕션 환경에 바로 release 하는 것은 문제가 될 수 있습니다. 이러한 경우로 임베디드 소프트웨어 등이 해당됩니다.\n따라서 잠재적으로는 release 가능하지만, 프로덕션 환경에 자동으로 release 되지 않는 것이 Continuous Delivery 입니다. 정리  CI: 지속적인 빌드 / 테스트 / 통합 CD: CI의 연장선 ~ Release 준비 완료(Delivery) or 제품 출시(Deploy)   Jenkins for CI/CD 다양한 CI/CD tool CI/CD 구현을 위한 다양한 tool이 있습니다. tool에 대한 자세한 정보 참고\n왜 Jenkins인가 Jenkins는 아래와 같은 강점을 가지고 있습니다.\n It is an open-source tool with great community support. It is easy to install. It has 1000+ plugins to ease your work. If a plugin does not exist, you can code it and share it with the community. It is free of cost. It is built with Java and hence, it is portable to all the major platforms.  하지만 단점 역시 존재합니다. CI/CD tool 중에서 자주 사용되는 Jenkins vs Gitlab vs Travis 비교 글을 참고하시면 Jenkins의 장단점을 이해하는데 도움이 됩니다.\n","id":11,"section":"posts","summary":"CI/CD란 CI/CD는 애플리케이션 개발 단계를 자동화하여 보다 작은 코드 단위와 짧은 주기로 Test와 Build를 수행하고 고객에게 제공하는 방법을 의미","tags":["jenkins"],"title":"CI/CD와 Jenkins","uri":"https://healinyoon.github.io/2020/08/20200827_cicd_and_jenkins/","year":"2020"},{"content":"HUGO 글 생성하기 $ hugo new {파일명}  HUGO 블로그 빌드 $ hugo -t {테마명}  Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master  ","id":12,"section":"posts","summary":"HUGO 글 생성하기 $ hugo new {파일명} HUGO 블로그 빌드 $ hugo -t {테마명} Git push $ cd public $ git add . $ git commit -m \u0026quot;{commit 메세지}\u0026quot; $ git push origin master $ cd .. $ git add . $ git commit -m \u0026quot;{commit 메","tags":["go","hugo"],"title":"HUGO 블로그 새글 업로드하기","uri":"https://healinyoon.github.io/2020/08/20200827_hugo_blog/","year":"2020"},{"content":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부한 내용을 정리하였다.\n 1. 쉘(shell) 이란? 쉘은 사용자가 입력한 명령을 해석해서 커널에 전달하거나, 커널의 처리 결과를 사용자에게 전달하는 역할을 한다.\n1-1. 쉘 명령문 형식 쉘 명령문의 형식은 다음과 같다.\n{프롬프트} {명령어} {옵션} {인자} ex) # cd ~ ex) # ls -al ex) # rm -rf testdir ex) # cat test.txt  1-2. 쉘 환경변수 쉘은 다양한 환경변수 값을 설정할 수 있는데, 리눅스의 주요 환경변수 목록은 아래와 같다.\n   환경변수 설명     HOME 현재 사용자의 홈디렉토리   PATH 실행 파일을 찾는 디렉토리 경로   LANG 기본 지원 언어   PWD 사용자의 현재 작업 디렉토리   TERM 로그인 터미널 타입   SHELL 로그인시 사용하는 쉘   USER 현재 사용자의 이름   DISPLAY X 디스플레이 이름   HOSTNAME 호스트 이름   USERNAME 현재 사용자 이름   OSTYPE 운영체제 타입    현재 설정된 환경 변수는 echo ${환경변수 이름} 명령어를 통해 확인할 수 있다.\nex) echo $HOME ex) echo $HOSTNAME  환경변수 설정 값을 변경하려면 export {환경변수}={값} 명령어를 실행하면 된다. 그 외의 환경변수는 printenv 명령어로 확인할 수 있다. 단, 일부 환경변수는 printenv 명령을 실행해도 출력되지 않는다.\n 2. 쉘 스크립트 사용해보기 쉘 스크립트도 일반적인 프로그래밍 언어와 비슷하게 변수, 반복문, 제어문 등을 사용할 수 있다.\n2-1. 쉘 스크립트 작성하기 아래와 같이 간단한 쉘 스크립트를 작성하고 script.sh으로 저장해보자.\n#!/bin/sh echo \u0026quot;User Name: \u0026quot; $USER echo \u0026quot;User Home Directory: \u0026quot; $HOME exit 0  위의 코드를 간단히 설명하면 다음과 같다.\n 1행: bash를 사용하겠다는 의미로, 쉘 스크립트 작성시 첫 행에 반드시 입력해야한다. 2, 3행: echo 명령은 화면에 출력하는 명령이며, 먼저 문자를 출력하고 ${환경변수}에 해당하는 값을 출력한다. 4행: 종료 코드를 반환하며, 0은 쉘 스크립트 실행 성공을 의미한다.  2-2. 쉘 스크립트 실행하기 쉘 스크립트를 실행하는 방법에는 크게 두 가지가 있다.\n1) sh 명령으로 실행\nsh {실행파일 이름} ex) sh script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  2) 파일 권한을 실행 가능하게 변경하고 실행\n# chmod +x {실행파일 이름} # ./{실행파일 경로} ex) # chmod +x script.sh # ./script.sh [실행 결과] User Name: rin_gu User Home Directory: /home/rin_gu  3. 변수 쉘 스크립트 변수 사용시 주의해야할 특징은 다음과 같다.\n 변수를 사용하기 전에 미리 선언하지 않으며, 처음 변수에 값이 할당되면 자동으로 변수가 생성된다. 변수에 넣는 모든 값을 \u0026lsquo;문자열\u0026rsquo;로 취급한다. 변수 이름의 대소문자를 구분한다. 변수를 대입할 때 \u0026lsquo;=\u0026rsquo; 좌우에 공백이 없어야 한다.  3-1. 변수의 입출력 \u0026lsquo;$\u0026rsquo; 라는 문자가 들어간 글자를 출력하려면 \u0026lsquo;\u0026lsquo;로 묶어주거나 앞에 \u0026lsquo;\\\u0026lsquo;를 붙여줘야 한다.\n#!/bin/sh var=\u0026quot;Hi Shell\u0026quot; echo $var echo '$var' echo \\$var exit 0 [실행 결과] Hi Shell $var $var  변수의 입력과 출력은 다음과 같이 사용할 수 있다.\n#!/bin/sh echo \u0026quot;값을 입력하세요: \u0026quot; read var echo \u0026quot;입력된 값은 \u0026quot; $var \u0026quot;입니다.\u0026quot; exit 0 [실행 결과] 값을 입력하세요: shell script study 입력된 값은 shell script study 입니다.  3-2. 숫자 계산 변수에 넣은 값은 모두 문자열이 되므로, shell script에서 연산을 하기 위해서는 expr 키워드를 사용해야한다. shell script에서 숫자 계산을 하기 위한 규칙은 다음과 같다.\n 수식과 함께 역따옴표로 묶어줘야 한다. 수식에 괄호를 사용하기 위해서는 그 앞에 역슬래시를 붙여줘야 한다. 곱하기 기호를 사용할 때도 그 앞에 역슬래시를 붙여줘야 한다.  #!/bin/sh n1=10 n2=$n1+20 echo $n2 n3=`expr $n1 + 20` echo $n3 n4=`expr \\( $n1 + 20 \\) / 10 \\* 2` echo $n4 exit 0 [실행 결과] 10+20 30 // ※ 각 단어를 띄어쓰기해야하는 것에 주의 6  3-3. 파라미터 변수 파라미터 변수는 $0, $1, $2 등의 형태를 가지며, 아래와 같이 매칭된다.\n   명령어 파라미터 1 파라미터 2 파라미터 n\u0026hellip;     $0 $1 $2 $n\u0026hellip;    예를 들면 다음과 같다.\n   ./script 1 2     $0 $1 $2    #!/bin/sh echo \u0026quot;실행파일 이름: \u0026lt;$0\u0026gt;\u0026quot; echo \u0026quot;첫 번째 파라미터: \u0026lt;$1\u0026gt;.\u0026quot; echo \u0026quot;두 번째 파라미터: \u0026lt;$2\u0026gt;.\u0026quot; echo \u0026quot;전체 파라미터는 \u0026lt;$*\u0026gt;이다.\u0026quot; exit 0 [실행 결과] 실행파일 이름: \u0026lt;./script.sh\u0026gt; 첫 번째 파라미터: \u0026lt;1\u0026gt;. 두 번째 파라미터: \u0026lt;2\u0026gt;. 전체 파라미터는 \u0026lt;1 2\u0026gt;이다.  4. if문 if문의 기본 사용법과 조건 연산자를 알아보자.\n4-1. if문 기본 문법 if문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 fi  이때 주의할 점은 [ 조건 ] 에서 각 단어에 모두 공백이 있어야 한다.\n#!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;rin_gu\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; fi exit 0 [실행 결과] 참 입니다.  4-2. if-else문 if-else문의 기본 문법은 다음과 같다.\nif [ 조건 ] then 참일 경우 실행 else문 거짓인 경우 실행 fi  #!/bin/sh if [ \u0026quot;rin_gu\u0026quot; = \u0026quot;shell script\u0026quot; ] then echo \u0026quot;참 입니다.\u0026quot; else echo \u0026quot;거짓 입니다.\u0026quot; fi exit 0 [실행 결과] 거짓 입니다.  4-3. 조건문에 사용되는 비교 연산자 조건문에 사용디는 비교 연산자에는 문자열 비교 연산자와 산술 비교 연산자가 있다.\n문자열 비교 연산자\n   문자열 비교 내용 예시     = 같으면 참 \u0026ldquo;문자열\u0026rdquo; = \u0026ldquo;문자열\u0026rdquo;   != 같지 않으면 참 \u0026ldquo;문자열\u0026rdquo; != \u0026ldquo;문자열\u0026rdquo;   -n 문자열이 NULL 값이 아니면 참 -n \u0026ldquo;문자열\u0026rdquo;   -z 문자열이 NULL 값이면 참 -z \u0026ldquo;문자열\u0026rdquo;    산술 비교 연산자\n   산술 비교 내용 예시     -eq 두 수식이 같으면 참 수식 -eq 수식   -ne 두 수식이 같지 않으면 참 수식 -ne 수식   -gt 왼쪽 수식이 크면 참 수식 -gt 수식   -ge 왼쪽 수식이 크거나 같으면 참 수식 -ge 수식   -lt 왼쪽 수식이 작으면 참 수식 -lt 수식   -le 왼쪽 수식이 작거나 같으면 참 수식 -le 수식   ! 수식이 거짓이라면 참 !수식    #!/bin/sh if [ 1024 -eq 2024 ] then echo \u0026quot;1024과 2024는 같다.\u0026quot; else echo \u0026quot;1024과 2024는 다르다.\u0026quot; fi exit 0 [실행 결과] 1024과 2024는 다르다.  4-4. 조건문에 사용되는 파일과 관련된 연산자 if문에서 파일을 처리하는 조건 연산자는 다음과 같다.\n파일 조건 | 내용 | 예시 \u0026mdash; | \u0026mdash; -d | 파일이 디렉토리면 참 | -d 파일명 -e | 파일이 존재하면 참 | -e 파일명 -f | 파일이 일반 파일이면 참 | -f 파일명 -g | 파일에 set-group-id가 설정되면 참 | -g 파일명 -r | 파일이 읽기 가능하면 참 | -r 파일명 -s | 파일 크기가 0이 아니면 참 | -s 파일명 -u | 파일에 set-user-id가 설정되면 참 | -u 파일명 -w | 파일이 쓰기 가능하면 참 | -w 파일명 -x | 파일이 실행 가능하면 참 | -x 파일명\n 5. case문 case문의 기본 사용법을 알아보자\n5-1. case문 기본 문법 case문의 기본 문법은 다음과 같다.\ncase [ 변수 ] in 변수값1) 변수와 변수값1이 일치할 경우 실행 변수값2) 변수와 변수값2가 일치할 경우 실행 ... esac  5-2. case 문 사용해보기 (작성중)\n","id":13,"section":"posts","summary":"Intro 이번 포스트에서는 쉘 스크립트에 대해 알아보고, 사용 방법을 다룬다. 전반적으로 \u0026ldquo;이것이 우분투 리눅스다\u0026rdquo; 책을 참고하였고, 공부","tags":["shell"],"title":"Shell Scripts 사용하기","uri":"https://healinyoon.github.io/2019/06/20190602_shell_script/","year":"2019"},{"content":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다.\n 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry   2. Docker 실행 및 정보 확인 2-1. Docker 실행하기 # 시스템 부팅 시 docker를 시작하도록 설정 systemctl enable docker.service # Docker 실행 systemctl start docker.service # Docker 상태 확인 systemctl status docker.service  2-2. Docker 명령어 사용하기 Docker 명령어의 기본 형식은 docker {명령어} 입니다.\n# Docker 버전 확인 docker version # Docker 실행 환경 확인 docker system info # Docker 디스크 상태 확인 docker system df # 그 외 Docker 명령어 살펴보기 docker -help   3. Docker 사용해보기 Docker가 정상적으로 동작하는지 확인하기 위해 컨테이너를 실행합니다.\n3-1. Docker 컨테이너 실행하기 # Docker 컨테이너 실행 명령어 docker run {옵션} {컨테이너 명/ID}  docker run 명령어는 컨테이너를 생성하고 실행시키는 명령어 입니다. 이때 dockers는 로컬에 해당 이미지가 있는지 학인하고, 없는 경우 docker hub에서 pull을 먼저 진행하고 컨테이너를 생성합니다.\n3-2. Hello, world! docker run hello-world  hello-world 컨테이너가 실행되면 다음과 같이 메세지가 출력됩니다.\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.  3-3. Nginx 이번에는 Nginx 웹서버를 docker로 설치해보겠습니다.\n# Nginx 이미지 다운로드 docker pull nginx # 다운로드한 이미지 확인 docker images # Nginx 컨테이너 실행 docker run --name nginx-webserver -d -p 80:80 nginx  docker run 명령어에서 자주 쓰이는 옵션은 다음과 같습니다.\n \u0026ndash;name: 컨테이너의 이름을 설정한다. -d: 컨테이너를 백그라운드에서 실행시킨다. -p: 호스트 포트와 컨테이너 내부의 포트를 매핑한다(형식: -p {host 포트}:{컨테이너 포트}).  실행중인 docker 컨테이너를 확인하고 싶을 때는 docker ps 명령어로 확인 할 수 있습니다.\n# 실행중인 컨테이너 확인 docker ps # 컨테이너 상태 확인 docker container stats  마지막으로 http://localhost:80으로 접속해서 Nginx 웹 브라우저를 확인합니다.\n 참고 사이트  https://niceman.tistory.com/36 https://futurecreator.github.io/2018/11/16/docker-container-basics/ http://pyrasis.com/Docker/Docker-HOWTO  ","id":14,"section":"posts","summary":"Intro 이번 포스트에서는 CentOS에 Docker를 설치하는 방법을 정리했습니다. 1. Docker 설치하기 # yum 패키지 업데이트 yum update # docker, docker registry 설치 yum -y install docker docker-registry 2. Docker 실행 및","tags":["docker"],"title":"Docker 설치하기(CentOS)","uri":"https://healinyoon.github.io/2019/06/20190611_docker_install/","year":"2019"}],"tags":[{"title":"azure","uri":"https://healinyoon.github.io/tags/azure/"},{"title":"docker","uri":"https://healinyoon.github.io/tags/docker/"},{"title":"go","uri":"https://healinyoon.github.io/tags/go/"},{"title":"hugo","uri":"https://healinyoon.github.io/tags/hugo/"},{"title":"java","uri":"https://healinyoon.github.io/tags/java/"},{"title":"jenkins","uri":"https://healinyoon.github.io/tags/jenkins/"},{"title":"kubernetes","uri":"https://healinyoon.github.io/tags/kubernetes/"},{"title":"openjdk","uri":"https://healinyoon.github.io/tags/openjdk/"},{"title":"shell","uri":"https://healinyoon.github.io/tags/shell/"}]}